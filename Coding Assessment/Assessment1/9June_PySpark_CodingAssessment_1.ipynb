{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbKEi8J2T+v21iehbOUOdn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simantinip04/Data-Engineering/blob/main/Coding%20Assessment/Assessment1/9June_PySpark_CodingAssessment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkOkpUTjBLcM"
      },
      "outputs": [],
      "source": [
        "#PySpark Coding Assessment-1 Master Task Set\n",
        "#Ingestion, Transformation, Spark SQL, Aggregations, Joins, UDFs, and Storage"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "\n",
        "spark=SparkSession.builder.appName(\"PySpark Assessment\").getOrCreate()\n",
        "\n",
        "print(spark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjyqcr_5ByoA",
        "outputId": "1bde36d3-c0c4-45f7-bd21-b7763485055b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7bce726f9b10>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 1- Data Ingestion & Exploration\n",
        "\n",
        "# Saving sample datasets\n",
        "customers_data = \"\"\"CustomerID,Name,Email,City,SignupDate\n",
        "101,Ali,ali@gmail.com,Mumbai,2022-05-10\n",
        "102,Neha,neha@yahoo.com,Delhi,2023-01-15\n",
        "103,Ravi,ravi@hotmail.com,Bangalore,2021-11-01\n",
        "104,Sneha,sneha@outlook.com,Hyderabad,2020-07-22\n",
        "105,Amit,amit@gmail.com,Chennai,2023-03-10\"\"\"\n",
        "\n",
        "orders_data = \"\"\"OrderID,CustomerID,Product,Category,Quantity,Price,OrderDate\n",
        "1,101,Laptop,Electronics,2,50000.0,2024-01-10\n",
        "2,101,Mouse,Electronics,1,1200.0,2024-01-15\n",
        "3,102,Tablet,Electronics,1,20000.0,2024-02-01\n",
        "4,103,Bookshelf,Furniture,1,3500.0,2024-02-10\n",
        "5,104,Mixer,Appliances,1,5000.0,2024-02-15\n",
        "6,105,Notebook,Stationery,5,500.0,2024-03-01\n",
        "7,102,Phone,Electronics,1,30000.0,2024-03-02\"\"\"\n",
        "\n",
        "with open(\"customers.csv\", \"w\") as f:\n",
        "    f.write(customers_data)\n",
        "\n",
        "with open(\"orders.csv\", \"w\") as f:\n",
        "    f.write(orders_data)\n",
        "\n",
        "# Loading CSV with schema inference\n",
        "customers = spark.read.csv(\"customers.csv\", header=True, inferSchema=True)\n",
        "orders = spark.read.csv(\"orders.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Show schema\n",
        "customers.printSchema()\n",
        "orders.printSchema()\n",
        "\n",
        "# Count total customers and orders\n",
        "print(\"Customers count:\", customers.count())\n",
        "print(\"Orders count:\", orders.count())\n",
        "\n",
        "# Distinct cities\n",
        "distinct_cities=customers.select(\"City\").distinct()\n",
        "print(\"Distinct cities:\")\n",
        "distinct_cities.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBcDJDL9DwMN",
        "outputId": "301fbf20-a915-4181-d1ba-644d79f9e36a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- CustomerID: integer (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Email: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- SignupDate: date (nullable = true)\n",
            "\n",
            "root\n",
            " |-- OrderID: integer (nullable = true)\n",
            " |-- CustomerID: integer (nullable = true)\n",
            " |-- Product: string (nullable = true)\n",
            " |-- Category: string (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- Price: double (nullable = true)\n",
            " |-- OrderDate: date (nullable = true)\n",
            "\n",
            "Customers count: 5\n",
            "Orders count: 7\n",
            "Distinct cities:\n",
            "+---------+\n",
            "|     City|\n",
            "+---------+\n",
            "|Bangalore|\n",
            "|  Chennai|\n",
            "|   Mumbai|\n",
            "|    Delhi|\n",
            "|Hyderabad|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 2- DataFrame Transformations\n",
        "from pyspark.sql.functions import col, year\n",
        "\n",
        "# Add a column TotalAmount = Price * Quantity\n",
        "orders = orders.withColumn(\"TotalAmount\", col(\"Price\") * col(\"Quantity\"))\n",
        "orders.show()\n",
        "\n",
        "# Create a new column OrderYear from OrderDate\n",
        "orders = orders.withColumn(\"OrderYear\", year(\"OrderDate\"))\n",
        "orders.show()\n",
        "\n",
        "# Filter orders with TotalAmount > 10,000\n",
        "orders_filtered = orders.filter(col(\"TotalAmount\") > 10000)\n",
        "orders_filtered.show()\n",
        "\n",
        "# Drop the Email column from customers\n",
        "customers = customers.drop(\"Email\")\n",
        "customers.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpIC1sPdDwPP",
        "outputId": "7bfdca61-fee0-4d13-8252-9e96f25ce16b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+\n",
            "|OrderID|CustomerID|  Product|   Category|Quantity|  Price| OrderDate|TotalAmount|\n",
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+\n",
            "|      1|       101|   Laptop|Electronics|       2|50000.0|2024-01-10|   100000.0|\n",
            "|      2|       101|    Mouse|Electronics|       1| 1200.0|2024-01-15|     1200.0|\n",
            "|      3|       102|   Tablet|Electronics|       1|20000.0|2024-02-01|    20000.0|\n",
            "|      4|       103|Bookshelf|  Furniture|       1| 3500.0|2024-02-10|     3500.0|\n",
            "|      5|       104|    Mixer| Appliances|       1| 5000.0|2024-02-15|     5000.0|\n",
            "|      6|       105| Notebook| Stationery|       5|  500.0|2024-03-01|     2500.0|\n",
            "|      7|       102|    Phone|Electronics|       1|30000.0|2024-03-02|    30000.0|\n",
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+\n",
            "\n",
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+---------+\n",
            "|OrderID|CustomerID|  Product|   Category|Quantity|  Price| OrderDate|TotalAmount|OrderYear|\n",
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+---------+\n",
            "|      1|       101|   Laptop|Electronics|       2|50000.0|2024-01-10|   100000.0|     2024|\n",
            "|      2|       101|    Mouse|Electronics|       1| 1200.0|2024-01-15|     1200.0|     2024|\n",
            "|      3|       102|   Tablet|Electronics|       1|20000.0|2024-02-01|    20000.0|     2024|\n",
            "|      4|       103|Bookshelf|  Furniture|       1| 3500.0|2024-02-10|     3500.0|     2024|\n",
            "|      5|       104|    Mixer| Appliances|       1| 5000.0|2024-02-15|     5000.0|     2024|\n",
            "|      6|       105| Notebook| Stationery|       5|  500.0|2024-03-01|     2500.0|     2024|\n",
            "|      7|       102|    Phone|Electronics|       1|30000.0|2024-03-02|    30000.0|     2024|\n",
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+---------+\n",
            "\n",
            "+-------+----------+-------+-----------+--------+-------+----------+-----------+---------+\n",
            "|OrderID|CustomerID|Product|   Category|Quantity|  Price| OrderDate|TotalAmount|OrderYear|\n",
            "+-------+----------+-------+-----------+--------+-------+----------+-----------+---------+\n",
            "|      1|       101| Laptop|Electronics|       2|50000.0|2024-01-10|   100000.0|     2024|\n",
            "|      3|       102| Tablet|Electronics|       1|20000.0|2024-02-01|    20000.0|     2024|\n",
            "|      7|       102|  Phone|Electronics|       1|30000.0|2024-03-02|    30000.0|     2024|\n",
            "+-------+----------+-------+-----------+--------+-------+----------+-----------+---------+\n",
            "\n",
            "+----------+-----+---------+----------+\n",
            "|CustomerID| Name|     City|SignupDate|\n",
            "+----------+-----+---------+----------+\n",
            "|       101|  Ali|   Mumbai|2022-05-10|\n",
            "|       102| Neha|    Delhi|2023-01-15|\n",
            "|       103| Ravi|Bangalore|2021-11-01|\n",
            "|       104|Sneha|Hyderabad|2020-07-22|\n",
            "|       105| Amit|  Chennai|2023-03-10|\n",
            "+----------+-----+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 3- Handling Nulls & Conditionals\n",
        "from pyspark.sql.functions import when, lit, to_date\n",
        "\n",
        "#Simulate a null in City and fill it with “Unknown”.\n",
        "customers = customers.withColumn(\"City\", when(col(\"CustomerID\") == 102, None).otherwise(col(\"City\")))\n",
        "customers = customers.fillna({\"City\": \"Unknown\"})\n",
        "customers.show()\n",
        "\n",
        "#Label customers as “Loyal” if SignupDate is before 2022, else “New”.\n",
        "customers = customers.withColumn(\"CustomerType\", when(to_date(\"SignupDate\") < \"2022-01-01\", \"Loyal\").otherwise(\"New\"))\n",
        "customers.show()\n",
        "\n",
        "#Create OrderType column: \"Low\" if < 5,000, \"High\" if ≥ 5,000.\n",
        "orders = orders.withColumn(\"OrderType\", when(col(\"TotalAmount\") < 5000, \"Low\").otherwise(\"High\"))\n",
        "orders.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAUUhOVVDwXb",
        "outputId": "e29e98be-9c94-440e-895f-bbba4f36a4ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+---------+----------+------------+\n",
            "|CustomerID| Name|     City|SignupDate|CustomerType|\n",
            "+----------+-----+---------+----------+------------+\n",
            "|       101|  Ali|   Mumbai|2022-05-10|         New|\n",
            "|       102| Neha|  Unknown|2023-01-15|         New|\n",
            "|       103| Ravi|Bangalore|2021-11-01|       Loyal|\n",
            "|       104|Sneha|Hyderabad|2020-07-22|       Loyal|\n",
            "|       105| Amit|  Chennai|2023-03-10|         New|\n",
            "+----------+-----+---------+----------+------------+\n",
            "\n",
            "+----------+-----+---------+----------+------------+\n",
            "|CustomerID| Name|     City|SignupDate|CustomerType|\n",
            "+----------+-----+---------+----------+------------+\n",
            "|       101|  Ali|   Mumbai|2022-05-10|         New|\n",
            "|       102| Neha|  Unknown|2023-01-15|         New|\n",
            "|       103| Ravi|Bangalore|2021-11-01|       Loyal|\n",
            "|       104|Sneha|Hyderabad|2020-07-22|       Loyal|\n",
            "|       105| Amit|  Chennai|2023-03-10|         New|\n",
            "+----------+-----+---------+----------+------------+\n",
            "\n",
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+---------+---------+\n",
            "|OrderID|CustomerID|  Product|   Category|Quantity|  Price| OrderDate|TotalAmount|OrderYear|OrderType|\n",
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+---------+---------+\n",
            "|      1|       101|   Laptop|Electronics|       2|50000.0|2024-01-10|   100000.0|     2024|     High|\n",
            "|      2|       101|    Mouse|Electronics|       1| 1200.0|2024-01-15|     1200.0|     2024|      Low|\n",
            "|      3|       102|   Tablet|Electronics|       1|20000.0|2024-02-01|    20000.0|     2024|     High|\n",
            "|      4|       103|Bookshelf|  Furniture|       1| 3500.0|2024-02-10|     3500.0|     2024|      Low|\n",
            "|      5|       104|    Mixer| Appliances|       1| 5000.0|2024-02-15|     5000.0|     2024|     High|\n",
            "|      6|       105| Notebook| Stationery|       5|  500.0|2024-03-01|     2500.0|     2024|      Low|\n",
            "|      7|       102|    Phone|Electronics|       1|30000.0|2024-03-02|    30000.0|     2024|     High|\n",
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 4- Joins & Aggregations\n",
        "from pyspark.sql.functions import sum as _sum, count as _count\n",
        "\n",
        "# Join\n",
        "joined = orders.join(customers, on=\"CustomerID\", how=\"inner\")\n",
        "joined.show()\n",
        "\n",
        "# Total orders and revenue per city\n",
        "orders_revenue_city= joined.groupBy(\"City\").agg(\n",
        "    _count(\"OrderID\").alias(\"TotalOrders\"),\n",
        "    _sum(\"TotalAmount\").alias(\"Revenue\")\n",
        ")\n",
        "orders_revenue_city.show()\n",
        "\n",
        "# Top 3 customers by total spend\n",
        "customers_by_total_spend= joined.groupBy(\"CustomerID\", \"Name\").agg(\n",
        "    _sum(\"TotalAmount\").alias(\"TotalSpend\")\n",
        ").orderBy(col(\"TotalSpend\").desc())\n",
        "customers_by_total_spend.show(3)\n",
        "\n",
        "# Count products sold per category\n",
        "product_per_category = orders.groupBy(\"Category\").agg(_count(\"Product\").alias(\"ProductsSold\"))\n",
        "product_per_category.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cAG26dtOVhe",
        "outputId": "ba82d54c-805c-49d7-f8f7-c2298d67473f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+---------+-----------+--------+-------+----------+-----------+---------+---------+-----+---------+----------+------------+\n",
            "|CustomerID|OrderID|  Product|   Category|Quantity|  Price| OrderDate|TotalAmount|OrderYear|OrderType| Name|     City|SignupDate|CustomerType|\n",
            "+----------+-------+---------+-----------+--------+-------+----------+-----------+---------+---------+-----+---------+----------+------------+\n",
            "|       101|      1|   Laptop|Electronics|       2|50000.0|2024-01-10|   100000.0|     2024|     High|  Ali|   Mumbai|2022-05-10|         New|\n",
            "|       101|      2|    Mouse|Electronics|       1| 1200.0|2024-01-15|     1200.0|     2024|      Low|  Ali|   Mumbai|2022-05-10|         New|\n",
            "|       102|      3|   Tablet|Electronics|       1|20000.0|2024-02-01|    20000.0|     2024|     High| Neha|  Unknown|2023-01-15|         New|\n",
            "|       103|      4|Bookshelf|  Furniture|       1| 3500.0|2024-02-10|     3500.0|     2024|      Low| Ravi|Bangalore|2021-11-01|       Loyal|\n",
            "|       104|      5|    Mixer| Appliances|       1| 5000.0|2024-02-15|     5000.0|     2024|     High|Sneha|Hyderabad|2020-07-22|       Loyal|\n",
            "|       105|      6| Notebook| Stationery|       5|  500.0|2024-03-01|     2500.0|     2024|      Low| Amit|  Chennai|2023-03-10|         New|\n",
            "|       102|      7|    Phone|Electronics|       1|30000.0|2024-03-02|    30000.0|     2024|     High| Neha|  Unknown|2023-01-15|         New|\n",
            "+----------+-------+---------+-----------+--------+-------+----------+-----------+---------+---------+-----+---------+----------+------------+\n",
            "\n",
            "+---------+-----------+--------+\n",
            "|     City|TotalOrders| Revenue|\n",
            "+---------+-----------+--------+\n",
            "|Bangalore|          1|  3500.0|\n",
            "|  Chennai|          1|  2500.0|\n",
            "|   Mumbai|          2|101200.0|\n",
            "|  Unknown|          2| 50000.0|\n",
            "|Hyderabad|          1|  5000.0|\n",
            "+---------+-----------+--------+\n",
            "\n",
            "+----------+-----+----------+\n",
            "|CustomerID| Name|TotalSpend|\n",
            "+----------+-----+----------+\n",
            "|       101|  Ali|  101200.0|\n",
            "|       102| Neha|   50000.0|\n",
            "|       104|Sneha|    5000.0|\n",
            "+----------+-----+----------+\n",
            "only showing top 3 rows\n",
            "\n",
            "+-----------+------------+\n",
            "|   Category|ProductsSold|\n",
            "+-----------+------------+\n",
            "| Stationery|           1|\n",
            "|Electronics|           4|\n",
            "|  Furniture|           1|\n",
            "| Appliances|           1|\n",
            "+-----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM customers\").show()\n",
        "spark.sql(\"SELECT * FROM orders\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4UjcEPdYni0",
        "outputId": "81d1142b-7af3-499e-acf4-4085a006cef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+---------+----------+------------+\n",
            "|CustomerID| Name|     City|SignupDate|CustomerType|\n",
            "+----------+-----+---------+----------+------------+\n",
            "|       101|  Ali|   Mumbai|2022-05-10|         New|\n",
            "|       102| Neha|  Unknown|2023-01-15|         New|\n",
            "|       103| Ravi|Bangalore|2021-11-01|       Loyal|\n",
            "|       104|Sneha|Hyderabad|2020-07-22|       Loyal|\n",
            "|       105| Amit|  Chennai|2023-03-10|         New|\n",
            "+----------+-----+---------+----------+------------+\n",
            "\n",
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+---------+---------+\n",
            "|OrderID|CustomerID|  Product|   Category|Quantity|  Price| OrderDate|TotalAmount|OrderYear|OrderType|\n",
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+---------+---------+\n",
            "|      1|       101|   Laptop|Electronics|       2|50000.0|2024-01-10|   100000.0|     2024|     High|\n",
            "|      2|       101|    Mouse|Electronics|       1| 1200.0|2024-01-15|     1200.0|     2024|      Low|\n",
            "|      3|       102|   Tablet|Electronics|       1|20000.0|2024-02-01|    20000.0|     2024|     High|\n",
            "|      4|       103|Bookshelf|  Furniture|       1| 3500.0|2024-02-10|     3500.0|     2024|      Low|\n",
            "|      5|       104|    Mixer| Appliances|       1| 5000.0|2024-02-15|     5000.0|     2024|     High|\n",
            "|      6|       105| Notebook| Stationery|       5|  500.0|2024-03-01|     2500.0|     2024|      Low|\n",
            "|      7|       102|    Phone|Electronics|       1|30000.0|2024-03-02|    30000.0|     2024|     High|\n",
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 5-Spark SQL Tasks\n",
        "# Create Database sales and tables\n",
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS sales\")\n",
        "spark.sql(\"USE sales\")\n",
        "customers.write.mode(\"overwrite\").saveAsTable(\"sales.customers\")\n",
        "orders.write.mode(\"overwrite\").saveAsTable(\"sales.orders\")\n",
        "\n",
        "# 1. List of Orders by Delhi customers\n",
        "list_delhi = spark.sql(\"\"\"\n",
        "SELECT o.* FROM orders o\n",
        "JOIN customers c ON o.CustomerID = c.CustomerID\n",
        "WHERE c.City = 'Delhi'\n",
        "\"\"\")\n",
        "list_delhi.show()\n",
        "\n",
        "# 2. Average order value per category\n",
        "avg_order_value = spark.sql(\"\"\"\n",
        "SELECT Category, AVG(Price * Quantity) as AvgOrderValue\n",
        "FROM orders GROUP BY Category\n",
        "\"\"\")\n",
        "avg_order_value.show()\n",
        "\n",
        "# 3. Monthly orders view\n",
        "spark.sql(\"\"\"\n",
        "CREATE OR REPLACE TEMP VIEW monthly_orders AS\n",
        "SELECT MONTH(OrderDate) as Month, SUM(Price * Quantity) as TotalAmount\n",
        "FROM orders GROUP BY MONTH(OrderDate)\n",
        "\"\"\")\n",
        "spark.sql(\"SELECT * FROM monthly_orders\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34d7rLH4O-tx",
        "outputId": "6cd0d3da-3605-4818-a447-aa70bda4e8a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+-------+--------+--------+-----+---------+-----------+---------+---------+\n",
            "|OrderID|CustomerID|Product|Category|Quantity|Price|OrderDate|TotalAmount|OrderYear|OrderType|\n",
            "+-------+----------+-------+--------+--------+-----+---------+-----------+---------+---------+\n",
            "+-------+----------+-------+--------+--------+-----+---------+-----------+---------+---------+\n",
            "\n",
            "+-----------+-------------+\n",
            "|   Category|AvgOrderValue|\n",
            "+-----------+-------------+\n",
            "| Stationery|       2500.0|\n",
            "|Electronics|      37800.0|\n",
            "|  Furniture|       3500.0|\n",
            "| Appliances|       5000.0|\n",
            "+-----------+-------------+\n",
            "\n",
            "+-----+-----------+\n",
            "|Month|TotalAmount|\n",
            "+-----+-----------+\n",
            "|    1|   101200.0|\n",
            "|    3|    32500.0|\n",
            "|    2|    28500.0|\n",
            "+-----+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 6-String & Date Functions\n",
        "from pyspark.sql.functions import regexp_extract, concat_ws, datediff, current_date, month, date_format, concat, lit\n",
        "\n",
        "# Mask emails using regex (e.g., a***@gmail.com )\n",
        "first_char = regexp_extract(\"Email\", r\"^(.).*@\", 1)\n",
        "domain = regexp_extract(\"Email\", r\"(@.*)\", 1)\n",
        "masked = customers.withColumn(\"MaskedEmail\", concat(first_char, lit(\"***\"), domain))\n",
        "masked.select(\"Email\", \"MaskedEmail\").show(truncate=False)\n",
        "\n",
        "# Concatenate Name and City as “Name from City”\n",
        "customers = customers.withColumn(\"Summary\", concat_ws(\" from \", \"Name\", \"City\"))\n",
        "customers.show()\n",
        "\n",
        "# Use datediff() to calculate customer age in days\n",
        "customers = customers.withColumn(\"AgeInDays\", datediff(current_date(), to_date(\"SignupDate\")))\n",
        "customers.show()\n",
        "\n",
        "# Extract month name from OrderDate\n",
        "orders = orders.withColumn(\"MonthName\", date_format(\"OrderDate\", \"MMMM\"))\n",
        "orders.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a382H2n3ZDrX",
        "outputId": "29d07f89-4947-4524-e69a-6110837ad1ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+----------------+\n",
            "|Email            |MaskedEmail     |\n",
            "+-----------------+----------------+\n",
            "|ali@gmail.com    |a***@gmail.com  |\n",
            "|neha@yahoo.com   |n***@yahoo.com  |\n",
            "|ravi@hotmail.com |r***@hotmail.com|\n",
            "|sneha@outlook.com|s***@outlook.com|\n",
            "|amit@gmail.com   |a***@gmail.com  |\n",
            "+-----------------+----------------+\n",
            "\n",
            "+----------+-----+-----------------+---------+----------+--------------------+---------+\n",
            "|CustomerID| Name|            Email|     City|SignupDate|             Summary|AgeInDays|\n",
            "+----------+-----+-----------------+---------+----------+--------------------+---------+\n",
            "|       101|  Ali|    ali@gmail.com|   Mumbai|2022-05-10|     Ali from Mumbai|     1126|\n",
            "|       102| Neha|   neha@yahoo.com|    Delhi|2023-01-15|     Neha from Delhi|      876|\n",
            "|       103| Ravi| ravi@hotmail.com|Bangalore|2021-11-01| Ravi from Bangalore|     1316|\n",
            "|       104|Sneha|sneha@outlook.com|Hyderabad|2020-07-22|Sneha from Hyderabad|     1783|\n",
            "|       105| Amit|   amit@gmail.com|  Chennai|2023-03-10|   Amit from Chennai|      822|\n",
            "+----------+-----+-----------------+---------+----------+--------------------+---------+\n",
            "\n",
            "+----------+-----+-----------------+---------+----------+--------------------+---------+\n",
            "|CustomerID| Name|            Email|     City|SignupDate|             Summary|AgeInDays|\n",
            "+----------+-----+-----------------+---------+----------+--------------------+---------+\n",
            "|       101|  Ali|    ali@gmail.com|   Mumbai|2022-05-10|     Ali from Mumbai|     1126|\n",
            "|       102| Neha|   neha@yahoo.com|    Delhi|2023-01-15|     Neha from Delhi|      876|\n",
            "|       103| Ravi| ravi@hotmail.com|Bangalore|2021-11-01| Ravi from Bangalore|     1316|\n",
            "|       104|Sneha|sneha@outlook.com|Hyderabad|2020-07-22|Sneha from Hyderabad|     1783|\n",
            "|       105| Amit|   amit@gmail.com|  Chennai|2023-03-10|   Amit from Chennai|      822|\n",
            "+----------+-----+-----------------+---------+----------+--------------------+---------+\n",
            "\n",
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+---------+---------+---------+\n",
            "|OrderID|CustomerID|  Product|   Category|Quantity|  Price| OrderDate|TotalAmount|OrderYear|OrderType|MonthName|\n",
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+---------+---------+---------+\n",
            "|      1|       101|   Laptop|Electronics|       2|50000.0|2024-01-10|   100000.0|     2024|     High|  January|\n",
            "|      2|       101|    Mouse|Electronics|       1| 1200.0|2024-01-15|     1200.0|     2024|      Low|  January|\n",
            "|      3|       102|   Tablet|Electronics|       1|20000.0|2024-02-01|    20000.0|     2024|     High| February|\n",
            "|      4|       103|Bookshelf|  Furniture|       1| 3500.0|2024-02-10|     3500.0|     2024|      Low| February|\n",
            "|      5|       104|    Mixer| Appliances|       1| 5000.0|2024-02-15|     5000.0|     2024|     High| February|\n",
            "|      6|       105| Notebook| Stationery|       5|  500.0|2024-03-01|     2500.0|     2024|      Low|    March|\n",
            "|      7|       102|    Phone|Electronics|       1|30000.0|2024-03-02|    30000.0|     2024|     High|    March|\n",
            "+-------+----------+---------+-----------+--------+-------+----------+-----------+---------+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 7- UDFs and Complex Logic\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# 1. Tagging customers\n",
        "def tag_customer(spend):\n",
        "    if spend > 50000:\n",
        "        return \"Gold\"\n",
        "    elif spend >= 10000:\n",
        "        return \"Silver\"\n",
        "    return \"Bronze\"\n",
        "\n",
        "tag_udf = udf(tag_customer, StringType())\n",
        "\n",
        "# Join and aggregate spend\n",
        "spend_df = joined.groupBy(\"CustomerID\", \"Name\").agg(_sum(\"TotalAmount\").alias(\"Spend\"))\n",
        "spend_df = spend_df.withColumn(\"Tier\", tag_udf(col(\"Spend\")))\n",
        "spend_df.show()\n",
        "\n",
        "# 2. Shorten product names\n",
        "def short_name(name):\n",
        "    return name[:3] + \"...\" if name else \"\"\n",
        "\n",
        "short_udf = udf(short_name, StringType())\n",
        "orders = orders.withColumn(\"ShortProduct\", short_udf(col(\"Product\")))\n",
        "orders.select(\"Product\", \"ShortProduct\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fbhyP80b6A3",
        "outputId": "edd5c4d2-3fad-45d3-ea3e-278d3f1a96f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+--------+------+\n",
            "|CustomerID| Name|   Spend|  Tier|\n",
            "+----------+-----+--------+------+\n",
            "|       105| Amit|  2500.0|Bronze|\n",
            "|       104|Sneha|  5000.0|Bronze|\n",
            "|       101|  Ali|101200.0|  Gold|\n",
            "|       102| Neha| 50000.0|Silver|\n",
            "|       103| Ravi|  3500.0|Bronze|\n",
            "+----------+-----+--------+------+\n",
            "\n",
            "+---------+------------+\n",
            "|  Product|ShortProduct|\n",
            "+---------+------------+\n",
            "|   Laptop|      Lap...|\n",
            "|    Mouse|      Mou...|\n",
            "|   Tablet|      Tab...|\n",
            "|Bookshelf|      Boo...|\n",
            "|    Mixer|      Mix...|\n",
            "| Notebook|      Not...|\n",
            "|    Phone|      Pho...|\n",
            "+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 8- Parquet & Views\n",
        "import time\n",
        "\n",
        "# Save the joined result as a Parquet file.\n",
        "joined.write.mode(\"overwrite\").parquet(\"/content/joined_data.parquet\")\n",
        "\n",
        "# Read it back and verify schema.\n",
        "parquet_df = spark.read.parquet(\"/content/joined_data.parquet\")\n",
        "parquet_df.printSchema()\n",
        "\n",
        "# Create and query a global temp view.\n",
        "parquet_df.createOrReplaceGlobalTempView(\"global_joined\")\n",
        "spark.sql(\"SELECT * FROM global_temp.global_joined LIMIT 5\").show()\n",
        "\n",
        "# Compare performance between CSV read and Parquet read.\n",
        "start = time.time()\n",
        "spark.read.csv(\"orders.csv\", header=True, inferSchema=True).count()\n",
        "print(\"CSV read time:\", time.time() - start)\n",
        "\n",
        "start = time.time()\n",
        "spark.read.parquet(\"/content/joined_data.parquet\").count()\n",
        "print(\"Parquet read time:\", time.time() - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DOz3DAhOVn9",
        "outputId": "f2ab7a85-ca1a-449e-9984-8e0c240e071f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- CustomerID: integer (nullable = true)\n",
            " |-- OrderID: integer (nullable = true)\n",
            " |-- Product: string (nullable = true)\n",
            " |-- Category: string (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- Price: double (nullable = true)\n",
            " |-- OrderDate: date (nullable = true)\n",
            " |-- TotalAmount: double (nullable = true)\n",
            " |-- OrderYear: integer (nullable = true)\n",
            " |-- OrderType: string (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- SignupDate: date (nullable = true)\n",
            " |-- CustomerType: string (nullable = true)\n",
            "\n",
            "+----------+-------+---------+-----------+--------+-------+----------+-----------+---------+---------+-----+---------+----------+------------+\n",
            "|CustomerID|OrderID|  Product|   Category|Quantity|  Price| OrderDate|TotalAmount|OrderYear|OrderType| Name|     City|SignupDate|CustomerType|\n",
            "+----------+-------+---------+-----------+--------+-------+----------+-----------+---------+---------+-----+---------+----------+------------+\n",
            "|       101|      1|   Laptop|Electronics|       2|50000.0|2024-01-10|   100000.0|     2024|     High|  Ali|   Mumbai|2022-05-10|         New|\n",
            "|       101|      2|    Mouse|Electronics|       1| 1200.0|2024-01-15|     1200.0|     2024|      Low|  Ali|   Mumbai|2022-05-10|         New|\n",
            "|       102|      3|   Tablet|Electronics|       1|20000.0|2024-02-01|    20000.0|     2024|     High| Neha|  Unknown|2023-01-15|         New|\n",
            "|       103|      4|Bookshelf|  Furniture|       1| 3500.0|2024-02-10|     3500.0|     2024|      Low| Ravi|Bangalore|2021-11-01|       Loyal|\n",
            "|       104|      5|    Mixer| Appliances|       1| 5000.0|2024-02-15|     5000.0|     2024|     High|Sneha|Hyderabad|2020-07-22|       Loyal|\n",
            "+----------+-------+---------+-----------+--------+-------+----------+-----------+---------+---------+-----+---------+----------+------------+\n",
            "\n",
            "CSV read time: 0.8827962875366211\n",
            "Parquet read time: 0.5064787864685059\n"
          ]
        }
      ]
    }
  ]
}