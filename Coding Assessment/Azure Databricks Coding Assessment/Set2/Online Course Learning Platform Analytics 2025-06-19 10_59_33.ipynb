{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccaff63a-4a63-496b-8bbd-54f02f1ba824",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=4054280922637182#setting/sparkui/0611-041854-oedbfkos/driver-4507201859227434482\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7eb4063a0d90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c754bcd2-5bc4-4309-98ce-7c35ac7243fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- EnrollID: string (nullable = true)\n |-- UserID: string (nullable = true)\n |-- CourseID: string (nullable = true)\n |-- CourseName: string (nullable = true)\n |-- Category: string (nullable = true)\n |-- EnrollDate: string (nullable = true)\n |-- CompletionDate: date (nullable = true)\n |-- ProgressPercent: integer (nullable = true)\n |-- Rating: integer (nullable = true)\n\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n|EnrollID|UserID|CourseID|       CourseName|    Category|EnrollDate|CompletionDate|ProgressPercent|Rating|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n|    E001|  U001|    C001|    Python Basics| Programming|01-04-2024|    2024-04-10|            100|     4|\n|    E002|  U002|    C002|Excel for Finance|Productivity|02-04-2024|          NULL|             45|  NULL|\n|    E003|  U003|    C003|  ML with PySpark|Data Science|03-04-2024|          NULL|             30|  NULL|\n|    E004|  U004|    C001|    Python Basics| Programming|04-04-2024|    2024-04-20|            100|     5|\n|    E005|  U005|    C004|Digital Marketing|   Marketing|05-04-2024|    2024-04-16|            100|     4|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n|EnrollID|UserID|CourseID|       CourseName|    Category|EnrollDate|CompletionDate|ProgressPercent|Rating|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n|    E001|  U001|    C001|    Python Basics| Programming|2024-04-01|    2024-04-10|            100|     4|\n|    E002|  U002|    C002|Excel for Finance|Productivity|2024-04-02|          NULL|             45|  NULL|\n|    E003|  U003|    C003|  ML with PySpark|Data Science|2024-04-03|          NULL|             30|  NULL|\n|    E004|  U004|    C001|    Python Basics| Programming|2024-04-04|    2024-04-20|            100|     5|\n|    E005|  U005|    C004|Digital Marketing|   Marketing|2024-04-05|    2024-04-16|            100|     4|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n\nroot\n |-- EnrollID: string (nullable = true)\n |-- UserID: string (nullable = true)\n |-- CourseID: string (nullable = true)\n |-- CourseName: string (nullable = true)\n |-- Category: string (nullable = true)\n |-- EnrollDate: date (nullable = true)\n |-- CompletionDate: date (nullable = true)\n |-- ProgressPercent: integer (nullable = true)\n |-- Rating: integer (nullable = true)\n\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+--------------+\n|EnrollID|UserID|CourseID|       CourseName|    Category|EnrollDate|CompletionDate|ProgressPercent|Rating|DaysToComplete|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+--------------+\n|    E001|  U001|    C001|    Python Basics| Programming|2024-04-01|    2024-04-10|            100|     4|             9|\n|    E002|  U002|    C002|Excel for Finance|Productivity|2024-04-02|          NULL|             45|  NULL|          NULL|\n|    E003|  U003|    C003|  ML with PySpark|Data Science|2024-04-03|          NULL|             30|  NULL|          NULL|\n|    E004|  U004|    C001|    Python Basics| Programming|2024-04-04|    2024-04-20|            100|     5|            16|\n|    E005|  U005|    C004|Digital Marketing|   Marketing|2024-04-05|    2024-04-16|            100|     4|            11|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Ingestion & Time Fields\n",
    "# Load into PySpark with inferred schema\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"file:/Workspace/Shared/Coding Assessment Datasets/course_enrollments.csv\")\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "# Convert EnrollDate and CompletionDate to date type\n",
    "from pyspark.sql.functions import to_date, datediff\n",
    "\n",
    "df = df.withColumn(\"EnrollDate\", to_date(\"EnrollDate\", \"dd-MM-yyyy\")) \\\n",
    "       .withColumn(\"CompletionDate\", to_date(\"CompletionDate\", \"dd-MM-yyyy\"))\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "# Add DaysToComplete column if completed\n",
    "df = df.withColumn(\"DaysToComplete\", datediff(\"CompletionDate\", \"EnrollDate\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeeea2a7-520e-4ec7-b816-9ddb406c69be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+------------------+\n|UserID|CoursesEnrolled|AvgProgressPercent|\n+------+---------------+------------------+\n|  U004|              1|             100.0|\n|  U005|              1|             100.0|\n|  U002|              1|              45.0|\n|  U003|              1|              30.0|\n|  U001|              1|             100.0|\n+------+---------------+------------------+\n\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+--------------+-----------+\n|EnrollID|UserID|CourseID|       CourseName|    Category|EnrollDate|CompletionDate|ProgressPercent|Rating|DaysToComplete|IsCompleted|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+--------------+-----------+\n|    E001|  U001|    C001|    Python Basics| Programming|2024-04-01|    2024-04-10|            100|     4|             9|       true|\n|    E002|  U002|    C002|Excel for Finance|Productivity|2024-04-02|          NULL|             45|  NULL|          NULL|      false|\n|    E003|  U003|    C003|  ML with PySpark|Data Science|2024-04-03|          NULL|             30|  NULL|          NULL|      false|\n|    E004|  U004|    C001|    Python Basics| Programming|2024-04-04|    2024-04-20|            100|     5|            16|       true|\n|    E005|  U005|    C004|Digital Marketing|   Marketing|2024-04-05|    2024-04-16|            100|     4|            11|       true|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+--------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 2.User Learning Path Progress\n",
    "from pyspark.sql.functions import col, avg, count, expr, when\n",
    "# Group by UserID : count of courses enrolled\n",
    "# Avg progress % across all enrollments\n",
    "user_progress = df.groupBy(\"UserID\").agg(\n",
    "    count(\"CourseID\").alias(\"CoursesEnrolled\"),\n",
    "    avg(\"ProgressPercent\").alias(\"AvgProgressPercent\")\n",
    ")\n",
    "\n",
    "user_progress.show()\n",
    "\n",
    "# Flag IsCompleted = ProgressPercent = 100\n",
    "df = df.withColumn(\"IsCompleted\", when(col(\"ProgressPercent\") == 100, True).otherwise(False)) \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cf234e7-8b0e-4db5-ae2a-3741352db8be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+--------------+-----------+\n|EnrollID|UserID|CourseID|       CourseName|    Category|EnrollDate|CompletionDate|ProgressPercent|Rating|DaysToComplete|IsCompleted|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+--------------+-----------+\n|    E001|  U001|    C001|    Python Basics| Programming|2024-04-01|    2024-04-10|            100|     4|             9|       true|\n|    E002|  U002|    C002|Excel for Finance|Productivity|2024-04-02|          NULL|             45|     0|          NULL|      false|\n|    E003|  U003|    C003|  ML with PySpark|Data Science|2024-04-03|          NULL|             30|     0|          NULL|      false|\n|    E004|  U004|    C001|    Python Basics| Programming|2024-04-04|    2024-04-20|            100|     5|            16|       true|\n|    E005|  U005|    C004|Digital Marketing|   Marketing|2024-04-05|    2024-04-16|            100|     4|            11|       true|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+--------------+-----------+\n\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+--------------+-----------+---------------+\n|EnrollID|UserID|CourseID|       CourseName|    Category|EnrollDate|CompletionDate|ProgressPercent|Rating|DaysToComplete|IsCompleted|EngagementScore|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+--------------+-----------+---------------+\n|    E001|  U001|    C001|    Python Basics| Programming|2024-04-01|    2024-04-10|            100|     4|             9|       true|            400|\n|    E002|  U002|    C002|Excel for Finance|Productivity|2024-04-02|          NULL|             45|     0|          NULL|      false|              0|\n|    E003|  U003|    C003|  ML with PySpark|Data Science|2024-04-03|          NULL|             30|     0|          NULL|      false|              0|\n|    E004|  U004|    C001|    Python Basics| Programming|2024-04-04|    2024-04-20|            100|     5|            16|       true|            500|\n|    E005|  U005|    C004|Digital Marketing|   Marketing|2024-04-05|    2024-04-16|            100|     4|            11|       true|            400|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+--------------+-----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 3. Engagement Scoring\n",
    "# Create a score: ProgressPercent * Rating (if not null)\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "#Replace null Rating with 0\n",
    "df = df.withColumn(\"Rating\", when(col(\"Rating\").isNull(), 0).otherwise(col(\"Rating\")))\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn(\"EngagementScore\", col(\"ProgressPercent\") * col(\"Rating\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1554b84a-66e8-4af3-8c21-578bae6910f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+--------------+-----------+---------------+\n|EnrollID|UserID|CourseID|       CourseName|    Category|EnrollDate|CompletionDate|ProgressPercent|Rating|DaysToComplete|IsCompleted|EngagementScore|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+--------------+-----------+---------------+\n|    E002|  U002|    C002|Excel for Finance|Productivity|2024-04-02|          NULL|             45|     0|          NULL|      false|              0|\n|    E003|  U003|    C003|  ML with PySpark|Data Science|2024-04-03|          NULL|             30|     0|          NULL|      false|              0|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+--------------+-----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 4.Identify Drop-offs\n",
    "# Filter all records with ProgressPercent < 50 and CompletionDate is null\n",
    "dropouts_df = df.filter(\n",
    "    (col(\"ProgressPercent\") < 50) &\n",
    "    (col(\"CompletionDate\").isNull())\n",
    ")\n",
    "\n",
    "# Create a view called Dropouts\n",
    "dropouts_df.createOrReplaceTempView(\"Dropouts\")\n",
    "\n",
    "#view content\n",
    "spark.sql(\"SELECT * FROM Dropouts\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ea23cc4-1014-4e30-b967-81cfd617a52e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Joins with Metadata\n",
    "# Create course_catalog.csv :\n",
    "# CourseID,Instructor,DurationHours,Level\n",
    "# C001,Abdullah Khan,8,Beginner\n",
    "# C002,Sana Gupta,5,Beginner\n",
    "# C003,Ibrahim Khan,10,Intermediate\n",
    "# C004,Zoya Sheikh,6,Beginner\n",
    "\n",
    "catalog_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"file:/Workspace/Shared/Coding Assessment Datasets/course_catlog.csv\")\n",
    "catalog_df.createOrReplaceTempView(\"course_catalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "404817ad-62e3-47a0-8eb4-3c9102ece5b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n|   Instructor|AvgProgress|\n+-------------+-----------+\n|Abdullah Khan|      100.0|\n|  Zoya Sheikh|      100.0|\n|   Sana Gupta|       45.0|\n|Inbrahim Khan|       30.0|\n+-------------+-----------+\n\n+--------+-------------+----------------+\n|CourseID|   Instructor|TotalEnrollments|\n+--------+-------------+----------------+\n|    C001|Abdullah Khan|               2|\n+--------+-------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"enrollments\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    c.Instructor, \n",
    "    ROUND(AVG(e.ProgressPercent), 2) AS AvgProgress\n",
    "FROM enrollments e\n",
    "JOIN course_catalog c \n",
    "  ON e.CourseID = c.CourseID\n",
    "GROUP BY c.Instructor\n",
    "ORDER BY AvgProgress DESC\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    e.CourseID, \n",
    "    c.Instructor,\n",
    "    COUNT(*) AS TotalEnrollments\n",
    "FROM enrollments e\n",
    "JOIN course_catalog c \n",
    "  ON e.CourseID = c.CourseID\n",
    "GROUP BY e.CourseID, c.Instructor\n",
    "ORDER BY TotalEnrollments DESC\n",
    "LIMIT 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13cacdb7-8322-4b30-99c3-9a957bf0999d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n|EnrollID|UserID|CourseID|       CourseName|    Category|EnrollDate|CompletionDate|ProgressPercent|Rating|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n|    E001|  U001|    C001|    Python Basics| Programming|01-04-2024|    2024-04-10|            100|     4|\n|    E002|  U002|    C002|Excel for Finance|Productivity|02-04-2024|          NULL|             45|  NULL|\n|    E003|  U003|    C003|  ML with PySpark|Data Science|03-04-2024|          NULL|             30|  NULL|\n|    E004|  U004|    C001|    Python Basics| Programming|04-04-2024|    2024-04-20|            100|     5|\n|    E005|  U005|    C004|Digital Marketing|   Marketing|05-04-2024|    2024-04-16|            100|     4|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n\n+-----------------+\n|num_affected_rows|\n+-----------------+\n|                2|\n+-----------------+\n\n+-----------------+\n|num_affected_rows|\n+-----------------+\n|                0|\n+-----------------+\n\n+-------+-------------------+----------------+----------------------------------+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----+-----------------+--------------------+-----------+-----------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|version|timestamp          |userId          |userName                          |operation                        |operationParameters                                                                                                                                     |job |notebook         |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                          |userMetadata|engineInfo                                |\n+-------+-------------------+----------------+----------------------------------+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----+-----------------+--------------------+-----------+-----------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|7      |2025-06-19 07:18:35|3955481281677681|azuser3561_mml.local@techademy.com|OPTIMIZE                         |{predicate -> [], auto -> true, clusterBy -> [], zOrderBy -> [], batchId -> 0}                                                                          |NULL|{108503599691472}|0611-041854-oedbfkos|5          |SnapshotIsolation|false        |{numRemovedFiles -> 2, numRemovedBytes -> 5086, p25FileSize -> 2640, numDeletionVectorsRemoved -> 1, conflictDetectionTimeMs -> 25, minFileSize -> 2640, numAddedFiles -> 1, maxFileSize -> 2640, p75FileSize -> 2640, p50FileSize -> 2640, numAddedBytes -> 2640}                                                        |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|6      |2025-06-19 07:18:34|3955481281677681|azuser3561_mml.local@techademy.com|DELETE                           |{predicate -> [\"(ProgressPercent#19368 = 0)\"]}                                                                                                          |NULL|{108503599691472}|0611-041854-oedbfkos|5          |WriteSerializable|false        |{numRemovedFiles -> 0, numRemovedBytes -> 0, numCopiedRows -> 0, numDeletionVectorsAdded -> 0, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 181, numDeletionVectorsUpdated -> 0, numDeletedRows -> 0, scanTimeMs -> 180, numAddedFiles -> 0, numAddedBytes -> 0, rewriteTimeMs -> 0}      |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|5      |2025-06-19 07:18:33|3955481281677681|azuser3561_mml.local@techademy.com|UPDATE                           |{predicate -> [\"(CourseName#18542 = Python Basics)\"]}                                                                                                   |NULL|{108503599691472}|0611-041854-oedbfkos|4          |WriteSerializable|false        |{numRemovedFiles -> 0, numRemovedBytes -> 0, numCopiedRows -> 0, numDeletionVectorsAdded -> 1, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 1132, numDeletionVectorsUpdated -> 0, scanTimeMs -> 516, numAddedFiles -> 1, numUpdatedRows -> 2, numAddedBytes -> 2448, rewriteTimeMs -> 607}|NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|4      |2025-06-19 07:18:30|3955481281677681|azuser3561_mml.local@techademy.com|CREATE OR REPLACE TABLE AS SELECT|{partitionBy -> [], clusterBy -> [], description -> NULL, isManaged -> true, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> false}|NULL|{108503599691472}|0611-041854-oedbfkos|3          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2638}                                                                                                                                                                                                                                                               |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|3      |2025-06-19 05:57:30|3955481281677681|azuser3561_mml.local@techademy.com|OPTIMIZE                         |{predicate -> [], auto -> true, clusterBy -> [], zOrderBy -> [], batchId -> 0}                                                                          |NULL|{108503599691472}|0611-041854-oedbfkos|1          |SnapshotIsolation|false        |{numRemovedFiles -> 2, numRemovedBytes -> 5008, p25FileSize -> 2594, numDeletionVectorsRemoved -> 1, conflictDetectionTimeMs -> 20, minFileSize -> 2594, numAddedFiles -> 1, maxFileSize -> 2594, p75FileSize -> 2594, p50FileSize -> 2594, numAddedBytes -> 2594}                                                        |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|2      |2025-06-19 05:57:28|3955481281677681|azuser3561_mml.local@techademy.com|DELETE                           |{predicate -> [\"(ProgressPercent#11424 = 0)\"]}                                                                                                          |NULL|{108503599691472}|0611-041854-oedbfkos|1          |WriteSerializable|false        |{numRemovedFiles -> 0, numRemovedBytes -> 0, numCopiedRows -> 0, numDeletionVectorsAdded -> 0, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 212, numDeletionVectorsUpdated -> 0, numDeletedRows -> 0, scanTimeMs -> 212, numAddedFiles -> 0, numAddedBytes -> 0, rewriteTimeMs -> 0}      |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|1      |2025-06-19 05:57:27|3955481281677681|azuser3561_mml.local@techademy.com|UPDATE                           |{predicate -> [\"(CourseName#10485 = Python Basics)\"]}                                                                                                   |NULL|{108503599691472}|0611-041854-oedbfkos|0          |WriteSerializable|false        |{numRemovedFiles -> 0, numRemovedBytes -> 0, numCopiedRows -> 0, numDeletionVectorsAdded -> 1, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 1349, numDeletionVectorsUpdated -> 0, scanTimeMs -> 625, numAddedFiles -> 1, numUpdatedRows -> 2, numAddedBytes -> 2416, rewriteTimeMs -> 722}|NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|0      |2025-06-19 05:57:24|3955481281677681|azuser3561_mml.local@techademy.com|CREATE OR REPLACE TABLE AS SELECT|{partitionBy -> [], clusterBy -> [], description -> NULL, isManaged -> true, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> false}|NULL|{108503599691472}|0611-041854-oedbfkos|NULL       |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2592}                                                                                                                                                                                                                                                               |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n+-------+-------------------+----------------+----------------------------------+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----+-----------------+--------------------+-----------+-----------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 6. Delta Lake Practice\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"file:/Workspace/Shared/Coding Assessment Datasets/course_enrollments.csv\")\n",
    "df.show()\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "df = df.withColumn(\"EnrollDate\", to_date(\"EnrollDate\", \"dd-MM-yyyy\")) \\\n",
    "       .withColumn(\"CompletionDate\", to_date(\"CompletionDate\", \"dd-MM-yyyy\"))\n",
    "\n",
    "# Save as Delta Table enrollments_delta\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"enrollments_delta\")\n",
    "\n",
    "# Update: Set all ratings to 5 where Course = 'Python Basics'\n",
    "spark.sql(\"\"\"\n",
    "UPDATE enrollments_delta\n",
    "SET Rating = 5\n",
    "WHERE CourseName = 'Python Basics'\n",
    "\"\"\").show()\n",
    "\n",
    "# Delete: All rows where ProgressPercent = 0\n",
    "spark.sql(\"\"\"\n",
    "DELETE FROM enrollments_delta\n",
    "WHERE ProgressPercent = 0\n",
    "\"\"\").show()\n",
    "\n",
    "# Show DESCRIBE HISTORY\n",
    "spark.sql(\"DESCRIBE HISTORY enrollments_delta\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2afa9e3d-c3cd-46a3-9730-0c2519243238",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+----------------+----+\n|CourseID|       CourseName|TotalEnrollments|Rank|\n+--------+-----------------+----------------+----+\n|    C001|    Python Basics|               2|   1|\n|    C004|Digital Marketing|               1|   2|\n|    C002|Excel for Finance|               1|   2|\n|    C003|  ML with PySpark|               1|   2|\n+--------+-----------------+----------------+----+\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[UserID: string, CourseName: string, EnrollDate: date, NextCourse: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Window Functions\n",
    "# Use dense_rank() to rank courses by number of enrollments\n",
    "df = spark.read.format(\"delta\").table(\"enrollments_delta\")\n",
    "\n",
    "from pyspark.sql.functions import count, dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Count enrollments per course\n",
    "course_counts = df.groupBy(\"CourseID\", \"CourseName\").agg(count(\"*\").alias(\"TotalEnrollments\"))\n",
    "\n",
    "# dense rank window\n",
    "rank_window = Window.orderBy(course_counts[\"TotalEnrollments\"].desc())\n",
    "\n",
    "ranked_courses = course_counts.withColumn(\"Rank\", dense_rank().over(rank_window))\n",
    "\n",
    "ranked_courses.show()\n",
    "\n",
    "# lead() to find next course by each user (sorted by EnrollDate)\n",
    "from pyspark.sql.functions import lead\n",
    "from pyspark.sql.window import Window\n",
    "user_course_window = Window.partitionBy(\"UserID\").orderBy(\"EnrollDate\")\n",
    "df_with_next = df.withColumn(\"NextCourse\", lead(\"CourseName\").over(user_course_window))\n",
    "df_with_next.select(\"UserID\", \"CourseName\", \"EnrollDate\", \"NextCourse\").orderBy(\"UserID\", \"EnrollDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49c9010b-3288-45db-91bb-594ff78efd4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+\n|EnrollDate|TotalEnrollments|\n+----------+----------------+\n|      NULL|               5|\n+----------+----------------+\n\n+------------+---------+\n|    Category|AvgRating|\n+------------+---------+\n| Programming|      5.0|\n|Productivity|     NULL|\n|   Marketing|      4.0|\n|Data Science|     NULL|\n+------------+---------+\n\n+-----------------+----------------+\n|       CourseName|TotalEnrollments|\n+-----------------+----------------+\n|    Python Basics|               2|\n|Digital Marketing|               1|\n|Excel for Finance|               1|\n+-----------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 8. SQL Logic for Dashboard Views\n",
    "# Create views:\n",
    "# daily_enrollments\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW daily_enrollments AS\n",
    "SELECT \n",
    "  EnrollDate,\n",
    "  COUNT(*) AS TotalEnrollments\n",
    "FROM enrollments_delta\n",
    "GROUP BY EnrollDate\n",
    "ORDER BY EnrollDate\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM daily_enrollments\").show()\n",
    "\n",
    "# category_performance (avg rating by category)\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW category_performance AS\n",
    "SELECT \n",
    "  Category,\n",
    "  ROUND(AVG(Rating), 2) AS AvgRating\n",
    "FROM enrollments_delta\n",
    "GROUP BY Category\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM category_performance\").show()\n",
    "\n",
    "# top_3_courses\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW top_3_courses AS\n",
    "SELECT \n",
    "  CourseName,\n",
    "  COUNT(*) AS TotalEnrollments\n",
    "FROM enrollments_delta\n",
    "GROUP BY CourseName\n",
    "ORDER BY TotalEnrollments DESC\n",
    "LIMIT 3\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM top_3_courses\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86e32b48-e20a-4e49-9d5b-dcd2f7b20135",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+----------------------------------+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----+-----------------+--------------------+-----------+-----------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|version|timestamp          |userId          |userName                          |operation                        |operationParameters                                                                                                                                     |job |notebook         |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                          |userMetadata|engineInfo                                |\n+-------+-------------------+----------------+----------------------------------+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----+-----------------+--------------------+-----------+-----------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|7      |2025-06-19 07:18:35|3955481281677681|azuser3561_mml.local@techademy.com|OPTIMIZE                         |{predicate -> [], auto -> true, clusterBy -> [], zOrderBy -> [], batchId -> 0}                                                                          |NULL|{108503599691472}|0611-041854-oedbfkos|5          |SnapshotIsolation|false        |{numRemovedFiles -> 2, numRemovedBytes -> 5086, p25FileSize -> 2640, numDeletionVectorsRemoved -> 1, conflictDetectionTimeMs -> 25, minFileSize -> 2640, numAddedFiles -> 1, maxFileSize -> 2640, p75FileSize -> 2640, p50FileSize -> 2640, numAddedBytes -> 2640}                                                        |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|6      |2025-06-19 07:18:34|3955481281677681|azuser3561_mml.local@techademy.com|DELETE                           |{predicate -> [\"(ProgressPercent#19368 = 0)\"]}                                                                                                          |NULL|{108503599691472}|0611-041854-oedbfkos|5          |WriteSerializable|false        |{numRemovedFiles -> 0, numRemovedBytes -> 0, numCopiedRows -> 0, numDeletionVectorsAdded -> 0, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 181, numDeletionVectorsUpdated -> 0, numDeletedRows -> 0, scanTimeMs -> 180, numAddedFiles -> 0, numAddedBytes -> 0, rewriteTimeMs -> 0}      |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|5      |2025-06-19 07:18:33|3955481281677681|azuser3561_mml.local@techademy.com|UPDATE                           |{predicate -> [\"(CourseName#18542 = Python Basics)\"]}                                                                                                   |NULL|{108503599691472}|0611-041854-oedbfkos|4          |WriteSerializable|false        |{numRemovedFiles -> 0, numRemovedBytes -> 0, numCopiedRows -> 0, numDeletionVectorsAdded -> 1, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 1132, numDeletionVectorsUpdated -> 0, scanTimeMs -> 516, numAddedFiles -> 1, numUpdatedRows -> 2, numAddedBytes -> 2448, rewriteTimeMs -> 607}|NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|4      |2025-06-19 07:18:30|3955481281677681|azuser3561_mml.local@techademy.com|CREATE OR REPLACE TABLE AS SELECT|{partitionBy -> [], clusterBy -> [], description -> NULL, isManaged -> true, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> false}|NULL|{108503599691472}|0611-041854-oedbfkos|3          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2638}                                                                                                                                                                                                                                                               |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|3      |2025-06-19 05:57:30|3955481281677681|azuser3561_mml.local@techademy.com|OPTIMIZE                         |{predicate -> [], auto -> true, clusterBy -> [], zOrderBy -> [], batchId -> 0}                                                                          |NULL|{108503599691472}|0611-041854-oedbfkos|1          |SnapshotIsolation|false        |{numRemovedFiles -> 2, numRemovedBytes -> 5008, p25FileSize -> 2594, numDeletionVectorsRemoved -> 1, conflictDetectionTimeMs -> 20, minFileSize -> 2594, numAddedFiles -> 1, maxFileSize -> 2594, p75FileSize -> 2594, p50FileSize -> 2594, numAddedBytes -> 2594}                                                        |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|2      |2025-06-19 05:57:28|3955481281677681|azuser3561_mml.local@techademy.com|DELETE                           |{predicate -> [\"(ProgressPercent#11424 = 0)\"]}                                                                                                          |NULL|{108503599691472}|0611-041854-oedbfkos|1          |WriteSerializable|false        |{numRemovedFiles -> 0, numRemovedBytes -> 0, numCopiedRows -> 0, numDeletionVectorsAdded -> 0, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 212, numDeletionVectorsUpdated -> 0, numDeletedRows -> 0, scanTimeMs -> 212, numAddedFiles -> 0, numAddedBytes -> 0, rewriteTimeMs -> 0}      |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|1      |2025-06-19 05:57:27|3955481281677681|azuser3561_mml.local@techademy.com|UPDATE                           |{predicate -> [\"(CourseName#10485 = Python Basics)\"]}                                                                                                   |NULL|{108503599691472}|0611-041854-oedbfkos|0          |WriteSerializable|false        |{numRemovedFiles -> 0, numRemovedBytes -> 0, numCopiedRows -> 0, numDeletionVectorsAdded -> 1, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 1349, numDeletionVectorsUpdated -> 0, scanTimeMs -> 625, numAddedFiles -> 1, numUpdatedRows -> 2, numAddedBytes -> 2416, rewriteTimeMs -> 722}|NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|0      |2025-06-19 05:57:24|3955481281677681|azuser3561_mml.local@techademy.com|CREATE OR REPLACE TABLE AS SELECT|{partitionBy -> [], clusterBy -> [], description -> NULL, isManaged -> true, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> false}|NULL|{108503599691472}|0611-041854-oedbfkos|NULL       |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2592}                                                                                                                                                                                                                                                               |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n+-------+-------------------+----------------+----------------------------------+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----+-----------------+--------------------+-----------+-----------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n|EnrollID|UserID|CourseID|       CourseName|    Category|EnrollDate|CompletionDate|ProgressPercent|Rating|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n|    E001|  U001|    C001|    Python Basics| Programming|      NULL|    2024-04-10|            100|     4|\n|    E002|  U002|    C002|Excel for Finance|Productivity|      NULL|          NULL|             45|  NULL|\n|    E003|  U003|    C003|  ML with PySpark|Data Science|      NULL|          NULL|             30|  NULL|\n|    E004|  U004|    C001|    Python Basics| Programming|      NULL|    2024-04-20|            100|     5|\n|    E005|  U005|    C004|Digital Marketing|   Marketing|      NULL|    2024-04-16|            100|     4|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n|EnrollID|UserID|CourseID|       CourseName|    Category|EnrollDate|CompletionDate|ProgressPercent|Rating|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n|    E002|  U002|    C002|Excel for Finance|Productivity|      NULL|          NULL|             45|  NULL|\n|    E003|  U003|    C003|  ML with PySpark|Data Science|      NULL|          NULL|             30|  NULL|\n|    E005|  U005|    C004|Digital Marketing|   Marketing|      NULL|    2024-04-16|            100|     4|\n|    E001|  U001|    C001|    Python Basics| Programming|      NULL|    2024-04-10|            100|     5|\n|    E004|  U004|    C001|    Python Basics| Programming|      NULL|    2024-04-20|            100|     5|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n|EnrollID|UserID|CourseID|       CourseName|    Category|EnrollDate|CompletionDate|ProgressPercent|Rating|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n|    E001|  U001|    C001|    Python Basics| Programming|2024-04-01|    2024-04-10|            100|     5|\n|    E004|  U004|    C001|    Python Basics| Programming|2024-04-04|    2024-04-20|            100|     5|\n|    E002|  U002|    C002|Excel for Finance|Productivity|2024-04-02|          NULL|             45|  NULL|\n|    E003|  U003|    C003|  ML with PySpark|Data Science|2024-04-03|          NULL|             30|  NULL|\n|    E005|  U005|    C004|Digital Marketing|   Marketing|2024-04-05|    2024-04-16|            100|     4|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n|EnrollID|UserID|CourseID|       CourseName|    Category|EnrollDate|CompletionDate|ProgressPercent|Rating|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n|    E002|  U002|    C002|Excel for Finance|Productivity|      NULL|          NULL|             45|  NULL|\n|    E003|  U003|    C003|  ML with PySpark|Data Science|      NULL|          NULL|             30|  NULL|\n|    E005|  U005|    C004|Digital Marketing|   Marketing|      NULL|    2024-04-16|            100|     4|\n|    E001|  U001|    C001|    Python Basics| Programming|      NULL|    2024-04-10|            100|     5|\n|    E004|  U004|    C001|    Python Basics| Programming|      NULL|    2024-04-20|            100|     5|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n|EnrollID|UserID|CourseID|       CourseName|    Category|EnrollDate|CompletionDate|ProgressPercent|Rating|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n|    E001|  U001|    C001|    Python Basics| Programming|      NULL|    2024-04-10|            100|     4|\n|    E002|  U002|    C002|Excel for Finance|Productivity|      NULL|          NULL|             45|  NULL|\n|    E003|  U003|    C003|  ML with PySpark|Data Science|      NULL|          NULL|             30|  NULL|\n|    E004|  U004|    C001|    Python Basics| Programming|      NULL|    2024-04-20|            100|     5|\n|    E005|  U005|    C004|Digital Marketing|   Marketing|      NULL|    2024-04-16|            100|     4|\n+--------+------+--------+-----------------+------------+----------+--------------+---------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# 9. Time Travel\n",
    "# View previous version before update/delete\n",
    "spark.sql(\"DESCRIBE HISTORY enrollments_delta\").show(truncate=False)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT * FROM enrollments_delta VERSION AS OF 0\n",
    "\"\"\").show()\n",
    "spark.sql(\"\"\"\n",
    "SELECT * FROM enrollments_delta VERSION AS OF 1\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT * FROM enrollments_delta\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT * FROM enrollments_delta TIMESTAMP AS OF '2025-06-19T05:57:27Z'\n",
    "\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW enrollments_before_update AS\n",
    "SELECT * FROM enrollments_delta VERSION AS OF 0\n",
    "\"\"\")\n",
    "spark.sql(\"SELECT * FROM enrollments_before_update\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbb2554e-440d-48d8-8271-9d3e0c39f54d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+---------+-----------+\n|       CourseName|TotalEnrollments|AvgRating|AvgProgress|\n+-----------------+----------------+---------+-----------+\n|Digital Marketing|               1|      4.0|      100.0|\n|    Python Basics|               2|      5.0|      100.0|\n|Excel for Finance|               1|     NULL|       45.0|\n|  ML with PySpark|               1|     NULL|       30.0|\n+-----------------+----------------+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#Export Reporting\n",
    "df = spark.read.format(\"delta\").table(\"enrollments_delta\")\n",
    "\n",
    "# Write to JSON, partitioned by Category\n",
    "df.write.mode(\"overwrite\") \\\n",
    "  .partitionBy(\"Category\") \\\n",
    "  .json(\"dbfs:/Workspace/Shared/Exports/enrollments_json_partitioned\")\n",
    "\n",
    "# Create summary DataFrame: CourseName, TotalEnrollments, AvgRating, AvgProgress\n",
    "from pyspark.sql.functions import count, avg, round\n",
    "\n",
    "summary_df = df.groupBy(\"CourseName\").agg(\n",
    "    count(\"*\").alias(\"TotalEnrollments\"),\n",
    "    round(avg(\"Rating\"), 2).alias(\"AvgRating\"),\n",
    "    round(avg(\"ProgressPercent\"), 2).alias(\"AvgProgress\")\n",
    ")\n",
    "summary_df.show()\n",
    "\n",
    "# Save as Parquet\n",
    "summary_df.write.mode(\"overwrite\") \\\n",
    "  .parquet(\"dbfs:/Workspace/Shared/Exports/course_summary.parquet\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Online Course Learning Platform Analytics 2025-06-19 10:59:33",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}