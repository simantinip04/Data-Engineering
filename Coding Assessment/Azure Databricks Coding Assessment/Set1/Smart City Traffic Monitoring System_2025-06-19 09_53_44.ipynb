{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb7388ef-5b39-49fc-8fea-f0fbfaf515d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=4054280922637182#setting/sparkui/0611-041854-oedbfkos/driver-4507201859227434482\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x73e0455f4dd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SmartCityTraffic\").getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f0baec0-fb50-4c00-8600-3071cb7e409b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- LogID: string (nullable = true)\n |-- VehicleID: string (nullable = true)\n |-- EntryPoint: string (nullable = true)\n |-- ExitPoint: string (nullable = true)\n |-- EntryTime: string (nullable = true)\n |-- ExitTime: string (nullable = true)\n |-- VehicleType: string (nullable = true)\n |-- SpeedKMH: integer (nullable = true)\n |-- TollPaid: integer (nullable = true)\n\nroot\n |-- LogID: string (nullable = true)\n |-- VehicleID: string (nullable = true)\n |-- EntryPoint: string (nullable = true)\n |-- ExitPoint: string (nullable = true)\n |-- EntryTime: timestamp (nullable = true)\n |-- ExitTime: timestamp (nullable = true)\n |-- VehicleType: string (nullable = true)\n |-- SpeedKMH: integer (nullable = true)\n |-- TollPaid: integer (nullable = true)\n\nroot\n |-- EntryTime: timestamp (nullable = true)\n |-- ExitTime: timestamp (nullable = true)\n\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+\n|LogID|VehicleID|EntryPoint|ExitPoint|          EntryTime|           ExitTime|VehicleType|SpeedKMH|TollPaid|\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+\n| L001|     V001|     GateA|    GateC|2024-05-01 08:01:00|2024-05-01 08:20:00|        Car|      60|      50|\n| L002|     V002|     GateB|    GateC|2024-05-01 08:10:00|2024-05-01 08:45:00|      Truck|      45|     100|\n| L003|     V003|     GateA|    GateD|2024-05-01 09:00:00|2024-05-01 09:18:00|       Bike|      55|      30|\n| L004|     V004|     GateC|    GateD|2024-05-01 09:15:00|2024-05-01 09:35:00|        Car|      80|      50|\n| L005|     V005|     GateB|    GateA|2024-05-01 10:05:00|2024-05-01 10:40:00|        Bus|      40|      70|\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# 1. Data Ingestion & Schema Analysis\n",
    "# Load CSV using PySpark with schema inference\n",
    "df_infer = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"file:/Workspace/Shared/Coding Assessment Datasets/traffic_logs.csv\")\n",
    "df_infer.printSchema()\n",
    "\n",
    "# Manually define schema and compare\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"LogID\", StringType(), True),\n",
    "    StructField(\"VehicleID\", StringType(), True),\n",
    "    StructField(\"EntryPoint\", StringType(), True),\n",
    "    StructField(\"ExitPoint\", StringType(), True),\n",
    "    StructField(\"EntryTime\", TimestampType(), True),\n",
    "    StructField(\"ExitTime\", TimestampType(), True),\n",
    "    StructField(\"VehicleType\", StringType(), True),\n",
    "    StructField(\"SpeedKMH\", IntegerType(), True),\n",
    "    StructField(\"TollPaid\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_manual = spark.read.schema(schema).option(\"header\", True).csv(\"file:/Workspace/Shared/Coding Assessment Datasets/traffic_logs.csv\")\n",
    "df_manual.printSchema()\n",
    "\n",
    "# Ensure EntryTime/ExitTime are timestamp\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "df = df_infer.withColumn(\"EntryTime\", to_timestamp(\"EntryTime\", \"dd-MM-yyyy HH:mm\")) \\\n",
    "       .withColumn(\"ExitTime\", to_timestamp(\"ExitTime\", \"dd-MM-yyyy HH:mm\"))\n",
    "\n",
    "df.select(\"EntryTime\", \"ExitTime\").printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91e51d4e-191e-4f66-b0d9-e5e9f102153b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+\n|EntryTime       |ExitTime        |\n+----------------+----------------+\n|01-05-2024 09:15|01-05-2024 09:35|\n|01-05-2024 10:05|01-05-2024 10:40|\n|01-05-2024 08:10|01-05-2024 08:45|\n|01-05-2024 09:00|01-05-2024 09:18|\n|01-05-2024 08:01|01-05-2024 08:20|\n+----------------+----------------+\n\n+-------------------+-------------------+\n|EntryTime          |ExitTime           |\n+-------------------+-------------------+\n|2024-05-01 08:01:00|2024-05-01 08:20:00|\n|2024-05-01 08:10:00|2024-05-01 08:45:00|\n|2024-05-01 09:00:00|2024-05-01 09:18:00|\n|2024-05-01 09:15:00|2024-05-01 09:35:00|\n|2024-05-01 10:05:00|2024-05-01 10:40:00|\n+-------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_infer.select(\"EntryTime\", \"ExitTime\").distinct().show(truncate=False)\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "df = df_infer.withColumn(\"EntryTime\", to_timestamp(\"EntryTime\", \"dd-MM-yyyy HH:mm\")) \\\n",
    "             .withColumn(\"ExitTime\", to_timestamp(\"ExitTime\", \"dd-MM-yyyy HH:mm\"))\n",
    "\n",
    "df.select(\"EntryTime\", \"ExitTime\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a42768b4-0602-44e0-a61f-4be7fa8f6a6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+-----------+\n|EntryTime          |ExitTime           |TripDurationMinutes|IsOverspeed|\n+-------------------+-------------------+-------------------+-----------+\n|2024-05-01 08:01:00|2024-05-01 08:20:00|19.0               |false      |\n|2024-05-01 08:10:00|2024-05-01 08:45:00|35.0               |false      |\n|2024-05-01 09:00:00|2024-05-01 09:18:00|18.0               |false      |\n|2024-05-01 09:15:00|2024-05-01 09:35:00|20.0               |true       |\n|2024-05-01 10:05:00|2024-05-01 10:40:00|35.0               |false      |\n+-------------------+-------------------+-------------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# 2. Derived Column Creation\n",
    "# Calculate TripDurationMinutes = ExitTime - EntryTime\n",
    "from pyspark.sql.functions import col, round, when\n",
    "\n",
    "df = df.withColumn(\"TripDurationMinutes\",\n",
    "                   round((col(\"ExitTime\").cast(\"long\") - col(\"EntryTime\").cast(\"long\")) / 60, 2)) \\\n",
    "       .withColumn(\"IsOverspeed\", when(col(\"SpeedKMH\") > 60, True).otherwise(False))\n",
    "\n",
    "df.select(\"EntryTime\", \"ExitTime\", \"TripDurationMinutes\", \"IsOverspeed\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4c2a819-5b0e-4122-b818-ca2de0a122fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n|VehicleType|avg(SpeedKMH)|\n+-----------+-------------+\n|       Bike|         55.0|\n|        Car|         70.0|\n|      Truck|         45.0|\n|        Bus|         40.0|\n+-----------+-------------+\n\n+----------+---------+\n|EntryPoint|TotalToll|\n+----------+---------+\n|     GateA|       80|\n|     GateB|      170|\n|     GateC|       50|\n+----------+---------+\n\n+---------+-----+\n|ExitPoint|count|\n+---------+-----+\n|    GateD|    2|\n+---------+-----+\nonly showing top 1 row\n\n"
     ]
    }
   ],
   "source": [
    "# 3. Vehicle Behavior Aggregations\n",
    "# Average speed per VehicleType\n",
    "df.groupBy(\"VehicleType\").avg(\"SpeedKMH\").show()\n",
    "\n",
    "# Total toll collected per gate (EntryPoint)\n",
    "df.groupBy(\"EntryPoint\").sum(\"TollPaid\").withColumnRenamed(\"sum(TollPaid)\", \"TotalToll\").show()\n",
    "\n",
    "# Most used ExitPoint\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "df.groupBy(\"ExitPoint\").count().orderBy(col(\"count\").desc()).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b2f2cc5-17ca-41dd-b40f-659ebdf444f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Window Functions\n",
    "# Rank vehicles by speed within VehicleType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, lag\n",
    "\n",
    "window_by_type = Window.partitionBy(\"VehicleType\").orderBy(col(\"SpeedKMH\").desc())\n",
    "\n",
    "df = df.withColumn(\"SpeedRank\", rank().over(window_by_type))\n",
    "\n",
    "#Find last exit time for each vehicle using lag()\n",
    "window_by_vehicle = Window.partitionBy(\"VehicleID\").orderBy(\"ExitTime\")\n",
    "\n",
    "df = df.withColumn(\"LastExitTime\", lag(\"ExitTime\").over(window_by_vehicle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb9c030a-3ef2-4eca-8d5e-43413fa49ab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+---------------+\n|LastExitTime       |EntryTime          |IdleTimeMinutes|\n+-------------------+-------------------+---------------+\n|NULL               |2024-05-01 08:01:00|NULL           |\n|2024-05-01 08:20:00|2024-05-01 08:10:00|-10.0          |\n|2024-05-01 08:45:00|2024-05-01 09:00:00|15.0           |\n|2024-05-01 09:18:00|2024-05-01 09:15:00|-3.0           |\n|2024-05-01 09:35:00|2024-05-01 10:05:00|30.0           |\n+-------------------+-------------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 5. Session Segmentation\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag\n",
    "window_spec = Window.orderBy(\"EntryTime\")\n",
    "\n",
    "# Add LastExitTime\n",
    "df = df.withColumn(\"LastExitTime\", lag(\"ExitTime\").over(window_spec))\n",
    "\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"IdleTimeMinutes\",\n",
    "    (unix_timestamp(\"EntryTime\") - unix_timestamp(\"LastExitTime\")) / 60\n",
    ")\n",
    "\n",
    "df.select(\"LastExitTime\", \"EntryTime\", \"IdleTimeMinutes\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e65e6c0-f5fe-440a-8ebf-0c6d2e45007b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+---------+---------+--------+-----------+--------+--------+-------------------+-----------+---------+------------+---------------+\n|LogID|VehicleID|EntryPoint|ExitPoint|EntryTime|ExitTime|VehicleType|SpeedKMH|TollPaid|TripDurationMinutes|IsOverspeed|SpeedRank|LastExitTime|IdleTimeMinutes|\n+-----+---------+----------+---------+---------+--------+-----------+--------+--------+-------------------+-----------+---------+------------+---------------+\n+-----+---------+----------+---------+---------+--------+-----------+--------+--------+-------------------+-----------+---------+------------+---------------+\n\n+-----+---------+----------+---------+---------+--------+-----------+--------+--------+-------------------+-----------+---------+------------+---------------+\n|LogID|VehicleID|EntryPoint|ExitPoint|EntryTime|ExitTime|VehicleType|SpeedKMH|TollPaid|TripDurationMinutes|IsOverspeed|SpeedRank|LastExitTime|IdleTimeMinutes|\n+-----+---------+----------+---------+---------+--------+-----------+--------+--------+-------------------+-----------+---------+------------+---------------+\n+-----+---------+----------+---------+---------+--------+-----------+--------+--------+-------------------+-----------+---------+------------+---------------+\n\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+---------+-------------------+---------------+\n|LogID|VehicleID|EntryPoint|ExitPoint|          EntryTime|           ExitTime|VehicleType|SpeedKMH|TollPaid|TripDurationMinutes|IsOverspeed|SpeedRank|       LastExitTime|IdleTimeMinutes|\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+---------+-------------------+---------------+\n| L005|     V005|     GateB|    GateA|2024-05-01 10:05:00|2024-05-01 10:40:00|        Bus|      40|      70|               35.0|      false|        1|2024-05-01 09:35:00|           30.0|\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+---------+-------------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 6. Anomaly Detection\n",
    "# Identify vehicles with speed > 70 and TripDuration < 10 minutes\n",
    "df.filter((col(\"SpeedKMH\") > 70) & (col(\"TripDurationMinutes\") < 10)).show()\n",
    "\n",
    "# Vehicles that paid less toll for longer trips\n",
    "df.filter((col(\"TripDurationMinutes\") > 30) & (col(\"TollPaid\") < 50)).show()\n",
    "\n",
    "# Suspicious backtracking (ExitPoint earlier than EntryPoint)\n",
    "df.filter(col(\"EntryPoint\") > col(\"ExitPoint\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42bf01b6-b9b3-4b69-9c0b-8a5052236cab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n|RegisteredCity|count|\n+--------------+-----+\n|     Bangalore|    1|\n|       Chennai|    1|\n|        Mumbai|    1|\n|          Pune|    1|\n|         Delhi|    1|\n+--------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# 7. Join with Metadata\n",
    "# Prepare this small vehicle_registry.csv :\n",
    "# VehicleID,OwnerName,Model,RegisteredCity\n",
    "# V001,Anil,Hyundai i20,Delhi\n",
    "# V002,Rakesh,Tata Truck,Chennai\n",
    "# V003,Sana,Yamaha R15,Mumbai\n",
    "# V004,Neha,Honda City,Bangalore\n",
    "# V005,Zoya,Volvo Bus,Pune\n",
    "# Join and group trips by RegisteredCity\n",
    "vehicle_registry = spark.read.option(\"header\", True).csv(\"file:/Workspace/Shared/Coding Assessment Datasets/vehicle_registry.csv\")\n",
    "\n",
    "df_joined = df.join(vehicle_registry, on=\"VehicleID\", how=\"left\")\n",
    "\n",
    "df_joined.groupBy(\"RegisteredCity\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3c45815-37fe-4b11-ab49-20e01c4bdfe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 8. Delta Lake Features\n",
    "# Save Delta Table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"traffic_logs\")\n",
    "\n",
    "# Read and verify\n",
    "spark.sql(\"SELECT * FROM traffic_logs\").show()\n",
    "\n",
    "# Update toll for Bikes using MERGE\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO traffic_logs AS target\n",
    "USING (\n",
    "    SELECT * FROM traffic_logs WHERE VehicleType = 'Bike'\n",
    ") AS source\n",
    "ON target.LogID = source.LogID\n",
    "WHEN MATCHED THEN\n",
    "UPDATE SET target.TollPaid = 40\n",
    "\"\"\")\n",
    "\n",
    "# Delete long trips\n",
    "spark.sql(\"\"\"\n",
    "DELETE FROM traffic_logs\n",
    "WHERE TripDurationMinutes > 60\n",
    "\"\"\")\n",
    "\n",
    "# View operation history\n",
    "spark.sql(\"\"\"\n",
    "DESCRIBE HISTORY traffic_logs\n",
    "\"\"\").show()\n",
    "\n",
    "# Query an earlier version (version 0, before update/delete)\n",
    "spark.sql(\"\"\"\n",
    "SELECT * FROM traffic_logs VERSION AS OF 0\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8593ab02-db5c-4324-8699-0729b9d5baee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+---------+-------------------+---------------+--------+\n|LogID|VehicleID|EntryPoint|ExitPoint|          EntryTime|           ExitTime|VehicleType|SpeedKMH|TollPaid|TripDurationMinutes|IsOverspeed|SpeedRank|       LastExitTime|IdleTimeMinutes|TripType|\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+---------+-------------------+---------------+--------+\n| L001|     V001|     GateA|    GateC|2024-05-01 08:01:00|2024-05-01 08:20:00|        Car|      60|      50|               19.0|      false|        2|               NULL|           NULL|  Medium|\n| L002|     V002|     GateB|    GateC|2024-05-01 08:10:00|2024-05-01 08:45:00|      Truck|      45|     100|               35.0|      false|        1|2024-05-01 08:20:00|          -10.0|    Long|\n| L003|     V003|     GateA|    GateD|2024-05-01 09:00:00|2024-05-01 09:18:00|       Bike|      55|      30|               18.0|      false|        1|2024-05-01 08:45:00|           15.0|  Medium|\n| L004|     V004|     GateC|    GateD|2024-05-01 09:15:00|2024-05-01 09:35:00|        Car|      80|      50|               20.0|       true|        1|2024-05-01 09:18:00|           -3.0|  Medium|\n| L005|     V005|     GateB|    GateA|2024-05-01 10:05:00|2024-05-01 10:40:00|        Bus|      40|      70|               35.0|      false|        1|2024-05-01 09:35:00|           30.0|    Long|\n+-----+---------+----------+---------+-------------------+-------------------+-----------+--------+--------+-------------------+-----------+---------+-------------------+---------------+--------+\n\n+---------+--------+-----+\n|VehicleID|TripDate|count|\n+---------+--------+-----+\n+---------+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# 9. Advanced Conditions\n",
    "# when/otherwise : Tag trip type as:\n",
    "# \"Short\" <15min\n",
    "# \"Medium\" 15-30min\n",
    "# \"Long\" >30min\n",
    "df = df.withColumn(\"TripType\", when(col(\"TripDurationMinutes\") < 15, \"Short\")\n",
    "                   .when(col(\"TripDurationMinutes\") <= 30, \"Medium\")\n",
    "                   .otherwise(\"Long\"))\n",
    "df.show()\n",
    "\n",
    "# Flag vehicles with more than 3 trips in a day\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "trip_count = df.withColumn(\"TripDate\", to_date(\"EntryTime\")) \\\n",
    "               .groupBy(\"VehicleID\", \"TripDate\").count() \\\n",
    "               .filter(col(\"count\") > 3)\n",
    "\n",
    "trip_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba2c2e7d-cdf6-40e8-9b65-f3f076b7cf5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+---------+\n|VehicleType|ExitPoint|TotalToll|\n+-----------+---------+---------+\n|        Car|    GateD|       50|\n|      Truck|    GateC|      100|\n|       Bike|    GateD|       30|\n|        Bus|    GateA|       70|\n|        Car|    GateC|       50|\n+-----------+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Export & Reporting\n",
    "# Write final enriched DataFrame to:\n",
    "# Parquet partitioned by VehicleType\n",
    "df.write.mode(\"overwrite\").partitionBy(\"VehicleType\").parquet(\"/output/traffic_partitioned/\")\n",
    "\n",
    "# CSV for dashboards\n",
    "df.write.mode(\"overwrite\").option(\"header\", True).csv(\"/output/traffic_csv/\")\n",
    "\n",
    "# Create summary SQL View: total toll by VehicleType + ExitPoint\n",
    "df.createOrReplaceTempView(\"traffic_view\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT VehicleType, ExitPoint, SUM(TollPaid) as TotalToll\n",
    "FROM traffic_view\n",
    "GROUP BY VehicleType, ExitPoint\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Smart City Traffic Monitoring System_2025-06-19 09:53:44",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}