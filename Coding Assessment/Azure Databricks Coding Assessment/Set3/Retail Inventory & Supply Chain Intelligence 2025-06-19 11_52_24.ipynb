{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2768bae-9297-4976-a9c9-a06161e78c42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=4054280922637182#setting/sparkui/0611-041854-oedbfkos/driver-4507201859227434482\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7137a7734d10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"InventoryAlertingSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "539e69c8-b442-4aed-a796-851e2a4c1084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- ItemID: string (nullable = true)\n |-- ItemName: string (nullable = true)\n |-- Category: string (nullable = true)\n |-- Warehouse: string (nullable = true)\n |-- StockQty: integer (nullable = true)\n |-- ReorderLevel: integer (nullable = true)\n |-- LastRestocked: date (nullable = true)\n |-- UnitPrice: integer (nullable = true)\n |-- Supplier: string (nullable = true)\n\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+\n|ItemID|    ItemName|   Category| Warehouse|StockQty|ReorderLevel|LastRestocked|UnitPrice| Supplier|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+\n|  I001|      LED TV|Electronics|WarehouseA|      50|          20|   2024-03-15|    30000|   AVTech|\n|  I002|      Laptop|Electronics|WarehouseB|      10|          15|   2024-04-01|    70000|TechWorld|\n|  I003|Office Chair|  Furniture|WarehouseA|      40|          10|   2024-03-25|     6000|  ChairCo|\n|  I004|Refrigerator| Appliances|WarehouseC|       5|          10|   2024-02-20|    25000| FreezeIt|\n|  I005|     Printer|Electronics|WarehouseB|       3|           5|   2024-03-30|     8000|PrintFast|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+\n\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+\n|ItemID|    ItemName|   Category| Warehouse|StockQty|ReorderLevel|LastRestocked|UnitPrice| Supplier|NeedsReorder|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+\n|  I001|      LED TV|Electronics|WarehouseA|      50|          20|   2024-03-15|    30000|   AVTech|       false|\n|  I002|      Laptop|Electronics|WarehouseB|      10|          15|   2024-04-01|    70000|TechWorld|        true|\n|  I003|Office Chair|  Furniture|WarehouseA|      40|          10|   2024-03-25|     6000|  ChairCo|       false|\n|  I004|Refrigerator| Appliances|WarehouseC|       5|          10|   2024-02-20|    25000| FreezeIt|        true|\n|  I005|     Printer|Electronics|WarehouseB|       3|           5|   2024-03-30|     8000|PrintFast|        true|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+\n\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+\n|ItemID|    ItemName|   Category| Warehouse|StockQty|ReorderLevel|LastRestocked|UnitPrice| Supplier|NeedsReorder|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+\n|  I002|      Laptop|Electronics|WarehouseB|      10|          15|   2024-04-01|    70000|TechWorld|        true|\n|  I004|Refrigerator| Appliances|WarehouseC|       5|          10|   2024-02-20|    25000| FreezeIt|        true|\n|  I005|     Printer|Electronics|WarehouseB|       3|           5|   2024-03-30|     8000|PrintFast|        true|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+------------+\n\n+----------+----------------+\n| Warehouse|RestockItemCount|\n+----------+----------------+\n|WarehouseB|               2|\n+----------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Scenario 1: Inventory Alerting System\n",
    "# Tasks:\n",
    "# 1. Load the data using PySpark.\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"file:/Workspace/Shared/Coding Assessment Datasets/inventory_supply.csv\")\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "# 2. Create a new column NeedsReorder = StockQty < ReorderLevel .\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_with_alerts = df.withColumn(\"NeedsReorder\", col(\"StockQty\") < col(\"ReorderLevel\"))\n",
    "df_with_alerts.show()\n",
    "\n",
    "# 3. Create a view of all items that need restocking.\n",
    "items_to_reorder = df_with_alerts.filter(col(\"NeedsReorder\") == True)\n",
    "items_to_reorder.createOrReplaceTempView(\"RestockView\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM RestockView\").show()\n",
    "\n",
    "# 4. Highlight warehouses with more than 2 such items.\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "critical_warehouses = items_to_reorder.groupBy(\"Warehouse\") \\\n",
    "    .agg(count(\"*\").alias(\"RestockItemCount\")) \\\n",
    "    .filter(col(\"RestockItemCount\") > 1)  \n",
    "\n",
    "critical_warehouses.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72586fb9-6ce1-4ebe-b053-b109a2ced6e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+\n| Supplier|AvgPricePerSupplier|\n+---------+-------------------+\n|   AVTech|            30000.0|\n|TechWorld|            70000.0|\n|PrintFast|             8000.0|\n| FreezeIt|            25000.0|\n|  ChairCo|             6000.0|\n+---------+-------------------+\n\n+-----------+----------------+\n|   Category|AvgCategoryPrice|\n+-----------+----------------+\n|Electronics|         36000.0|\n| Appliances|         25000.0|\n|  Furniture|          6000.0|\n+-----------+----------------+\n\n+------+------------+-----------+---------+----------------+---------+------------------+\n|ItemID|    ItemName|   Category|UnitPrice|AvgCategoryPrice| Supplier|IsBelowCategoryAvg|\n+------+------------+-----------+---------+----------------+---------+------------------+\n|  I001|      LED TV|Electronics|    30000|         36000.0|   AVTech|              true|\n|  I002|      Laptop|Electronics|    70000|         36000.0|TechWorld|             false|\n|  I003|Office Chair|  Furniture|     6000|          6000.0|  ChairCo|             false|\n|  I004|Refrigerator| Appliances|    25000|         25000.0| FreezeIt|             false|\n|  I005|     Printer|Electronics|     8000|         36000.0|PrintFast|              true|\n+------+------------+-----------+---------+----------------+---------+------------------+\n\n+---------+----------+-------------+--------+\n| Supplier|TotalItems|BelowAvgItems|GoodDeal|\n+---------+----------+-------------+--------+\n|   AVTech|         1|            1|    true|\n|TechWorld|         1|            0|   false|\n|PrintFast|         1|            1|    true|\n| FreezeIt|         1|            0|   false|\n|  ChairCo|         1|            0|   false|\n+---------+----------+-------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#Scenario 2: Supplier Price Optimization\n",
    "# Tasks:\n",
    "# 1. Group items by Supplier and compute average price.\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "supplier_avg_price = df.groupBy(\"Supplier\").agg(avg(\"UnitPrice\").alias(\"AvgPricePerSupplier\"))\n",
    "supplier_avg_price.show()\n",
    "\n",
    "# 2. Find which suppliers offer items below average price in their category.\n",
    "category_avg_price = df.groupBy(\"Category\").agg(avg(\"UnitPrice\").alias(\"AvgCategoryPrice\"))\n",
    "category_avg_price.show()\n",
    "\n",
    "df_with_category_avg = df.join(category_avg_price, on=\"Category\", how=\"left\")\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_deals = df_with_category_avg.withColumn(\n",
    "    \"IsBelowCategoryAvg\", col(\"UnitPrice\") < col(\"AvgCategoryPrice\")\n",
    ")\n",
    "df_deals.select(\"ItemID\", \"ItemName\", \"Category\", \"UnitPrice\", \"AvgCategoryPrice\", \"Supplier\", \"IsBelowCategoryAvg\").show()\n",
    "\n",
    "# 3. Tag suppliers with Good Deal if >50% of their items are below market average.\n",
    "from pyspark.sql.functions import sum as _sum, count as _count, when\n",
    "\n",
    "supplier_deal_stats = df_deals.groupBy(\"Supplier\").agg(\n",
    "    _count(\"*\").alias(\"TotalItems\"),\n",
    "    _sum(when(col(\"IsBelowCategoryAvg\") == True, 1).otherwise(0)).alias(\"BelowAvgItems\")\n",
    ")\n",
    "supplier_deal_tagged = supplier_deal_stats.withColumn(\n",
    "    \"GoodDeal\",\n",
    "    (col(\"BelowAvgItems\") / col(\"TotalItems\") > 0.5)\n",
    ")\n",
    "supplier_deal_tagged.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e70082f-3497-431c-a26b-b78df456925b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+--------+---------+---------------+\n|ItemID|    ItemName|StockQty|UnitPrice|TotalStockValue|\n+------+------------+--------+---------+---------------+\n|  I001|      LED TV|      50|    30000|        1500000|\n|  I002|      Laptop|      10|    70000|         700000|\n|  I003|Office Chair|      40|     6000|         240000|\n|  I004|Refrigerator|       5|    25000|         125000|\n|  I005|     Printer|       3|     8000|          24000|\n+------+------------+--------+---------+---------------+\n\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+---------------+\n|ItemID|    ItemName|   Category| Warehouse|StockQty|ReorderLevel|LastRestocked|UnitPrice| Supplier|TotalStockValue|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+---------------+\n|  I001|      LED TV|Electronics|WarehouseA|      50|          20|   2024-03-15|    30000|   AVTech|        1500000|\n|  I002|      Laptop|Electronics|WarehouseB|      10|          15|   2024-04-01|    70000|TechWorld|         700000|\n|  I003|Office Chair|  Furniture|WarehouseA|      40|          10|   2024-03-25|     6000|  ChairCo|         240000|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Scenario 3: Cost Forecasting\n",
    "# Tasks:\n",
    "# 1. Calculate TotalStockValue = StockQty * UnitPrice .\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_with_stock_value = df.withColumn(\n",
    "    \"TotalStockValue\", col(\"StockQty\") * col(\"UnitPrice\")\n",
    ")\n",
    "\n",
    "df_with_stock_value.select(\"ItemID\", \"ItemName\", \"StockQty\", \"UnitPrice\", \"TotalStockValue\").show()\n",
    "\n",
    "# 2. Identify top 3 highest-value items.\n",
    "top_3_items = df_with_stock_value.orderBy(col(\"TotalStockValue\").desc()).limit(3)\n",
    "top_3_items.show()\n",
    "\n",
    "# 3. Export the result as a Parquet file partitioned by Warehouse .\n",
    "df_with_stock_value.write \\\n",
    "    .partitionBy(\"Warehouse\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"output/inventory_stock_value/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c638205-19e6-4b02-b4c8-3a6823f816ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n| Warehouse|ItemCount|\n+----------+---------+\n|WarehouseA|        2|\n|WarehouseC|        1|\n|WarehouseB|        2|\n+----------+---------+\n\n+----------+-----------+--------+\n| Warehouse|   Category|AvgStock|\n+----------+-----------+--------+\n|WarehouseB|Electronics|     6.5|\n|WarehouseC| Appliances|     5.0|\n|WarehouseA|  Furniture|    40.0|\n|WarehouseA|Electronics|    50.0|\n+----------+-----------+--------+\n\n+----------+----------+\n| Warehouse|TotalStock|\n+----------+----------+\n|WarehouseA|        90|\n|WarehouseC|         5|\n|WarehouseB|        13|\n+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Scenario 4: Warehouse Utilization\n",
    "# Tasks:\n",
    "# 1. Count items stored per warehouse.\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "items_per_warehouse = df.groupBy(\"Warehouse\").agg(\n",
    "    countDistinct(\"ItemID\").alias(\"ItemCount\")\n",
    ")\n",
    "items_per_warehouse.show()\n",
    "\n",
    "# 2. Average stock per category in each warehouse.\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "avg_stock_per_category_warehouse = df.groupBy(\"Warehouse\", \"Category\").agg(\n",
    "    avg(\"StockQty\").alias(\"AvgStock\")\n",
    ")\n",
    "avg_stock_per_category_warehouse.show()\n",
    "\n",
    "# 3. Determine underutilized warehouses ( total stock < 100 ).\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "total_stock_per_warehouse = df.groupBy(\"Warehouse\").agg(\n",
    "    _sum(\"StockQty\").alias(\"TotalStock\")\n",
    ")\n",
    "\n",
    "underutilized_warehouses = total_stock_per_warehouse.filter(col(\"TotalStock\") < 100)\n",
    "underutilized_warehouses.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eccb909a-8636-4191-a10a-10f4924b7d8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n|num_affected_rows|\n+-----------------+\n|                1|\n+-----------------+\n\n+-----------------+\n|num_affected_rows|\n+-----------------+\n|                0|\n+-----------------+\n\n+-------+-------------------+----------------+----------------------------------+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|version|timestamp          |userId          |userName                          |operation                        |operationParameters                                                                                                                                     |job |notebook          |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                          |userMetadata|engineInfo                                |\n+-------+-------------------+----------------+----------------------------------+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n|3      |2025-06-19 08:00:56|3955481281677681|azuser3561_mml.local@techademy.com|OPTIMIZE                         |{predicate -> [], auto -> true, clusterBy -> [], zOrderBy -> [], batchId -> 0}                                                                          |NULL|{2567282729753189}|0611-041854-oedbfkos|1          |SnapshotIsolation|false        |{numRemovedFiles -> 2, numRemovedBytes -> 4961, p25FileSize -> 2634, numDeletionVectorsRemoved -> 1, conflictDetectionTimeMs -> 15, minFileSize -> 2634, numAddedFiles -> 1, maxFileSize -> 2634, p75FileSize -> 2634, p50FileSize -> 2634, numAddedBytes -> 2634}                                                        |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|2      |2025-06-19 08:00:55|3955481281677681|azuser3561_mml.local@techademy.com|DELETE                           |{predicate -> [\"(StockQty#25917 = 0)\"]}                                                                                                                 |NULL|{2567282729753189}|0611-041854-oedbfkos|1          |WriteSerializable|false        |{numRemovedFiles -> 0, numRemovedBytes -> 0, numCopiedRows -> 0, numDeletionVectorsAdded -> 0, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 158, numDeletionVectorsUpdated -> 0, numDeletedRows -> 0, scanTimeMs -> 158, numAddedFiles -> 0, numAddedBytes -> 0, rewriteTimeMs -> 0}      |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|1      |2025-06-19 08:00:53|3955481281677681|azuser3561_mml.local@techademy.com|UPDATE                           |{predicate -> [\"(ItemName#25092 = Laptop)\"]}                                                                                                            |NULL|{2567282729753189}|0611-041854-oedbfkos|0          |WriteSerializable|false        |{numRemovedFiles -> 0, numRemovedBytes -> 0, numCopiedRows -> 0, numDeletionVectorsAdded -> 1, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 1217, numDeletionVectorsUpdated -> 0, scanTimeMs -> 564, numAddedFiles -> 1, numUpdatedRows -> 1, numAddedBytes -> 2328, rewriteTimeMs -> 652}|NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n|0      |2025-06-19 08:00:51|3955481281677681|azuser3561_mml.local@techademy.com|CREATE OR REPLACE TABLE AS SELECT|{partitionBy -> [], clusterBy -> [], description -> NULL, isManaged -> true, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> false}|NULL|{2567282729753189}|0611-041854-oedbfkos|NULL       |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 2633}                                                                                                                                                                                                                                                               |NULL        |Databricks-Runtime/15.4.x-photon-scala2.12|\n+-------+-------------------+----------------+----------------------------------+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Scenario 5: Delta Audit Trail\n",
    "# Tasks:\n",
    "# 1. Save as Delta table retail_inventory .\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"retail_inventory\")\n",
    "\n",
    "# 2. Update stock of 'Laptop' to 20.\n",
    "spark.sql(\"\"\"\n",
    "UPDATE retail_inventory\n",
    "SET StockQty = 20\n",
    "WHERE ItemName = 'Laptop'\n",
    "\"\"\").show()\n",
    "\n",
    "# 3. Delete any item with StockQty = 0 .\n",
    "spark.sql(\"\"\"\n",
    "DELETE FROM retail_inventory\n",
    "WHERE StockQty = 0\n",
    "\"\"\").show()\n",
    "\n",
    "# 4. Run DESCRIBE HISTORY and query VERSION AS OF previous state.\n",
    "spark.sql(\"DESCRIBE HISTORY retail_inventory\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "913562f4-15e7-4ca2-9989-397ffc4eac2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+--------+-------------+-----------+-----------------+\n|ItemID|    ItemName|StockQty|QuantityAdded|NewStockQty|RestockedRecently|\n+------+------------+--------+-------------+-----------+-----------------+\n|  I002|      Laptop|      30|           10|         40|             true|\n|  I001|      LED TV|      70|           20|         90|             true|\n|  I005|     Printer|       8|            5|         13|             true|\n|  I003|Office Chair|      40|         NULL|         40|            false|\n|  I004|Refrigerator|       5|         NULL|          5|            false|\n+------+------------+--------+-------------+-----------+-----------------+\n\n+-----------------+----------------+----------------+-----------------+\n|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n+-----------------+----------------+----------------+-----------------+\n|                5|               5|               0|                0|\n+-----------------+----------------+----------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Scenario 6: Alerts from Restock Logs (Join Task)\n",
    "# restock_logs.csv :\n",
    "# ItemID,RestockDate,QuantityAdded\n",
    "# I002,2024-04-20,10\n",
    "# I005,2024-04-22,5\n",
    "# I001,2024-04-25,20\n",
    "# Tasks:\n",
    "# 1. Join with inventory table to update StockQty.\n",
    "restock_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"file:/Workspace/Shared/Coding Assessment Datasets/restock_logs.csv\")\n",
    "inventory_df = spark.table(\"retail_inventory\")\n",
    "\n",
    "joined_df = inventory_df.alias(\"inv\") \\\n",
    "    .join(restock_df.alias(\"log\"), on=\"ItemID\", how=\"left\")\n",
    "\n",
    "# 2. Calculate new stock and flag RestockedRecently = true for updated items.\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "updated_df = joined_df.withColumn(\n",
    "    \"NewStockQty\",\n",
    "    when(col(\"QuantityAdded\").isNotNull(), col(\"StockQty\") + col(\"QuantityAdded\")).otherwise(col(\"StockQty\"))\n",
    ").withColumn(\n",
    "    \"RestockedRecently\",\n",
    "    col(\"QuantityAdded\").isNotNull()\n",
    ")\n",
    "\n",
    "updated_df.select(\"ItemID\", \"ItemName\", \"StockQty\", \"QuantityAdded\", \"NewStockQty\", \"RestockedRecently\").show()\n",
    "\n",
    "# 3. Use MERGE INTO to update in Delta.\n",
    "from pyspark.sql.functions import lit\n",
    "df_existing = spark.table(\"retail_inventory\")\n",
    "df_extended = df_existing.withColumn(\"RestockedRecently\", lit(False))\n",
    "df_extended.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"retail_inventory\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO retail_inventory AS target\n",
    "USING restock_updates AS source\n",
    "ON target.ItemID = source.ItemID\n",
    "WHEN MATCHED THEN UPDATE SET \n",
    "    StockQty = source.NewStockQty,\n",
    "    LastRestocked = current_date(),\n",
    "    RestockedRecently = source.RestockedRecently\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8abb0df9-d5ca-44d9-a8e9-a6dbcf8c7420",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+--------+------------+---------------+\n|    ItemName|   Category|StockQty|NeedsReorder|TotalStockValue|\n+------------+-----------+--------+------------+---------------+\n|      Laptop|Electronics|      40|       false|        2800000|\n|      LED TV|Electronics|      90|       false|        2700000|\n|Office Chair|  Furniture|      40|       false|         240000|\n|Refrigerator| Appliances|       5|        true|         125000|\n|     Printer|Electronics|      13|       false|         104000|\n+------------+-----------+--------+------------+---------------+\n\n+---------+------------+\n| Supplier|AvgUnitPrice|\n+---------+------------+\n|  ChairCo|      6000.0|\n|PrintFast|      8000.0|\n| FreezeIt|     25000.0|\n|   AVTech|     30000.0|\n|TechWorld|     70000.0|\n+---------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Scenario 7: Report Generation with SQL Views\n",
    "# Tasks:\n",
    "# 1. Create SQL view inventory_summary with: ItemName, Category, StockQty, NeedsReorder, TotalStockValue\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW inventory_summary AS\n",
    "SELECT\n",
    "  ItemName,\n",
    "  Category,\n",
    "  StockQty,\n",
    "  CASE WHEN StockQty < ReorderLevel THEN true ELSE false END AS NeedsReorder,\n",
    "  (StockQty * UnitPrice) AS TotalStockValue\n",
    "FROM retail_inventory\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM inventory_summary\").show()\n",
    "\n",
    "# 2. Create view supplier_leaderboard sorted by average price\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW supplier_leaderboard AS\n",
    "SELECT\n",
    "  Supplier,\n",
    "  ROUND(AVG(UnitPrice), 2) AS AvgUnitPrice\n",
    "FROM retail_inventory\n",
    "GROUP BY Supplier\n",
    "ORDER BY AvgUnitPrice ASC\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM supplier_leaderboard\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a058ff6-5a71-4183-9ec9-ebfec17c9824",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+--------+------------+-----------+\n|ItemID|    ItemName|StockQty|ReorderLevel|StockStatus|\n+------+------------+--------+------------+-----------+\n|  I001|      LED TV|      50|          20|Overstocked|\n|  I002|      Laptop|      10|          15|   LowStock|\n|  I003|Office Chair|      40|          10|Overstocked|\n|  I004|Refrigerator|       5|          10|   LowStock|\n|  I005|     Printer|       3|           5|   LowStock|\n+------+------------+--------+------------+-----------+\n\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+-----------+\n|ItemID|    ItemName|   Category| Warehouse|StockQty|ReorderLevel|LastRestocked|UnitPrice| Supplier|StockStatus|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+-----------+\n|  I002|      Laptop|Electronics|WarehouseB|      10|          15|   2024-04-01|    70000|TechWorld|   LowStock|\n|  I004|Refrigerator| Appliances|WarehouseC|       5|          10|   2024-02-20|    25000| FreezeIt|   LowStock|\n|  I005|     Printer|Electronics|WarehouseB|       3|           5|   2024-03-30|     8000|PrintFast|   LowStock|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+-----------+\n\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+-----------+\n|ItemID|    ItemName|   Category| Warehouse|StockQty|ReorderLevel|LastRestocked|UnitPrice| Supplier|StockStatus|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+-----------+\n|  I002|      Laptop|Electronics|WarehouseB|      10|          15|   2024-04-01|    70000|TechWorld|   LowStock|\n|  I004|Refrigerator| Appliances|WarehouseC|       5|          10|   2024-02-20|    25000| FreezeIt|   LowStock|\n|  I005|     Printer|Electronics|WarehouseB|       3|           5|   2024-03-30|     8000|PrintFast|   LowStock|\n+------+------------+-----------+----------+--------+------------+-------------+---------+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Scenario 8: Advanced Filtering\n",
    "# Tasks:\n",
    "# 1. Use when / otherwise to categorize items:\n",
    "# \"Overstocked\" (>2x ReorderLevel)\n",
    "# \"LowStock\"\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "categorized_df = df.withColumn(\n",
    "    \"StockStatus\",\n",
    "    when(col(\"StockQty\") > 2 * col(\"ReorderLevel\"), \"Overstocked\")\n",
    "    .when(col(\"StockQty\") < col(\"ReorderLevel\"), \"LowStock\")\n",
    "    .otherwise(\"Normal\")\n",
    ")\n",
    "\n",
    "categorized_df.select(\"ItemID\", \"ItemName\", \"StockQty\", \"ReorderLevel\", \"StockStatus\").show()\n",
    "\n",
    "# 2. Use .filter() and .where() for the same and compare.\n",
    "low_stock_items = categorized_df.filter(col(\"StockStatus\") == \"LowStock\")\n",
    "low_stock_items.show()\n",
    "\n",
    "low_stock_items_alt = categorized_df.where(col(\"StockStatus\") == \"LowStock\")\n",
    "low_stock_items_alt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "898db770-ad13-4437-b628-9d19533b9412",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+------------+\n|ItemID|LastRestocked|RestockMonth|\n+------+-------------+------------+\n|  I001|   2024-03-15|       March|\n|  I002|   2024-04-01|       April|\n|  I003|   2024-03-25|       March|\n|  I004|   2024-02-20|    February|\n|  I005|   2024-03-30|       March|\n+------+-------------+------------+\n\n+------+-------------+--------+\n|ItemID|LastRestocked|StockAge|\n+------+-------------+--------+\n|  I001|   2024-03-15|     461|\n|  I002|   2024-04-01|     444|\n|  I003|   2024-03-25|     451|\n|  I004|   2024-02-20|     485|\n|  I005|   2024-03-30|     446|\n+------+-------------+--------+\n\n+------+-------------+--------+----------------+\n|ItemID|LastRestocked|StockAge|StockAgeCategory|\n+------+-------------+--------+----------------+\n|  I001|   2024-03-15|     461|           Stale|\n|  I002|   2024-04-01|     444|           Stale|\n|  I003|   2024-03-25|     451|           Stale|\n|  I004|   2024-02-20|     485|           Stale|\n|  I005|   2024-03-30|     446|           Stale|\n+------+-------------+--------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Scenario 9: Feature Engineering\n",
    "# Tasks:\n",
    "# 1. Extract RestockMonth from LastRestocked .\n",
    "from pyspark.sql.functions import month, date_format\n",
    "\n",
    "# Use month name (e.g., \"March\")\n",
    "df_with_month = df.withColumn(\"RestockMonth\", date_format(\"LastRestocked\", \"MMMM\"))\n",
    "df_with_month.select(\"ItemID\", \"LastRestocked\", \"RestockMonth\").show()\n",
    "\n",
    "# 2. Create feature: StockAge = CURRENT_DATE - LastRestocked\n",
    "from pyspark.sql.functions import datediff, current_date\n",
    "\n",
    "df_with_age = df_with_month.withColumn(\n",
    "    \"StockAge\", datediff(current_date(), \"LastRestocked\")\n",
    ")\n",
    "\n",
    "df_with_age.select(\"ItemID\", \"LastRestocked\", \"StockAge\").show()\n",
    "\n",
    "# 3. Bucket StockAge into: New, Moderate, Stale\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df_bucketted = df_with_age.withColumn(\n",
    "    \"StockAgeCategory\",\n",
    "    when(col(\"StockAge\") < 30, \"New\")\n",
    "    .when((col(\"StockAge\") >= 30) & (col(\"StockAge\") <= 90), \"Moderate\")\n",
    "    .otherwise(\"Stale\")\n",
    ")\n",
    "\n",
    "df_bucketted.select(\"ItemID\", \"LastRestocked\", \"StockAge\", \"StockAgeCategory\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6899654-fbcd-4667-9143-4b4ada03deb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scenario 10: Export Options\n",
    "# Tasks:\n",
    "data = [\n",
    "    {\"Item\": \"item1\", \"StockAgeCategory\": \"Fresh\", \"Warehouse\": \"A\"},\n",
    "    {\"Item\": \"item2\", \"StockAgeCategory\": \"Stale\", \"Warehouse\": \"B\"},\n",
    "    {\"Item\": \"item3\", \"StockAgeCategory\": \"Stale\", \"Warehouse\": \"A\"},\n",
    "    {\"Item\": \"item4\", \"StockAgeCategory\": \"Fresh\", \"Warehouse\": \"B\"}\n",
    "]\n",
    "final_df = spark.createDataFrame(data)\n",
    "# 1. Write full DataFrame to:\n",
    "# CSV for analysts\n",
    "final_df.write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"header\", True) \\\n",
    "  .csv(\"/export/inventory/csv_full/\")\n",
    "\n",
    "# JSON for integration\n",
    "final_df.write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .json(\"/export/inventory/json_full/\")\n",
    "\n",
    "# Delta for pipelines\n",
    "final_df.write \\\n",
    "  .format(\"delta\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save(\"/export/inventory/delta_full/\")\n",
    "\n",
    "\n",
    "# 2. Save with meaningful file and partition names like /export/inventory/stale_items/\n",
    "stale_df = final_df.filter(final_df.StockAgeCategory == \"Stale\")\n",
    "\n",
    "stale_df.write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .partitionBy(\"Warehouse\") \\\n",
    "  .parquet(\"/export/inventory/stale_items/\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Retail Inventory & Supply Chain Intelligence 2025-06-19 11:52:24",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}