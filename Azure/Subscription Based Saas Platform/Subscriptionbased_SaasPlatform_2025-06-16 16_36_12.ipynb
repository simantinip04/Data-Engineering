{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f63935d-de5a-4c9f-8e94-c47307d8f848",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=4054280922637182#setting/sparkui/0611-041854-oedbfkos/driver-4544267575270260318\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x705c4c159cd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4a6a42b-b344-44f2-83a1-c6032f56465c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- SubscriptionID: string (nullable = true)\n |-- UserID: string (nullable = true)\n |-- PlanType: string (nullable = true)\n |-- StartDate: date (nullable = true)\n |-- EndDate: date (nullable = true)\n |-- PriceUSD: integer (nullable = true)\n |-- IsActive: boolean (nullable = true)\n |-- AutoRenew: boolean (nullable = true)\n\nroot\n |-- UserID: string (nullable = true)\n |-- EventTime: string (nullable = true)\n |-- EventType: string (nullable = true)\n |-- FeatureUsed: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#Load the data\n",
    "from pyspark.sql.functions import col\n",
    "subscriptions=spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"file:/Workspace/Shared/subscriptions.csv\")\n",
    "activity=spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"file:/Workspace/Shared/user_activity.csv\")\n",
    "\n",
    "subscriptions.printSchema()\n",
    "activity.printSchema()\n",
    "\n",
    "subscriptions=subscriptions.withColumn(\"StartDate\", col(\"StartDate\").cast(\"date\")) \\\n",
    "                             .withColumn(\"EndDate\", col(\"EndDate\").cast(\"date\")) \\\n",
    "                             .withColumn(\"PriceUSD\", col(\"PriceUSD\").cast(\"double\"))\n",
    "\n",
    "activity=activity.withColumn(\"EventTime\", col(\"EventTime\").cast(\"timestamp\"))\n",
    "\n",
    "subscriptions.createOrReplaceTempView(\"subscriptions\")\n",
    "activity.createOrReplaceTempView(\"activity\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c0d37e9-441f-4b3e-be26-ef9e35043dad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+--------+----------+----------+--------+--------+---------+-----------+\n|SubscriptionID|UserID|PlanType| StartDate|   EndDate|PriceUSD|IsActive|AutoRenew|active_days|\n+--------------+------+--------+----------+----------+--------+--------+---------+-----------+\n|        SUB001|  U001|   Basic|2024-01-01|2024-04-01|    30.0|    true|     true|         91|\n|        SUB002|  U002|     Pro|2024-02-15|2024-05-15|    90.0|    true|    false|         90|\n|        SUB003|  U003|     Pro|2024-03-10|2024-06-10|    90.0|   false|    false|         92|\n|        SUB004|  U001| Premium|2024-04-05|2024-07-05|   120.0|    true|     true|         91|\n|        SUB005|  U004|   Basic|2024-01-20|2024-04-20|    30.0|   false|    false|         91|\n+--------------+------+--------+----------+----------+--------+--------+---------+-----------+\n\n+------+---------------+\n|UserID|events_per_user|\n+------+---------------+\n|  U004|              1|\n|  U002|              1|\n|  U003|              1|\n|  U001|              2|\n+------+---------------+\n\n+------+--------+--------+-----------+---------------+------------------+\n|UserID|PlanType|PriceUSD|active_days|events_per_user|  engagement_score|\n+------+--------+--------+-----------+---------------+------------------+\n|  U001|   Basic|    30.0|         91|              2|0.6593406593406594|\n|  U002|     Pro|    90.0|         90|              1|               1.0|\n|  U003|     Pro|    90.0|         92|              1|0.9782608695652174|\n|  U001| Premium|   120.0|         91|              2|2.6373626373626378|\n|  U004|   Basic|    30.0|         91|              1|0.3296703296703297|\n+------+--------+--------+-----------+---------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff, count, col\n",
    "# Calculate:\n",
    "# active_days = EndDate - StartDate\n",
    "subscriptions_with_days = subscriptions.withColumn(\n",
    "    \"active_days\", datediff(\"EndDate\", \"StartDate\")\n",
    ")\n",
    "subscriptions_with_days.show()\n",
    "\n",
    "# events_per_user = count(EventType) grouped by UserID\n",
    "events_per_user = activity.groupBy(\"UserID\").agg(\n",
    "    count(\"EventType\").alias(\"events_per_user\")\n",
    ")\n",
    "events_per_user.show()\n",
    "\n",
    "# Create a score: engagement_score = (events_per_user / active_days) * PriceUSD\n",
    "combined_df = subscriptions_with_days.join(events_per_user, on=\"UserID\", how=\"inner\")\n",
    "\n",
    "combined_df = combined_df.withColumn(\n",
    "    \"engagement_score\", (col(\"events_per_user\") / col(\"active_days\")) * col(\"PriceUSD\")\n",
    ")\n",
    "combined_df.select(\"UserID\", \"PlanType\", \"PriceUSD\", \"active_days\", \"events_per_user\", \"engagement_score\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ddea351-befb-4ca0-8898-feeba60081ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inactive but recently active:\n+------+--------+---------+\n|UserID|IsActive|LastEvent|\n+------+--------+---------+\n+------+--------+---------+\n\nAutoRenew true but no activity in last 30 days:\n+------+---------+\n|UserID|LastEvent|\n+------+---------+\n|  U001|     NULL|\n+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# B. Anomaly Detection via SQL\n",
    "# Identify users with:\n",
    "# Subscription inactive but recent activity\n",
    "#Inactive but recently active\n",
    "print(\"Inactive but recently active:\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT s.UserID, s.IsActive, MAX(a.EventTime) AS LastEvent\n",
    "FROM subscriptions s\n",
    "LEFT JOIN activity a ON s.UserID = a.UserID\n",
    "GROUP BY s.UserID, s.IsActive\n",
    "HAVING s.IsActive = false AND LastEvent > current_date() - INTERVAL 7 DAYS\n",
    "\"\"\").show()\n",
    "\n",
    "#AutoRenew true but no activity in last 30 days\n",
    "print(\"AutoRenew true but no activity in last 30 days:\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT s.UserID, MAX(a.EventTime) AS LastEvent\n",
    "FROM subscriptions s\n",
    "LEFT JOIN activity a ON s.UserID = a.UserID\n",
    "WHERE s.AutoRenew = true\n",
    "GROUP BY s.UserID\n",
    "HAVING LastEvent < current_date() - INTERVAL 30 DAYS OR LastEvent IS NULL\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "894ce474-5fef-4f10-925d-3950d0c72e95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+----------------+-----------------+\n|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n+-----------------+----------------+----------------+-----------------+\n|                1|               1|               0|                0|\n+-----------------+----------------+----------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# C. Delta Lake + Merge Simulation\n",
    "# Imagine a billing fix needs to be applied:\n",
    "# For all Pro plans in March, increase price by $5 retroactively.\n",
    "# Use MERGE INTO on Delta table to apply the change.\n",
    "subscriptions.write.format(\"delta\").mode(\"overwrite\").save(\"file:/Workspace/Shared/subscriptions\")\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO delta.`file:/Workspace/Shared/subscriptions` target\n",
    "USING (\n",
    "  SELECT * FROM delta.`file:/Workspace/Shared/subscriptions` \n",
    "  WHERE PlanType = 'Pro' AND month(StartDate) = 3\n",
    ") src\n",
    "ON target.SubscriptionID = src.SubscriptionID\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET target.PriceUSD = target.PriceUSD + 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e044554-5390-4855-bcf1-eee723f147a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|version|           timestamp|          userId|            userName|operation| operationParameters| job|          notebook|           clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+--------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|      1|2025-06-16 11:29:...|3955481281677681|azuser3561_mml.lo...|    MERGE|{predicate -> [\"(...|NULL|{2045471936929044}|0611-041854-oedbfkos|          0|WriteSerializable|        false|{numTargetRowsCop...|        NULL|Databricks-Runtim...|\n|      0|2025-06-16 11:29:...|3955481281677681|azuser3561_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{2045471936929044}|0611-041854-oedbfkos|       NULL|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n+-------+--------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n\n+--------------+------+--------+----------+----------+--------+--------+---------+\n|SubscriptionID|UserID|PlanType| StartDate|   EndDate|PriceUSD|IsActive|AutoRenew|\n+--------------+------+--------+----------+----------+--------+--------+---------+\n|        SUB002|  U002|     Pro|2024-02-15|2024-05-15|    90.0|    true|    false|\n|        SUB003|  U003|     Pro|2024-03-10|2024-06-10|    90.0|   false|    false|\n+--------------+------+--------+----------+----------+--------+--------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# D. Time Travel Debugging\n",
    "# Show describe history of the table before and after the billing fix.\n",
    "# Query using VERSION AS OF to prove the issue existed.\n",
    "spark.sql(\"DESCRIBE HISTORY delta.`file:/Workspace/Shared/subscriptions`\").show()\n",
    "\n",
    "old=spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"file:/Workspace/Shared/subscriptions\")\n",
    "old.filter(col(\"PlanType\") == \"Pro\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "070889c5-a03d-46b1-a1b3-3c77d6e7fc63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+--------+---------+-------+--------+--------+---------+--------+\n|SubscriptionID|UserID|PlanType|StartDate|EndDate|PriceUSD|IsActive|AutoRenew|PrevPlan|\n+--------------+------+--------+---------+-------+--------+--------+---------+--------+\n+--------------+------+--------+---------+-------+--------+--------+---------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# E. Build Tier Migration Table\n",
    "# Identify users who upgraded:\n",
    "# From Basic → Pro → Premium\n",
    "# Use PySpark with lag() function to model this.\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag\n",
    "w = Window.partitionBy(\"UserID\").orderBy(\"StartDate\")\n",
    "migration = subscriptions.withColumn(\"PrevPlan\", lag(\"PlanType\").over(w))\n",
    "migration.filter(\n",
    "    (col(\"PrevPlan\") == \"Basic\") & (col(\"PlanType\") == \"Pro\")\n",
    ").show()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c02dd45-508e-4d41-b7b7-008460a35a0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n|UserID|feature_count|\n+------+-------------+\n|  U004|            1|\n|  U002|            1|\n|  U003|            1|\n|  U001|            1|\n+------+-------------+\n\n+------+-----------+\n|UserID|login_count|\n+------+-----------+\n|  U004|          1|\n|  U001|          1|\n+------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# F. Power Users Detection\n",
    "# Define a power user as:\n",
    "# Used ≥ 2 features\n",
    "# Logged in ≥ 3 times\n",
    "# Create a separate Delta table power_users\n",
    "\n",
    "from pyspark.sql.functions import countDistinct, col\n",
    "feature = activity.groupBy(\"UserID\").agg(countDistinct(\"FeatureUsed\").alias(\"feature_count\"))\n",
    "feature.show()\n",
    "login = activity.filter(col(\"EventType\") == \"login\").groupBy(\"UserID\").count().withColumnRenamed(\"count\", \"login_count\")\n",
    "login.show()\n",
    "power_users = feature.join(login, \"UserID\") \\\n",
    "    .filter((col(\"feature_count\") >= 2) & (col(\"login_count\") >= 3))\n",
    "power_users.write.format(\"delta\").mode(\"overwrite\").save(\"file:/Workspace/Shared/power_users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9a736cf-82c8-4afe-9bd5-e20ffdcf9105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sessions:\n+------+---------+---------+---------------+\n|UserID|EventTime|EventType|SessionDuration|\n+------+---------+---------+---------------+\n|  U001|     NULL|    login|           NULL|\n|  U001|     NULL|   logout|           NULL|\n|  U002|     NULL|   upload|           NULL|\n|  U003|     NULL| download|           NULL|\n|  U004|     NULL|    login|           NULL|\n+------+---------+---------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# G. Session Replay View\n",
    "# Build a user session trace table using:\n",
    "# Window.partitionBy(\"UserID\").orderBy(\"EventTime\")\n",
    "# Show how long each user spent between login and logout events.\n",
    "from pyspark.sql.functions import lag, unix_timestamp, when\n",
    "window=Window.partitionBy(\"UserID\").orderBy(\"EventTime\")\n",
    "sessions=activity.withColumn(\"PrevEvent\", lag(\"EventType\").over(window)) \\\n",
    "                   .withColumn(\"PrevTime\", lag(\"EventTime\").over(window)) \\\n",
    "                   .withColumn(\"SessionDuration\", \n",
    "                        when((col(\"PrevEvent\") == \"login\") & (col(\"EventType\") == \"logout\"),\n",
    "                             unix_timestamp(\"EventTime\") - unix_timestamp(\"PrevTime\"))\n",
    "                    )\n",
    "print(\"Sessions:\")\n",
    "sessions.select(\"UserID\", \"EventTime\", \"EventType\", \"SessionDuration\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Subscriptionbased_SaasPlatform_2025-06-16 16:36:12",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}