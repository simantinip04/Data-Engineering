{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a544379d-e7f0-425c-8e0d-74cf0ff3df9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=4054280922637182#setting/sparkui/0611-041854-oedbfkos/driver-3936340718372101785\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ba7cc241e10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76b91529-9fb9-443b-b64d-b02921b7eb7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+--------------------+-----------+----------+---------------+------+---------+\n|EnrollmentID|StudentName|          CourseName|   Category|EnrollDate|ProgressPercent|Rating|   Status|\n+------------+-----------+--------------------+-----------+----------+---------------+------+---------+\n|      ENR001|     Aditya|Python for Beginners|Programming|2024-05-10|             80|   4.5|   Active|\n|      ENR002|     Simran|Data Analysis wit...|  Analytics|2024-05-12|            100|   4.7|Completed|\n|      ENR003|     Aakash| Power BI Essentials|  Analytics|2024-05-13|             30|   3.8|   Active|\n|      ENR004|       Neha|         Java Basics|Programming|2024-05-15|              0|  NULL| Inactive|\n|      ENR005|       Zara|Machine Learning 101|         AI|2024-05-17|             60|   4.2|   Active|\n|      ENR006|    Ibrahim|Python for Beginners|Programming|2024-05-18|             90|   4.6|Completed|\n+------------+-----------+--------------------+-----------+----------+---------------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\n",
    "  \"fs.azure.account.key.hexastore1234.blob.core.windows.net\",\n",
    "  \"Zo+EUYTy5ACS3dZ3yg01mSV+Gc8Ts4FtPL1zxI9ohStKv1El9YjmtvVTz9Om8q7H9bvfQNMk5rSK+ASt6al4Ng==\"\n",
    ")\n",
    " \n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\n",
    "  \"wasbs://data@hexastore1234.blob.core.windows.net/course_enrollments.csv\"\n",
    ")\n",
    " \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3df8b611-4a32-4ea9-ba67-633471664fb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- EnrollmentID: string (nullable = true)\n |-- StudentName: string (nullable = true)\n |-- CourseName: string (nullable = true)\n |-- Category: string (nullable = true)\n |-- EnrollDate: date (nullable = true)\n |-- ProgressPercent: integer (nullable = true)\n |-- Rating: double (nullable = true)\n |-- Status: string (nullable = true)\n\nroot\n |-- EnrollmentID: string (nullable = true)\n |-- StudentName: string (nullable = true)\n |-- CourseName: string (nullable = true)\n |-- Category: string (nullable = true)\n |-- EnrollDate: date (nullable = true)\n |-- ProgressPercent: integer (nullable = true)\n |-- Rating: double (nullable = true)\n |-- Status: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#Exercise Set – Online Course Use Case\n",
    "# Data Loading\n",
    "# 1. Load the data with schema inference enabled.\n",
    "df_inferred = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\n",
    "  \"wasbs://data@hexastore1234.blob.core.windows.net/course_enrollments.csv\"\n",
    ")\n",
    "df_inferred.printSchema()\n",
    "\n",
    "# 2. Manually define schema and compare both approaches.\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "manual_schema = StructType([\n",
    "    StructField(\"EnrollmentID\", StringType(), True),\n",
    "    StructField(\"StudentName\", StringType(), True),\n",
    "    StructField(\"CourseName\", StringType(), True),\n",
    "    StructField(\"Category\", StringType(), True),\n",
    "    StructField(\"EnrollDate\", DateType(), True),\n",
    "    StructField(\"ProgressPercent\", IntegerType(), True),\n",
    "    StructField(\"Rating\", DoubleType(), True),\n",
    "    StructField(\"Status\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_manual = spark.read.option(\"header\", True).schema(manual_schema).csv(\n",
    "  \"wasbs://data@hexastore1234.blob.core.windows.net/course_enrollments.csv\"\n",
    ")\n",
    "df_manual.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6241e16f-9ecb-43fb-830d-4390bf6968e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-------------------+-----------+----------+---------------+------+--------+\n|EnrollmentID|StudentName|         CourseName|   Category|EnrollDate|ProgressPercent|Rating|  Status|\n+------------+-----------+-------------------+-----------+----------+---------------+------+--------+\n|      ENR003|     Aakash|Power BI Essentials|  Analytics|2024-05-13|             30|   3.8|  Active|\n|      ENR004|       Neha|        Java Basics|Programming|2024-05-15|              0|  NULL|Inactive|\n+------------+-----------+-------------------+-----------+----------+---------------+------+--------+\n\n+------------+-----------+--------------------+-----------+----------+---------------+-----------------+---------+\n|EnrollmentID|StudentName|          CourseName|   Category|EnrollDate|ProgressPercent|           Rating|   Status|\n+------------+-----------+--------------------+-----------+----------+---------------+-----------------+---------+\n|      ENR001|     Aditya|Python for Beginners|Programming|2024-05-10|             80|              4.5|   Active|\n|      ENR002|     Simran|Data Analysis wit...|  Analytics|2024-05-12|            100|              4.7|Completed|\n|      ENR003|     Aakash| Power BI Essentials|  Analytics|2024-05-13|             30|              3.8|   Active|\n|      ENR004|       Neha|         Java Basics|Programming|2024-05-15|              0|4.359999999999999| Inactive|\n|      ENR005|       Zara|Machine Learning 101|         AI|2024-05-17|             60|              4.2|   Active|\n|      ENR006|    Ibrahim|Python for Beginners|Programming|2024-05-18|             90|              4.6|Completed|\n+------------+-----------+--------------------+-----------+----------+---------------+-----------------+---------+\n\n+------------+-----------+--------------------+-----------+----------+---------------+-----------------+---------+--------+\n|EnrollmentID|StudentName|          CourseName|   Category|EnrollDate|ProgressPercent|           Rating|   Status|IsActive|\n+------------+-----------+--------------------+-----------+----------+---------------+-----------------+---------+--------+\n|      ENR001|     Aditya|Python for Beginners|Programming|2024-05-10|             80|              4.5|   Active|       1|\n|      ENR002|     Simran|Data Analysis wit...|  Analytics|2024-05-12|            100|              4.7|Completed|       0|\n|      ENR003|     Aakash| Power BI Essentials|  Analytics|2024-05-13|             30|              3.8|   Active|       1|\n|      ENR004|       Neha|         Java Basics|Programming|2024-05-15|              0|4.359999999999999| Inactive|       0|\n|      ENR005|       Zara|Machine Learning 101|         AI|2024-05-17|             60|              4.2|   Active|       1|\n|      ENR006|    Ibrahim|Python for Beginners|Programming|2024-05-18|             90|              4.6|Completed|       0|\n+------------+-----------+--------------------+-----------+----------+---------------+-----------------+---------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Filtering and Transformation\n",
    "# 3. Filter records where ProgressPercent < 50 .\n",
    "ppgt50 = df.filter(df.ProgressPercent < 50)\n",
    "ppgt50.show()\n",
    "\n",
    "# 4. Replace null ratings with average rating.\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "avg_rating = df.select(avg(\"Rating\")).first()[0]\n",
    "replace_null = df.na.fill({'Rating': avg_rating})\n",
    "replace_null.show()\n",
    "\n",
    "# 5. Add column IsActive → 1 if Status is Active, else 0.\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df_status = replace_null.withColumn(\"IsActive\", when(col(\"Status\") == \"Active\", 1).otherwise(0))\n",
    "df_status.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d340922-f800-4e6c-bff6-12bd23d4315a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n|          CourseName|avg(ProgressPercent)|\n+--------------------+--------------------+\n|Data Analysis wit...|               100.0|\n|         Java Basics|                 0.0|\n|Machine Learning 101|                60.0|\n|Python for Beginners|                85.0|\n| Power BI Essentials|                30.0|\n+--------------------+--------------------+\n\n+-----------+-----+\n|   Category|count|\n+-----------+-----+\n|Programming|    3|\n|         AI|    1|\n|  Analytics|    2|\n+-----------+-----+\n\n+--------------------+-----+\n|          CourseName|count|\n+--------------------+-----+\n|Python for Beginners|    2|\n+--------------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Aggregations & Metrics\n",
    "# 6. Find average progress by course.\n",
    "avg_prog_course = df_status.groupBy(\"CourseName\").avg(\"ProgressPercent\")\n",
    "avg_prog_course.show()\n",
    "\n",
    "# 7. Get count of students in each course category.\n",
    "count_category = df_status.groupBy(\"Category\").count()\n",
    "count_category.show()\n",
    "\n",
    "# 8. Identify the most enrolled course.\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "high_enrolled_course = df_status.groupBy(\"CourseName\").count().orderBy(desc(\"count\")).limit(1)\n",
    "high_enrolled_course.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6034b557-f4e3-45b7-bc68-1cb97dbaad34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----------+\n|          CourseName|DurationWeeks|Instructor|\n+--------------------+-------------+----------+\n|Python for Beginners|            4|    Rakesh|\n|Data Analysis wit...|            3|    Anjali|\n| Power BI Essentials|            5|     Rekha|\n|         Java Basics|            6|     Manoj|\n|Machine Learning 101|            8|     Samir|\n+--------------------+-------------+----------+\n\n+--------------------+------------+-----------+-----------+----------+---------------+-----------------+---------+--------+-------------+----------+\n|          CourseName|EnrollmentID|StudentName|   Category|EnrollDate|ProgressPercent|           Rating|   Status|IsActive|DurationWeeks|Instructor|\n+--------------------+------------+-----------+-----------+----------+---------------+-----------------+---------+--------+-------------+----------+\n|Python for Beginners|      ENR001|     Aditya|Programming|2024-05-10|             80|              4.5|   Active|       1|            4|    Rakesh|\n|Data Analysis wit...|      ENR002|     Simran|  Analytics|2024-05-12|            100|              4.7|Completed|       0|            3|    Anjali|\n| Power BI Essentials|      ENR003|     Aakash|  Analytics|2024-05-13|             30|              3.8|   Active|       1|            5|     Rekha|\n|         Java Basics|      ENR004|       Neha|Programming|2024-05-15|              0|4.359999999999999| Inactive|       0|            6|     Manoj|\n|Machine Learning 101|      ENR005|       Zara|         AI|2024-05-17|             60|              4.2|   Active|       1|            8|     Samir|\n|Python for Beginners|      ENR006|    Ibrahim|Programming|2024-05-18|             90|              4.6|Completed|       0|            4|    Rakesh|\n+--------------------+------------+-----------+-----------+----------+---------------+-----------------+---------+--------+-------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Joins\n",
    "# 9. Create second CSV: course_details.csv\n",
    "#         CourseName,DurationWeeks,Instructor\n",
    "#         Python for Beginners,4,Rakesh\n",
    "#         Data Analysis with Excel,3,Anjali\n",
    "#         Power BI Essentials,5,Rekha\n",
    "#         Java Basics,6,Manoj\n",
    "#         Machine Learning 101,8,Samir\n",
    "df_course_details = spark.read.option(\"header\", True).csv(\n",
    "  \"wasbs://data@hexastore1234.blob.core.windows.net/course_details.csv\"\n",
    ")\n",
    "df_course_details.show()\n",
    "\n",
    "# 10. Join course_enrollments with course_details to include duration and instructor.\n",
    "df_joined = df_status.join(df_course_details, on=\"CourseName\", how=\"left\")\n",
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c054a2c-b018-4b37-b003-86ee864d91dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+---------------+----+\n|StudentName|          CourseName|ProgressPercent|Rank|\n+-----------+--------------------+---------------+----+\n|     Simran|Data Analysis wit...|            100|   1|\n|       Neha|         Java Basics|              0|   1|\n|       Zara|Machine Learning 101|             60|   1|\n|     Aakash| Power BI Essentials|             30|   1|\n|    Ibrahim|Python for Beginners|             90|   1|\n|     Aditya|Python for Beginners|             80|   2|\n+-----------+--------------------+---------------+----+\n\n+-----------+-----------+----------+----------+----------+\n|StudentName|   Category|EnrollDate|  LeadDate|   LagDate|\n+-----------+-----------+----------+----------+----------+\n|       Zara|         AI|2024-05-17|      NULL|      NULL|\n|     Simran|  Analytics|2024-05-12|2024-05-13|      NULL|\n|     Aakash|  Analytics|2024-05-13|      NULL|2024-05-12|\n|     Aditya|Programming|2024-05-10|2024-05-15|      NULL|\n|       Neha|Programming|2024-05-15|2024-05-18|2024-05-10|\n|    Ibrahim|Programming|2024-05-18|      NULL|2024-05-15|\n+-----------+-----------+----------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Window Functions\n",
    "# 11. Rank students in each course based on ProgressPercent .\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "window_spec = Window.partitionBy(\"CourseName\").orderBy(col(\"ProgressPercent\").desc())\n",
    "df_ranked = df_joined.withColumn(\"Rank\", rank().over(window_spec))\n",
    "df_ranked.select(\"StudentName\", \"CourseName\", \"ProgressPercent\", \"Rank\").show()\n",
    "\n",
    "# 12. Get lead and lag of EnrollDate by Category.\n",
    "from pyspark.sql.functions import lead, lag\n",
    "\n",
    "window_cat = Window.partitionBy(\"Category\").orderBy(\"EnrollDate\")\n",
    "df_leadlag = df_joined.withColumn(\"LeadDate\", lead(\"EnrollDate\", 1).over(window_cat)) \\\n",
    "                      .withColumn(\"LagDate\", lag(\"EnrollDate\", 1).over(window_cat))\n",
    "df_leadlag.select(\"StudentName\", \"Category\", \"EnrollDate\", \"LeadDate\", \"LagDate\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "285f6ff0-fb45-4d20-b63b-e75f6648acbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+---------+--------+\n|   Category|Active|Completed|Inactive|\n+-----------+------+---------+--------+\n|Programming|     1|        1|       1|\n|         AI|     1|     NULL|    NULL|\n|  Analytics|     1|        1|    NULL|\n+-----------+------+---------+--------+\n\n+------------+----------+----------+-----------+\n|EnrollmentID|EnrollDate|EnrollYear|EnrollMonth|\n+------------+----------+----------+-----------+\n|      ENR001|2024-05-10|      2024|          5|\n|      ENR002|2024-05-12|      2024|          5|\n|      ENR003|2024-05-13|      2024|          5|\n|      ENR004|2024-05-15|      2024|          5|\n|      ENR005|2024-05-17|      2024|          5|\n|      ENR006|2024-05-18|      2024|          5|\n+------------+----------+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Pivoting & Formatting\n",
    "# 13. Pivot data to show total enrollments by Category and Status.\n",
    "df_pivot = df_joined.groupBy(\"Category\").pivot(\"Status\").count()\n",
    "df_pivot.show()\n",
    "\n",
    "# 14. Extract year and month from EnrollDate .\n",
    "from pyspark.sql.functions import year, month\n",
    "\n",
    "df_dated = df_joined.withColumn(\"EnrollYear\", year(\"EnrollDate\")) \\\n",
    "                    .withColumn(\"EnrollMonth\", month(\"EnrollDate\"))\n",
    "df_dated.select(\"EnrollmentID\", \"EnrollDate\", \"EnrollYear\", \"EnrollMonth\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abe9ab97-6886-4848-9636-655b1d8f8f34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----------+-----------+----------+---------------+-----------------+---------+--------+-------------+----------+----------+-----------+\n|          CourseName|EnrollmentID|StudentName|   Category|EnrollDate|ProgressPercent|           Rating|   Status|IsActive|DurationWeeks|Instructor|EnrollYear|EnrollMonth|\n+--------------------+------------+-----------+-----------+----------+---------------+-----------------+---------+--------+-------------+----------+----------+-----------+\n|Python for Beginners|      ENR001|     Aditya|Programming|2024-05-10|             80|              4.5|   Active|       1|            4|    Rakesh|      2024|          5|\n|Data Analysis wit...|      ENR002|     Simran|  Analytics|2024-05-12|            100|              4.7|Completed|       0|            3|    Anjali|      2024|          5|\n| Power BI Essentials|      ENR003|     Aakash|  Analytics|2024-05-13|             30|              3.8|   Active|       1|            5|     Rekha|      2024|          5|\n|         Java Basics|      ENR004|       Neha|Programming|2024-05-15|              0|4.359999999999999| Inactive|       0|            6|     Manoj|      2024|          5|\n|Machine Learning 101|      ENR005|       Zara|         AI|2024-05-17|             60|              4.2|   Active|       1|            8|     Samir|      2024|          5|\n|Python for Beginners|      ENR006|    Ibrahim|Programming|2024-05-18|             90|              4.6|Completed|       0|            4|    Rakesh|      2024|          5|\n+--------------------+------------+-----------+-----------+----------+---------------+-----------------+---------+--------+-------------+----------+----------+-----------+\n\n+--------------------+------------+-----------+-----------+----------+---------------+-----------------+---------+--------+-------------+----------+----------+-----------+\n|          CourseName|EnrollmentID|StudentName|   Category|EnrollDate|ProgressPercent|           Rating|   Status|IsActive|DurationWeeks|Instructor|EnrollYear|EnrollMonth|\n+--------------------+------------+-----------+-----------+----------+---------------+-----------------+---------+--------+-------------+----------+----------+-----------+\n|         Java Basics|      ENR004|       Neha|Programming|2024-05-15|              0|4.359999999999999| Inactive|       0|            6|     Manoj|      2024|          5|\n|Machine Learning 101|      ENR005|       Zara|         AI|2024-05-17|             60|              4.2|   Active|       1|            8|     Samir|      2024|          5|\n|Data Analysis wit...|      ENR002|     Simran|  Analytics|2024-05-12|            100|              4.7|Completed|       0|            3|    Anjali|      2024|          5|\n| Power BI Essentials|      ENR003|     Aakash|  Analytics|2024-05-13|             30|              3.8|   Active|       1|            5|     Rekha|      2024|          5|\n|Python for Beginners|      ENR006|    Ibrahim|Programming|2024-05-18|             90|              4.6|Completed|       0|            4|    Rakesh|      2024|          5|\n|Python for Beginners|      ENR001|     Aditya|Programming|2024-05-10|             80|              4.5|   Active|       1|            4|    Rakesh|      2024|          5|\n+--------------------+------------+-----------+-----------+----------+---------------+-----------------+---------+--------+-------------+----------+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Cleaning and Deduplication\n",
    "# 15. Drop rows where Status is null or empty.\n",
    "df_clean = df_dated.filter((col(\"Status\").isNotNull()) & (col(\"Status\") != \"\"))\n",
    "df_clean.show()\n",
    "\n",
    "# 16. Remove duplicate enrollments using dropDuplicates() . \n",
    "df_final = df_clean.dropDuplicates([\"EnrollmentID\"])\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac79cb30-dbc8-474f-ae95-950b58ff1ff3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Export\n",
    "# 17. Write the final cleaned DataFrame to:\n",
    "# CSV (overwrite mode)\n",
    "df_final.write.mode(\"overwrite\").option(\"header\", True).csv(\n",
    "  \"wasbs://data@hexastore1234.blob.core.windows.net/output/final_csv\"\n",
    ")\n",
    "\n",
    "# JSON (overwrite mode)\n",
    "df_final.write.mode(\"overwrite\").json(\n",
    "  \"wasbs://data@hexastore1234.blob.core.windows.net/output/final_json\"\n",
    ")\n",
    "\n",
    "# Parquet (snappy compression)\n",
    "df_final.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet(\n",
    "  \"wasbs://data@hexastore1234.blob.core.windows.net/output/final_parquet\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Online_Course_Enrollments_2025-06-16 10:03:18",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}