{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e08d4b30-7279-4bd2-b8ca-4d88eecd832b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=4054280922637182#setting/sparkui/0611-041854-oedbfkos/driver-3616197065006495200\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x75a0d81e0c10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"Combining Existing Data\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1440ad6-bb3f-4abd-88ab-821b69fb5dac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+\n|  Name| Department|Salary|\n+------+-----------+------+\n|Ananya|         HR| 52000|\n| Rahul|Engineering| 65000|\n| Priya|Engineering| 60000|\n|  Zoya|  Marketing| 48000|\n| Karan|         HR| 53000|\n|Naveen|Engineering| 70000|\n|Fatima|  Marketing| 45000|\n+------+-----------+------+\n\n+------+----+------+\n|  Name|Year|Rating|\n+------+----+------+\n|Ananya|2023|   4.5|\n| Rahul|2023|   4.9|\n| Priya|2023|   4.3|\n|  Zoya|2023|   3.8|\n| Karan|2023|   4.1|\n|Naveen|2023|   4.7|\n|Fatima|2023|   3.9|\n+------+----+------+\n\n+------+----------------+-----------+\n|  Name|         Project|HoursWorked|\n+------+----------------+-----------+\n|Ananya|       HR Portal|        120|\n| Rahul|   Data Platform|        200|\n| Priya|   Data Platform|        180|\n|  Zoya|Campaign Tracker|        100|\n| Karan|       HR Portal|        130|\n|Naveen|     ML Pipeline|        220|\n|Fatima|Campaign Tracker|         90|\n+------+----------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Employee Data\n",
    "employee_data = [\n",
    "    (\"Ananya\", \"HR\", 52000),\n",
    "    (\"Rahul\", \"Engineering\", 65000),\n",
    "    (\"Priya\", \"Engineering\", 60000),\n",
    "    (\"Zoya\", \"Marketing\", 48000),\n",
    "    (\"Karan\", \"HR\", 53000),\n",
    "    (\"Naveen\", \"Engineering\", 70000),\n",
    "    (\"Fatima\", \"Marketing\", 45000)\n",
    "]\n",
    "columns_emp = [\"Name\", \"Department\", \"Salary\"]\n",
    "df_emp = spark.createDataFrame(employee_data, columns_emp)\n",
    "df_emp.show()\n",
    "\n",
    "# Performance Data\n",
    "performance_data = [\n",
    "    (\"Ananya\", 2023, 4.5),\n",
    "    (\"Rahul\",  2023, 4.9),\n",
    "    (\"Priya\", 2023, 4.3),\n",
    "    (\"Zoya\", 2023, 3.8),\n",
    "    (\"Karan\", 2023, 4.1),\n",
    "    (\"Naveen\", 2023, 4.7),\n",
    "    (\"Fatima\", 2023, 3.9)\n",
    "]\n",
    "columns_perf = [\"Name\", \"Year\", \"Rating\"]\n",
    "df_perf = spark.createDataFrame(performance_data, columns_perf)\n",
    "df_perf.show()\n",
    "\n",
    "# Project Data\n",
    "project_data = [\n",
    "    (\"Ananya\", \"HR Portal\", 120),\n",
    "    (\"Rahul\", \"Data Platform\", 200),\n",
    "    (\"Priya\", \"Data Platform\", 180),\n",
    "    (\"Zoya\", \"Campaign Tracker\", 100),\n",
    "    (\"Karan\", \"HR Portal\", 130),\n",
    "    (\"Naveen\", \"ML Pipeline\", 220),\n",
    "    (\"Fatima\", \"Campaign Tracker\", 90)\n",
    "]\n",
    "columns_proj = [\"Name\", \"Project\", \"HoursWorked\"]\n",
    "df_proj = spark.createDataFrame(project_data, columns_proj)\n",
    "df_proj.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaf19ad7-cf9d-49d5-a132-42892dd8494d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----+------+----------------+-----------+\n|  Name| Department|Salary|Year|Rating|         Project|HoursWorked|\n+------+-----------+------+----+------+----------------+-----------+\n|Ananya|         HR| 52000|2023|   4.5|       HR Portal|        120|\n| Rahul|Engineering| 65000|2023|   4.9|   Data Platform|        200|\n| Priya|Engineering| 60000|2023|   4.3|   Data Platform|        180|\n|  Zoya|  Marketing| 48000|2023|   3.8|Campaign Tracker|        100|\n| Karan|         HR| 53000|2023|   4.1|       HR Portal|        130|\n|Naveen|Engineering| 70000|2023|   4.7|     ML Pipeline|        220|\n|Fatima|  Marketing| 45000|2023|   3.9|Campaign Tracker|         90|\n+------+-----------+------+----+------+----------------+-----------+\n\n+-----------+----------+\n| Department|TotalHours|\n+-----------+----------+\n|         HR|       250|\n|Engineering|       600|\n|  Marketing|       190|\n+-----------+----------+\n\n+----------------+------------------+\n|         Project|         AvgRating|\n+----------------+------------------+\n|       HR Portal|               4.3|\n|   Data Platform|               4.6|\n|Campaign Tracker|3.8499999999999996|\n|     ML Pipeline|               4.7|\n+----------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Joins and Advanced Aggregations\n",
    "from pyspark.sql.functions import sum, avg\n",
    "# 1. Join employee_data , performance_data , and project_data .\n",
    "df_joined = df_emp.join(df_perf, on=\"Name\").join(df_proj, on=\"Name\")\n",
    "df_joined.show()\n",
    "\n",
    "# 2. Compute total hours worked per department.\n",
    "df_hours_dept = df_joined.groupBy(\"Department\").agg(sum(\"HoursWorked\").alias(\"TotalHours\"))\n",
    "df_hours_dept.show()\n",
    "\n",
    "# 3. Compute average rating per project.\n",
    "df_avg_rating_proj = df_joined.groupBy(\"Project\").agg(avg(\"Rating\").alias(\"AvgRating\"))\n",
    "df_avg_rating_proj.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b50af516-1546-4672-a3e3-9724225dbd5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+\n|  Name|Year|Rating|\n+------+----+------+\n|Ananya|2023|   4.5|\n| Rahul|2023|   4.9|\n| Priya|2023|   4.3|\n|  Zoya|2023|   3.8|\n| Karan|2023|   4.1|\n|Naveen|2023|   4.7|\n|Fatima|2023|   3.9|\n| Meena|2023|  NULL|\n+------+----+------+\n\n+-----+----+------+\n| Name|Year|Rating|\n+-----+----+------+\n|Meena|2023|  NULL|\n+-----+----+------+\n\n+------+-----------+------+------------+\n|  Name| Department|Rating|RatingFilled|\n+------+-----------+------+------------+\n|Ananya|         HR|   4.5|         4.5|\n| Rahul|Engineering|   4.9|         4.9|\n| Priya|Engineering|   4.3|         4.3|\n|  Zoya|  Marketing|   3.8|         3.8|\n| Karan|         HR|   4.1|         4.1|\n|Naveen|Engineering|   4.7|         4.7|\n|Fatima|  Marketing|   3.9|         3.9|\n| Meena|       NULL|  NULL|        NULL|\n+------+-----------+------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Handling Missing Data\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import when, avg\n",
    "# 4. Add a row to performance_data with a None rating.\n",
    "new_perf_row = Row(\"Meena\", 2023, None)\n",
    "df_perf_with_null = df_perf.union(spark.createDataFrame([new_perf_row], df_perf.schema))\n",
    "df_perf_with_null.show()\n",
    "\n",
    "# 5. Filter rows with null values.\n",
    "df_perf_with_null.filter(df_perf_with_null[\"Rating\"].isNull()).show()\n",
    "\n",
    "# 6. Replace null ratings with the department average.\n",
    "# Join to get department info\n",
    "df_perf_joined = df_perf_with_null.join(df_emp, on=\"Name\", how=\"left\")\n",
    "\n",
    "# Calculate department average\n",
    "dept_avg = df_perf_joined.groupBy(\"Department\").agg(avg(\"Rating\").alias(\"DeptAvg\"))\n",
    "\n",
    "# Join again to get average per row\n",
    "df_filled = df_perf_joined.join(dept_avg, on=\"Department\", how=\"left\")\n",
    "\n",
    "# Replace nulls\n",
    "df_filled = df_filled.withColumn(\n",
    "    \"RatingFilled\",\n",
    "    when(df_filled[\"Rating\"].isNull(), df_filled[\"DeptAvg\"]).otherwise(df_filled[\"Rating\"])\n",
    ")\n",
    "df_filled.select(\"Name\", \"Department\", \"Rating\", \"RatingFilled\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d58e45ff-3ee2-48a2-a4d6-db03fa4a2263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------------------+\n|  Name|Rating|PerformanceCategory|\n+------+------+-------------------+\n|Ananya|   4.5|               Good|\n| Rahul|   4.9|          Excellent|\n| Priya|   4.3|               Good|\n|  Zoya|   3.8|            Average|\n| Karan|   4.1|               Good|\n|Naveen|   4.7|          Excellent|\n|Fatima|   3.9|            Average|\n+------+------+-------------------+\n\n+------+-----------+-----+\n|  Name|HoursWorked|Bonus|\n+------+-----------+-----+\n|Ananya|        120| 5000|\n| Priya|        180| 5000|\n| Rahul|        200| 5000|\n|  Zoya|        100| 5000|\n| Karan|        130| 5000|\n|Naveen|        220|10000|\n|Fatima|         90| 5000|\n+------+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#Built-In Functions and UDF\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "# 7. Create a column PerformanceCategory :\n",
    "# Excellent (>=4.7),\n",
    "# Good (4.0–4.69),\n",
    "# Average (<4.0)\n",
    "df_perf_cat = df_joined.withColumn(\n",
    "    \"PerformanceCategory\",\n",
    "    when(df_joined[\"Rating\"] >= 4.7, \"Excellent\")\n",
    "    .when(df_joined[\"Rating\"] >= 4.0, \"Good\")\n",
    "    .otherwise(\"Average\")\n",
    ")\n",
    "df_perf_cat.select(\"Name\", \"Rating\", \"PerformanceCategory\").show()\n",
    "\n",
    "# 8. Create a UDF to assign bonus:\n",
    "# If project hours > 200 →\n",
    "# 10,000\n",
    "# Else →\n",
    "# 5,000\n",
    "def calc_bonus(hours):\n",
    "    return 10000 if hours > 200 else 5000\n",
    "\n",
    "bonus_udf = udf(calc_bonus, IntegerType())\n",
    "\n",
    "df_bonus = df_perf_cat.withColumn(\"Bonus\", bonus_udf(\"HoursWorked\"))\n",
    "df_bonus.select(\"Name\", \"HoursWorked\", \"Bonus\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7453462d-de77-4df2-84b7-de4a8d0531a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------------+\n|  Name|  JoinDate|MonthsWorked|\n+------+----------+------------+\n|Ananya|2021-06-01| 48.32258065|\n| Priya|2021-06-01| 48.32258065|\n| Rahul|2021-06-01| 48.32258065|\n|  Zoya|2021-06-01| 48.32258065|\n| Karan|2021-06-01| 48.32258065|\n|Naveen|2021-06-01| 48.32258065|\n|Fatima|2021-06-01| 48.32258065|\n+------+----------+------------+\n\nEmployees joined before 2022: 7\n"
     ]
    }
   ],
   "source": [
    "#Date and Time Functions\n",
    "from pyspark.sql.functions import lit, months_between, current_date, to_date\n",
    "from pyspark.sql.functions import to_date\n",
    "# 9. Add a column JoinDate with 2021-06-01 for all, then add MonthsWorked as difference from today.\n",
    "df_with_dates = df_bonus.withColumn(\"JoinDate\", to_date(lit(\"2021-06-01\")))\n",
    "df_with_dates = df_with_dates.withColumn(\"MonthsWorked\", months_between(current_date(), \"JoinDate\"))\n",
    "df_with_dates.select(\"Name\", \"JoinDate\", \"MonthsWorked\").show()\n",
    "\n",
    "# 10. Calculate how many employees joined before 2022.\n",
    "count_before_2022 = df_with_dates.filter(df_with_dates[\"JoinDate\"] < to_date(lit(\"2022-01-01\"))).count()\n",
    "print(\"Employees joined before 2022:\", count_before_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ccc6749-d44a-4838-affe-f212d21c5577",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+\n|  Name| Department|Salary|\n+------+-----------+------+\n|Ananya|         HR| 52000|\n| Rahul|Engineering| 65000|\n| Priya|Engineering| 60000|\n|  Zoya|  Marketing| 48000|\n| Karan|         HR| 53000|\n|Naveen|Engineering| 70000|\n|Fatima|  Marketing| 45000|\n| Meena|         HR| 48000|\n|   Raj|  Marketing| 51000|\n+------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#Unions\n",
    "#11. Create another small team DataFrame and union() it with employee_data .\n",
    "# extra_employees = [\n",
    "# (\"Meena\", \"HR\", 48000),\n",
    "# (\"Raj\", \"Marketing\", 51000)\n",
    "# ]\n",
    "extra_employees = [\n",
    "    (\"Meena\", \"HR\", 48000),\n",
    "    (\"Raj\", \"Marketing\", 51000)\n",
    "]\n",
    "df_extra = spark.createDataFrame(extra_employees, columns_emp)\n",
    "\n",
    "df_emp_extended = df_emp.union(df_extra)\n",
    "df_emp_extended.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74240ef9-4f57-4a49-8d6a-eb55e95a530d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Saving Results\n",
    "#12. Save the final merged dataset (all 3 joins) as a partitioned Parquet file based on Department .\n",
    "df_with_dates.write.mode(\"overwrite\").partitionBy(\"Department\").parquet(\"dbfs:/tmp/final_employee_data_partitioned\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CombiningData2025-06-11 14:32:50",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}