{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "187ff906-2bb1-409d-8fcd-9ceda776ef3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.session.SparkSession at 0xfff9399cc2d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"ECom Transactions\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdc2ab5c-71ac-4bfb-bd69-28bb0cde4ca8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#E-Commerce Transactions + Returns + Inventory\n",
    "# PySpark + Delta\n",
    "# 1. Ingest all 3 CSVs as Delta Tables.\n",
    "#CSV files\n",
    "df_orders = spark.read.csv(\"/Volumes/workspace/ecommerce/csv_data/orders.csv\", header=True, inferSchema=True)\n",
    "df_customers = spark.read.csv(\"/Volumes/workspace/ecommerce/csv_data/customers.csv\", header=True, inferSchema=True)\n",
    "df_products = spark.read.csv(\"/Volumes/workspace/ecommerce/csv_data/products.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Save as Delta Tables in a database called 'ecommerce'\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS ecommerce\")\n",
    "\n",
    "df_orders.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"ecommerce.orders\")\n",
    "df_customers.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"ecommerce.customers\")\n",
    "df_products.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"ecommerce.products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b91fcacd-2a31-4b82-9bf7-42c52f596fc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_customers.show()\n",
    "df_orders.show()\n",
    "df_products.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33952ebb-86f4-41ed-a26e-38165305e207",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Write SQL to get the total revenue per Product.\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ProductID, \n",
    "        SUM(Quantity * Price) AS TotalRevenue\n",
    "    FROM ecommerce.orders\n",
    "    WHERE Status = 'Delivered'\n",
    "    GROUP BY ProductID\n",
    "    ORDER BY TotalRevenue DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# 3. Join Orders + Customers to find revenue by Region.\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.Region, \n",
    "        SUM(o.Quantity * o.Price) AS RegionalRevenue\n",
    "    FROM ecommerce.orders o\n",
    "    JOIN ecommerce.customers c \n",
    "        ON o.CustomerID = c.CustomerID\n",
    "    WHERE o.Status = 'Delivered'\n",
    "    GROUP BY c.Region\n",
    "\"\"\").show()\n",
    "\n",
    "# 4. Update the Status of Pending orders to 'Cancelled'.\n",
    "spark.sql(\"\"\"\n",
    "  UPDATE ecommerce.orders\n",
    "  SET Status = 'Cancelled'\n",
    "  WHERE Status = 'Pending'\n",
    "\"\"\").show()\n",
    "\n",
    "# 5. Merge a new return record into Orders.\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Simulated return record\n",
    "new_return = spark.createDataFrame([\n",
    "    Row(OrderID=3006, CustomerID='C003', ProductID='P1003', Quantity=1, Price=30000, OrderDate='2024-05-06', Status='Returned')\n",
    "])\n",
    "\n",
    "new_return.createOrReplaceTempView(\"new_return\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO ecommerce.orders AS target\n",
    "USING new_return AS source\n",
    "ON target.OrderID = source.OrderID\n",
    "WHEN MATCHED THEN UPDATE SET *\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95b71f50-4681-461e-901f-ebd68fed9dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DLT Pipeline\n",
    "# 6. Create raw → cleaned → aggregated tables:\n",
    "# Clean: Remove rows with NULLs\n",
    "# Aggregated: Total revenue per Category\n",
    "df_cleaned = spark.sql(\"SELECT * FROM ecommerce.orders\").dropna()\n",
    "df_cleaned.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"ecommerce.orders_cleaned\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        p.Category, \n",
    "        SUM(o.Quantity * o.Price) AS TotalRevenue\n",
    "    FROM ecommerce.orders_cleaned o\n",
    "    JOIN ecommerce.products p\n",
    "        ON o.ProductID = p.ProductID\n",
    "    WHERE o.Status = 'Delivered'\n",
    "    GROUP BY p.Category\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b5f1ee2-022c-424e-a33d-0f4731ca1f65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Time Travel\n",
    "# 7. View data before the Status update.\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM ecommerce.orders VERSION AS OF 0\n",
    "\"\"\").show()\n",
    "\n",
    "# 8. Restore to an older version of the orders table. \n",
    "df_old = spark.read.format(\"delta\").option(\"versionAsOf\", 0).table(\"ecommerce.orders\")\n",
    "df_old.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"ecommerce.orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11ff737a-2c52-47ff-95fb-81ba6187e05c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Vacuum + Retention\n",
    "# 9. Run VACUUM after changing default retention.\n",
    "# Change retention period to 0 hours\n",
    "# Disable retention check for immediate VACUUM\n",
    "spark.conf.set(\"spark.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "spark.sql(\"VACUUM ecommerce.orders RETAIN 0 HOURS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f847df90-57ed-4bad-ac49-dabf82cb23a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Expectations\n",
    "# 10. Quantity > 0 , Price > 0 , OrderDate is not null\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM ecommerce.orders\n",
    "    WHERE Quantity > 0 AND Price > 0 AND OrderDate IS NOT NULL\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebc4ed12-b5f0-4b37-85fe-849a82566c94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Bonus\n",
    "# 11. Use when-otherwise to create a new column: OrderType = \"Return\" if Status ==\n",
    "# 'Returned'\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *, \n",
    "        CASE \n",
    "            WHEN Status = 'Returned' THEN 'Return'\n",
    "            ELSE 'Sale'\n",
    "        END AS OrderType\n",
    "    FROM ecommerce.orders\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "E-Commerce Transactions, Returns&Inventory-06-18 20:13:39",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}