{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b01b591a-300e-41b6-8961-1bba475ae0c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RetailTransactions\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40a45239-d8c2-43d8-b4a8-cc9f9279dadb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+\n|TransactionID|Customer|     City|Product|   Category|Quantity|UnitPrice|TotalPrice|TransactionDate|PaymentMode|\n+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+\n|        T1001|     Ali|   Mumbai| Laptop|Electronics|       1|    70000|     70000|     2024-01-15|       Card|\n|        T1002|    Neha|Bangalore| Tablet|Electronics|       2|    30000|     60000|     2024-01-20|        UPI|\n|        T1003|    Ravi|Hyderabad|   Desk|  Furniture|       1|    15000|     15000|     2024-02-10|Net Banking|\n|        T1004|    Zoya|    Delhi|  Chair|  Furniture|       4|     5000|     20000|     2024-02-12|       Card|\n|        T1005|   Karan|   Mumbai|  Phone|Electronics|       1|    50000|     50000|     2024-02-15|       Card|\n|        T1006|   Farah|    Delhi|  Mouse|Electronics|       3|     1000|      3000|     2024-02-18|       Cash|\n+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+\n\nroot\n |-- TransactionID: string (nullable = true)\n |-- Customer: string (nullable = true)\n |-- City: string (nullable = true)\n |-- Product: string (nullable = true)\n |-- Category: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- UnitPrice: integer (nullable = true)\n |-- TotalPrice: integer (nullable = true)\n |-- TransactionDate: date (nullable = true)\n |-- PaymentMode: string (nullable = true)\n\nroot\n |-- TransactionID: string (nullable = true)\n |-- Customer: string (nullable = true)\n |-- City: string (nullable = true)\n |-- Product: string (nullable = true)\n |-- Category: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- UnitPrice: integer (nullable = true)\n |-- TotalPrice: integer (nullable = true)\n |-- TransactionDate: string (nullable = true)\n |-- PaymentMode: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Basics\n",
    "# 1. Load retail_data.csv into a PySpark DataFrame and display schema.\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/Volumes/workspace/ecommerce/csv_data/retail_data.csv\")\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "# 2. Infer schema as False â€” then manually cast columns.\n",
    "df_raw = spark.read.option(\"header\", True).option(\"inferSchema\", False).csv(\"/Volumes/workspace/ecommerce/csv_data/retail_data.csv\")\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "df_casted = df_raw.select(\n",
    "    col(\"TransactionID\"),\n",
    "    col(\"Customer\"),\n",
    "    col(\"City\"),\n",
    "    col(\"Product\"),\n",
    "    col(\"Category\"),\n",
    "    col(\"Quantity\").cast(\"int\"),\n",
    "    col(\"UnitPrice\").cast(\"int\"),\n",
    "    col(\"TotalPrice\").cast(\"int\"),\n",
    "    col(\"TransactionDate\"),\n",
    "    col(\"PaymentMode\")\n",
    ")\n",
    "df_casted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2af93a78-7c80-4828-900f-fc9c6329847f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+\n|TransactionID|Customer|     City|Product|   Category|Quantity|UnitPrice|TotalPrice|TransactionDate|PaymentMode|\n+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+\n|        T1001|     Ali|   Mumbai| Laptop|Electronics|       1|    70000|     70000|     2024-01-15|       Card|\n|        T1002|    Neha|Bangalore| Tablet|Electronics|       2|    30000|     60000|     2024-01-20|        UPI|\n|        T1005|   Karan|   Mumbai|  Phone|Electronics|       1|    50000|     50000|     2024-02-15|       Card|\n+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+\n\n+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+\n|TransactionID|Customer|     City|Product|   Category|Quantity|UnitPrice|TotalPrice|TransactionDate|PaymentMode|\n+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+\n|        T1001|     Ali|   Mumbai| Laptop|Electronics|       1|    70000|     70000|     2024-01-15|       Card|\n|        T1002|    Neha|Bangalore| Tablet|Electronics|       2|    30000|     60000|     2024-01-20|        UPI|\n|        T1003|    Ravi|Hyderabad|   Desk|  Furniture|       1|    15000|     15000|     2024-02-10|Net Banking|\n|        T1004|    Zoya|    Delhi|  Chair|  Furniture|       4|     5000|     20000|     2024-02-12|       Card|\n|        T1005|   Karan|   Mumbai|  Phone|Electronics|       1|    50000|     50000|     2024-02-15|       Card|\n|        T1006|   Farah|    Delhi|  Mouse|Electronics|       3|     1000|      3000|     2024-02-18|       Cash|\n+-------------+--------+---------+-------+-----------+--------+---------+----------+---------------+-----------+\n\n+---------+\n|     City|\n+---------+\n|Bangalore|\n|    Delhi|\n|   Mumbai|\n|Hyderabad|\n+---------+\n\n+-------------+--------+-----+-------+-----------+--------+---------+----------+---------------+-----------+\n|TransactionID|Customer| City|Product|   Category|Quantity|UnitPrice|TotalPrice|TransactionDate|PaymentMode|\n+-------------+--------+-----+-------+-----------+--------+---------+----------+---------------+-----------+\n|        T1004|    Zoya|Delhi|  Chair|  Furniture|       4|     5000|     20000|     2024-02-12|       Card|\n|        T1006|   Farah|Delhi|  Mouse|Electronics|       3|     1000|      3000|     2024-02-18|       Cash|\n+-------------+--------+-----+-------+-----------+--------+---------+----------+---------------+-----------+\n\n+-------------+--------+-----+-------+-----------+--------+---------+----------+---------------+-----------+\n|TransactionID|Customer| City|Product|   Category|Quantity|UnitPrice|TotalPrice|TransactionDate|PaymentMode|\n+-------------+--------+-----+-------+-----------+--------+---------+----------+---------------+-----------+\n|        T1004|    Zoya|Delhi|  Chair|  Furniture|       4|     5000|     20000|     2024-02-12|       Card|\n|        T1006|   Farah|Delhi|  Mouse|Electronics|       3|     1000|      3000|     2024-02-18|       Cash|\n+-------------+--------+-----+-------+-----------+--------+---------+----------+---------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Data Exploration & Filtering\n",
    "# 3. Filter transactions where TotalPrice > 40000 .\n",
    "df.filter(df.TotalPrice > 40000).show()\n",
    "df.show()\n",
    "\n",
    "# 4. Get unique cities from the dataset.\n",
    "df.select(\"City\").distinct().show()\n",
    "\n",
    "# 5. Find all transactions from \"Delhi\" using .filter() and .where() .\n",
    "df.filter(df.City == \"Delhi\").show()\n",
    "df.where(df.City == \"Delhi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f03ea494-5ebe-419e-b089-14ae1f40be3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+---------------+\n|TransactionID|TotalPrice|DiscountedPrice|\n+-------------+----------+---------------+\n|        T1001|     70000|        63000.0|\n|        T1002|     60000|        54000.0|\n|        T1003|     15000|        13500.0|\n|        T1004|     20000|        18000.0|\n|        T1005|     50000|        45000.0|\n|        T1006|      3000|         2700.0|\n+-------------+----------+---------------+\n\n+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+\n|TransactionID|Customer|     City|Product|   Category|Quantity|TotalPrice|   TxnDate|PaymentMode|DiscountedPrice|\n+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+\n|        T1001|     Ali|   Mumbai| Laptop|Electronics|       1|     70000|2024-01-15|       Card|        63000.0|\n|        T1002|    Neha|Bangalore| Tablet|Electronics|       2|     60000|2024-01-20|        UPI|        54000.0|\n|        T1003|    Ravi|Hyderabad|   Desk|  Furniture|       1|     15000|2024-02-10|Net Banking|        13500.0|\n|        T1004|    Zoya|    Delhi|  Chair|  Furniture|       4|     20000|2024-02-12|       Card|        18000.0|\n|        T1005|   Karan|   Mumbai|  Phone|Electronics|       1|     50000|2024-02-15|       Card|        45000.0|\n|        T1006|   Farah|    Delhi|  Mouse|Electronics|       3|      3000|2024-02-18|       Cash|         2700.0|\n+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+\n\n+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+\n|TransactionID|Customer|     City|Product|   Category|Quantity|TotalPrice|   TxnDate|PaymentMode|DiscountedPrice|\n+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+\n|        T1001|     Ali|   Mumbai| Laptop|Electronics|       1|     70000|2024-01-15|       Card|        63000.0|\n|        T1002|    Neha|Bangalore| Tablet|Electronics|       2|     60000|2024-01-20|        UPI|        54000.0|\n|        T1003|    Ravi|Hyderabad|   Desk|  Furniture|       1|     15000|2024-02-10|Net Banking|        13500.0|\n|        T1004|    Zoya|    Delhi|  Chair|  Furniture|       4|     20000|2024-02-12|       Card|        18000.0|\n|        T1005|   Karan|   Mumbai|  Phone|Electronics|       1|     50000|2024-02-15|       Card|        45000.0|\n|        T1006|   Farah|    Delhi|  Mouse|Electronics|       3|      3000|2024-02-18|       Cash|         2700.0|\n+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Data Manipulation\n",
    "# 6. Add a column DiscountedPrice = TotalPrice - 10%.\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(\"DiscountedPrice\", col(\"TotalPrice\") * 0.9)\n",
    "df.select(\"TransactionID\", \"TotalPrice\", \"DiscountedPrice\").show()\n",
    "\n",
    "# 7. Rename TransactionDate to TxnDate .\n",
    "df = df.withColumnRenamed(\"TransactionDate\", \"TxnDate\")\n",
    "df.show()\n",
    "\n",
    "# 8. Drop the column UnitPrice .\n",
    "df = df.drop(\"UnitPrice\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "288d2174-4e6c-436d-ba76-f690335ff876",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n|     City|TotalSales|\n+---------+----------+\n|Bangalore|     60000|\n|    Delhi|     23000|\n|   Mumbai|    120000|\n|Hyderabad|     15000|\n+---------+----------+\n\n+-----------+------------+\n|   Category|AvgUnitPrice|\n+-----------+------------+\n|  Furniture|     10000.0|\n|Electronics|     37750.0|\n+-----------+------------+\n\n+-----------+-----+\n|PaymentMode|count|\n+-----------+-----+\n|Net Banking|    1|\n|       Cash|    1|\n|       Card|    3|\n|        UPI|    1|\n+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Aggregations\n",
    "# 9. Get total sales by city.\n",
    "df.groupBy(\"City\").sum(\"TotalPrice\").withColumnRenamed(\"sum(TotalPrice)\", \"TotalSales\").show()\n",
    "\n",
    "# 10. Get average unit price by category.\n",
    "df_casted.groupBy(\"Category\").avg(\"UnitPrice\").withColumnRenamed(\"avg(UnitPrice)\", \"AvgUnitPrice\").show()\n",
    "\n",
    "# 11. Count of transactions grouped by PaymentMode.\n",
    "df.groupBy(\"PaymentMode\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c75b7d3-34d4-4b66-9334-1ec83741ce82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+----------+----+\n|TransactionID|     City|TotalPrice|Rank|\n+-------------+---------+----------+----+\n|        T1002|Bangalore|     60000|   1|\n|        T1004|    Delhi|     20000|   1|\n|        T1006|    Delhi|      3000|   2|\n|        T1003|Hyderabad|     15000|   1|\n|        T1001|   Mumbai|     70000|   1|\n|        T1005|   Mumbai|     50000|   2|\n+-------------+---------+----------+----+\n\n+-------------+---------+----------+----------+\n|TransactionID|     City|TotalPrice|PrevAmount|\n+-------------+---------+----------+----------+\n|        T1002|Bangalore|     60000|      NULL|\n|        T1004|    Delhi|     20000|      NULL|\n|        T1006|    Delhi|      3000|     20000|\n|        T1003|Hyderabad|     15000|      NULL|\n|        T1001|   Mumbai|     70000|      NULL|\n|        T1005|   Mumbai|     50000|     70000|\n+-------------+---------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Window Functions\n",
    "# 12. Use a window partitioned by City to rank transactions by TotalPrice .\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "windowSpec = Window.partitionBy(\"City\").orderBy(df[\"TotalPrice\"].desc())\n",
    "df.withColumn(\"Rank\", rank().over(windowSpec)).select(\"TransactionID\", \"City\", \"TotalPrice\", \"Rank\").show()\n",
    "\n",
    "# 13. Use lag function to get previous transaction amount per city.\n",
    "from pyspark.sql.functions import lag\n",
    "\n",
    "df.withColumn(\"PrevAmount\", lag(\"TotalPrice\").over(windowSpec)).select(\"TransactionID\", \"City\", \"TotalPrice\", \"PrevAmount\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feff1318-9bca-4024-acdc-e236772f3d7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n|     City|Region|\n+---------+------+\n|   Mumbai|  West|\n|    Delhi| North|\n|Bangalore| South|\n|Hyderabad| South|\n+---------+------+\n\n+------+----------+\n|Region|TotalSales|\n+------+----------+\n|  West|    120000|\n| North|     23000|\n| South|     75000|\n+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Joins\n",
    "# 14. Create a second DataFrame city_region :\n",
    "# City,Region\n",
    "# Mumbai,West\n",
    "# Delhi,North\n",
    "# Bangalore,South\n",
    "# Hyderabad,South\n",
    "\n",
    "data_region = [(\"Mumbai\", \"West\"), (\"Delhi\", \"North\"), (\"Bangalore\", \"South\"), (\"Hyderabad\", \"South\")]\n",
    "columns_region = [\"City\", \"Region\"]\n",
    "\n",
    "df_region = spark.createDataFrame(data_region, columns_region)\n",
    "df_region.show()\n",
    "\n",
    "# 15. Join with main DataFrame and group total sales by Region.\n",
    "df_joined = df.join(df_region, on=\"City\", how=\"left\")\n",
    "df_joined.groupBy(\"Region\").sum(\"TotalPrice\").withColumnRenamed(\"sum(TotalPrice)\", \"TotalSales\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2601c7b-25ab-4d72-9c73-415dd3a8316d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+\n|TransactionID|Customer|     City|Product|   Category|Quantity|TotalPrice|   TxnDate|PaymentMode|DiscountedPrice|\n+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+\n|        T1001|     Ali|   Mumbai| Laptop|Electronics|       1|     70000|2024-01-15|       Card|        63000.0|\n|        T1002|    Neha|Bangalore| Tablet|Electronics|       1|     60000|2024-01-20|        UPI|        54000.0|\n|        T1003|    Ravi|Hyderabad|   Desk|  Furniture|       1|     15000|2024-02-10|Net Banking|        13500.0|\n|        T1004|    Zoya|    Delhi|  Chair|  Furniture|       1|     20000|2024-02-12|       Card|        18000.0|\n|        T1005|   Karan|   Mumbai|  Phone|Electronics|       1|     50000|2024-02-15|       Card|        45000.0|\n|        T1006|   Farah|    Delhi|  Mouse|Electronics|       1|      3000|2024-02-18|       Cash|         2700.0|\n+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+\n\n+-------------+--------+----+-------+--------+--------+----------+-------+-----------+---------------+\n|TransactionID|Customer|City|Product|Category|Quantity|TotalPrice|TxnDate|PaymentMode|DiscountedPrice|\n+-------------+--------+----+-------+--------+--------+----------+-------+-----------+---------------+\n+-------------+--------+----+-------+--------+--------+----------+-------+-----------+---------------+\n\n+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+\n|TransactionID|Customer|     City|Product|   Category|Quantity|TotalPrice|   TxnDate|PaymentMode|DiscountedPrice|\n+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+\n|        T1001|     Ali|   Mumbai| Laptop|Electronics|       1|     70000|2024-01-15|       Card|        63000.0|\n|        T1002|    Neha|Bangalore| Tablet|Electronics|       2|     60000|2024-01-20|        UPI|        54000.0|\n|        T1003|    Ravi|Hyderabad|   Desk|  Furniture|       1|     15000|2024-02-10|Net Banking|        13500.0|\n|        T1004|    Zoya|    Delhi|  Chair|  Furniture|       4|     20000|2024-02-12|       Card|        18000.0|\n|        T1005|   Karan|   Mumbai|  Phone|Electronics|       1|     50000|2024-02-15|       Card|        45000.0|\n|        T1006|   Farah|    Delhi|  Mouse|Electronics|       3|      3000|2024-02-18|       Cash|         2700.0|\n+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Nulls and Data Cleaning\n",
    "# 16. Introduce some nulls and replace them with default values.\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df_null = df.withColumn(\"Quantity\", lit(None).cast(\"int\"))\n",
    "df_filled = df_null.fillna({\"Quantity\": 1})\n",
    "df_filled.show()\n",
    "\n",
    "# 17. Drop rows where Quantity is null.\n",
    "df_null.dropna(subset=[\"Quantity\"]).show()\n",
    "\n",
    "# 18. Fill null PaymentMode with \"Unknown\".\n",
    "df.fillna({\"PaymentMode\": \"Unknown\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23dfd080-eac7-4a1c-83c6-92670daaf14d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+----------+\n|TransactionID|TotalPrice|OrderLabel|\n+-------------+----------+----------+\n|        T1001|     70000|      High|\n|        T1002|     60000|      High|\n|        T1003|     15000|       Low|\n|        T1004|     20000|       Low|\n|        T1005|     50000|    Medium|\n|        T1006|      3000|       Low|\n+-------------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Custom Functions\n",
    "# 19. Write a UDF to label orders:\n",
    "# def label_order(amount):\n",
    "# if amount > 50000: return \"High\"\n",
    "# elif amount >= 30000: return \"Medium\"\n",
    "# else: return \"Low\"\n",
    "# Apply this to classify TotalPrice .\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def label_order(amount):\n",
    "    if amount > 50000:\n",
    "        return \"High\"\n",
    "    elif amount >= 30000:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Low\"\n",
    "\n",
    "label_udf = udf(label_order, StringType())\n",
    "df = df.withColumn(\"OrderLabel\", label_udf(col(\"TotalPrice\")))\n",
    "df.select(\"TransactionID\", \"TotalPrice\", \"OrderLabel\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7507cb5-1756-4493-92ef-3e582875b87e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+\n|   TxnDate|Year|Month|Day|\n+----------+----+-----+---+\n|2024-01-15|2024|    1| 15|\n|2024-01-20|2024|    1| 20|\n|2024-02-10|2024|    2| 10|\n|2024-02-12|2024|    2| 12|\n|2024-02-15|2024|    2| 15|\n|2024-02-18|2024|    2| 18|\n+----------+----+-----+---+\n\n+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+----------+----+-----+---+\n|TransactionID|Customer|     City|Product|   Category|Quantity|TotalPrice|   TxnDate|PaymentMode|DiscountedPrice|OrderLabel|Year|Month|Day|\n+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+----------+----+-----+---+\n|        T1003|    Ravi|Hyderabad|   Desk|  Furniture|       1|     15000|2024-02-10|Net Banking|        13500.0|       Low|2024|    2| 10|\n|        T1004|    Zoya|    Delhi|  Chair|  Furniture|       4|     20000|2024-02-12|       Card|        18000.0|       Low|2024|    2| 12|\n|        T1005|   Karan|   Mumbai|  Phone|Electronics|       1|     50000|2024-02-15|       Card|        45000.0|    Medium|2024|    2| 15|\n|        T1006|   Farah|    Delhi|  Mouse|Electronics|       3|      3000|2024-02-18|       Cash|         2700.0|       Low|2024|    2| 18|\n+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+----------+----+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Date & Time\n",
    "# 20. Extract year, month, and day from TxnDate .\n",
    "from pyspark.sql.functions import to_date, year, month, dayofmonth\n",
    "\n",
    "df = df.withColumn(\"TxnDate\", to_date(\"TxnDate\", \"yyyy-MM-dd\"))\n",
    "df = df.withColumn(\"Year\", year(\"TxnDate\")).withColumn(\"Month\", month(\"TxnDate\")).withColumn(\"Day\", dayofmonth(\"TxnDate\"))\n",
    "df.select(\"TxnDate\", \"Year\", \"Month\", \"Day\").show()\n",
    "\n",
    "# 21. Filter transactions that happened in February.\n",
    "df.filter(month(\"TxnDate\") == 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95596973-5e26-41c8-a222-02c9b625dbc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+----------+----+-----+---+\n|TransactionID|Customer|     City|Product|   Category|Quantity|TotalPrice|   TxnDate|PaymentMode|DiscountedPrice|OrderLabel|Year|Month|Day|\n+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+----------+----+-----+---+\n|        T1006|   Farah|    Delhi|  Mouse|Electronics|       3|      3000|2024-02-18|       Cash|         2700.0|       Low|2024|    2| 18|\n|        T1004|    Zoya|    Delhi|  Chair|  Furniture|       4|     20000|2024-02-12|       Card|        18000.0|       Low|2024|    2| 12|\n|        T1002|    Neha|Bangalore| Tablet|Electronics|       2|     60000|2024-01-20|        UPI|        54000.0|      High|2024|    1| 20|\n|        T1003|    Ravi|Hyderabad|   Desk|  Furniture|       1|     15000|2024-02-10|Net Banking|        13500.0|       Low|2024|    2| 10|\n|        T1001|     Ali|   Mumbai| Laptop|Electronics|       1|     70000|2024-01-15|       Card|        63000.0|      High|2024|    1| 15|\n|        T1005|   Karan|   Mumbai|  Phone|Electronics|       1|     50000|2024-02-15|       Card|        45000.0|    Medium|2024|    2| 15|\n+-------------+--------+---------+-------+-----------+--------+----------+----------+-----------+---------------+----------+----+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Union & Duplicate Handling\n",
    "# 22. Duplicate the DataFrame using union() and remove duplicates.\n",
    "df_duped = df.union(df)\n",
    "df_unique = df_duped.dropDuplicates()\n",
    "df_unique.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Retail Transactions 2025-06-18 22:17:07",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}