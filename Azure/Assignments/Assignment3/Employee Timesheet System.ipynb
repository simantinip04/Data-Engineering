{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bf90f3c-3243-41c8-8159-f35a2538788d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"EmployeeTimesheet\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26606e7f-b5ae-4b29-acf8-fc2863e5dc86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate|Location |  Mode|\n+----------+-----+----------+-------+---------+----------+---------+------+\n|      E101|Anita|        IT|  Alpha|        8|01-05-2024|Bangalore|Remote|\n|      E102|  Raj|        HR|   Beta|        7|01-05-2024|   Mumbai|Onsite|\n|      E103| John|   Finance|  Alpha|        5|02-05-2024|    Delhi|Remote|\n|      E101|Anita|        IT|  Alpha|        9|03-05-2024|Bangalore|Remote|\n|      E104|Meena|        IT|  Gamma|        6|03-05-2024|Hyderabad|Onsite|\n|      E102|  Raj|        HR|   Beta|        8|04-05-2024|   Mumbai|Remote|\n+----------+-----+----------+-------+---------+----------+---------+------+\n\n+----------+-----+----------+-------+---------+--------+---------+------+\n|EmployeeID| Name|Department|Project|WorkHours|WorkDate| Location|  Mode|\n+----------+-----+----------+-------+---------+--------+---------+------+\n|      E101|Anita|        IT|  Alpha|        8|    NULL|Bangalore|Remote|\n|      E102|  Raj|        HR|   Beta|        7|    NULL|   Mumbai|Onsite|\n|      E103| John|   Finance|  Alpha|        5|    NULL|    Delhi|Remote|\n|      E101|Anita|        IT|  Alpha|        9|    NULL|Bangalore|Remote|\n|      E104|Meena|        IT|  Gamma|        6|    NULL|Hyderabad|Onsite|\n|      E102|  Raj|        HR|   Beta|        8|    NULL|   Mumbai|Remote|\n+----------+-----+----------+-------+---------+--------+---------+------+\n\n+----------+-----+----------+-------+---------+--------+---------+------+-------+\n|EmployeeID| Name|Department|Project|WorkHours|WorkDate| Location|  Mode|Weekday|\n+----------+-----+----------+-------+---------+--------+---------+------+-------+\n|      E101|Anita|        IT|  Alpha|        8|    NULL|Bangalore|Remote|   NULL|\n|      E102|  Raj|        HR|   Beta|        7|    NULL|   Mumbai|Onsite|   NULL|\n|      E103| John|   Finance|  Alpha|        5|    NULL|    Delhi|Remote|   NULL|\n|      E101|Anita|        IT|  Alpha|        9|    NULL|Bangalore|Remote|   NULL|\n|      E104|Meena|        IT|  Gamma|        6|    NULL|Hyderabad|Onsite|   NULL|\n|      E102|  Raj|        HR|   Beta|        8|    NULL|   Mumbai|Remote|   NULL|\n+----------+-----+----------+-------+---------+--------+---------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Data Ingestion & Schema Handling\n",
    "# 1. Load the CSV using inferred schema.\n",
    "df_inferred = spark.read.option(\"header\", True).csv(\"/Volumes/workspace/ecommerce/csv_data/employee_timesheet.csv\", inferSchema=True)\n",
    "df_inferred.show()\n",
    "\n",
    "# 2. Load the same file with schema explicitly defined.\n",
    "schema = StructType([\n",
    "    StructField(\"EmployeeID\", StringType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "    StructField(\"Project\", StringType(), True),\n",
    "    StructField(\"WorkHours\", IntegerType(), True),\n",
    "    StructField(\"WorkDate\", DateType(), True),\n",
    "    StructField(\"Location\", StringType(), True),\n",
    "    StructField(\"Mode\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.option(\"header\", True).schema(schema).csv(\"/Volumes/workspace/ecommerce/csv_data/employee_timesheet.csv\")\n",
    "df.show()\n",
    "\n",
    "# 3. Add a new column Weekday extracted from WorkDate .\n",
    "df = df.withColumn(\"Weekday\", date_format(\"WorkDate\", \"EEEE\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e75c693-70fe-4110-8a0f-c050d16ee1db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+\n|EmployeeID| Name|TotalHours|\n+----------+-----+----------+\n|      E103| John|         5|\n|      E104|Meena|         6|\n|      E101|Anita|        17|\n|      E102|  Raj|        15|\n+----------+-----+----------+\n\n+----------+-----------------+\n|Department|         AvgHours|\n+----------+-----------------+\n|        HR|              7.5|\n|        IT|7.666666666666667|\n|   Finance|              5.0|\n+----------+-----------------+\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/expressions.py:1017: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+----+\n|EmployeeID| Name|TotalHours|Rank|\n+----------+-----+----------+----+\n|      E101|Anita|        17|   1|\n|      E102|  Raj|        15|   2|\n+----------+-----+----------+----+\n\n"
     ]
    }
   ],
   "source": [
    "# Aggregations & Grouping\n",
    "# 4. Calculate total work hours by employee.\n",
    "df.groupBy(\"EmployeeID\", \"Name\").agg(sum(\"WorkHours\").alias(\"TotalHours\")).show()\n",
    "\n",
    "# 5. Calculate average work hours per department.\n",
    "df.groupBy(\"Department\").agg(avg(\"WorkHours\").alias(\"AvgHours\")).show()\n",
    "\n",
    "# 6. Get top 2 employees by total hours using window function.\n",
    "window_spec = Window.orderBy(desc(\"TotalHours\"))\n",
    "df_total = df.groupBy(\"EmployeeID\", \"Name\").agg(sum(\"WorkHours\").alias(\"TotalHours\"))\n",
    "df_total.withColumn(\"Rank\", rank().over(window_spec)).filter(col(\"Rank\") <= 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e985f92-4a71-4c11-8c4b-1eadac69748a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- EmployeeID: string (nullable = true)\n |-- Name: string (nullable = true)\n |-- Department: string (nullable = true)\n |-- Project: string (nullable = true)\n |-- WorkHours: integer (nullable = true)\n |-- WorkDate: date (nullable = true)\n |-- Location: string (nullable = true)\n |-- Mode: string (nullable = true)\n |-- Weekday: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b919449a-bf64-4360-ae02-e6e2bf75a1a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+-------+---------+--------+--------+----+-------+\n|EmployeeID|Name|Department|Project|WorkHours|WorkDate|Location|Mode|Weekday|\n+----------+----+----------+-------+---------+--------+--------+----+-------+\n+----------+----+----------+-------+---------+--------+--------+----+-------+\n\n+----------+-----+----------+-------+---------+--------+---------+------+-------+-----------------+\n|EmployeeID| Name|Department|Project|WorkHours|WorkDate| Location|  Mode|Weekday|RunningTotalHours|\n+----------+-----+----------+-------+---------+--------+---------+------+-------+-----------------+\n|      E101|Anita|        IT|  Alpha|        8|    NULL|Bangalore|Remote|   NULL|                8|\n|      E101|Anita|        IT|  Alpha|        9|    NULL|Bangalore|Remote|   NULL|               17|\n|      E102|  Raj|        HR|   Beta|        7|    NULL|   Mumbai|Onsite|   NULL|                7|\n|      E102|  Raj|        HR|   Beta|        8|    NULL|   Mumbai|Remote|   NULL|               15|\n|      E103| John|   Finance|  Alpha|        5|    NULL|    Delhi|Remote|   NULL|                5|\n|      E104|Meena|        IT|  Gamma|        6|    NULL|Hyderabad|Onsite|   NULL|                6|\n+----------+-----+----------+-------+---------+--------+---------+------+-------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Date Operations\n",
    "# 7. Filter entries where WorkDate falls on a weekend.\n",
    "# 1 = Sunday, 7 = Saturday\n",
    "from pyspark.sql.functions import dayofweek\n",
    "\n",
    "df.filter(dayofweek(\"WorkDate\").isin([1, 7])).show()\n",
    "\n",
    "# 8. Calculate running total of hours per employee using window function.\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "window_emp = Window.partitionBy(\"EmployeeID\").orderBy(\"WorkDate\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "df = df.withColumn(\"RunningTotalHours\", sum(\"WorkHours\").over(window_emp))\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3914672c-4caf-4322-b242-09a99be9d2d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n|Department|DeptHead|\n+----------+--------+\n|        IT|   Anand|\n|        HR|  Shruti|\n|   Finance|   Kamal|\n+----------+--------+\n\n+----------+-----+----------+--------+\n|EmployeeID| Name|Department|DeptHead|\n+----------+-----+----------+--------+\n|      E101|Anita|        IT|   Anand|\n|      E102|  Raj|        HR|  Shruti|\n|      E103| John|   Finance|   Kamal|\n|      E101|Anita|        IT|   Anand|\n|      E104|Meena|        IT|   Anand|\n|      E102|  Raj|        HR|  Shruti|\n+----------+-----+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Joining DataFrames\n",
    "# 9. Create department_location.csv :\n",
    "# Department,DeptHead\n",
    "# IT,Anand\n",
    "# HR,Shruti\n",
    "# Finance,Kamal\n",
    "df_dept = spark.read.option(\"header\", True).csv(\"/Volumes/workspace/ecommerce/csv_data/department_location.csv\", inferSchema=True)\n",
    "df_dept.show()\n",
    "\n",
    "# 10. Join with timesheet data and list all employees with their DeptHead.\n",
    "df_joined = df.join(df_dept, on=\"Department\", how=\"left\")\n",
    "df_joined.select(\"EmployeeID\", \"Name\", \"Department\", \"DeptHead\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87070a66-8e11-420a-8547-dfa446b918b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+-----+\n|EmployeeID|Alpha|Beta|Gamma|\n+----------+-----+----+-----+\n|      E104| NULL|NULL|    6|\n|      E101|   17|NULL| NULL|\n|      E102| NULL|  15| NULL|\n|      E103|    5|NULL| NULL|\n+----------+-----+----+-----+\n\n+----------+------+---------+\n|EmployeeID|  Mode|ModeHours|\n+----------+------+---------+\n|      E103|Remote|        5|\n|      E101|Remote|       17|\n|      E102|Onsite|        7|\n|      E102|Remote|        8|\n|      E104|Onsite|        6|\n+----------+------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Pivot & Unpivot\n",
    "# 11. Pivot table: total hours per employee per project.\n",
    "df.groupBy(\"EmployeeID\").pivot(\"Project\").agg(sum(\"WorkHours\")).show()\n",
    "\n",
    "# 12. Unpivot example: Convert mode-specific hours into rows.\n",
    "pivot_df = df.groupBy(\"EmployeeID\", \"Mode\").agg(sum(\"WorkHours\").alias(\"ModeHours\"))\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22a5b04f-18f4-4e74-b003-1485b93f5284",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+--------+---------+------+-------+-----------------+----------------+\n|EmployeeID| Name|Department|Project|WorkHours|WorkDate| Location|  Mode|Weekday|RunningTotalHours|WorkloadCategory|\n+----------+-----+----------+-------+---------+--------+---------+------+-------+-----------------+----------------+\n|      E101|Anita|        IT|  Alpha|        8|    NULL|Bangalore|Remote|   NULL|                8|            Full|\n|      E101|Anita|        IT|  Alpha|        9|    NULL|Bangalore|Remote|   NULL|               17|            Full|\n|      E102|  Raj|        HR|   Beta|        7|    NULL|   Mumbai|Onsite|   NULL|                7|         Partial|\n|      E102|  Raj|        HR|   Beta|        8|    NULL|   Mumbai|Remote|   NULL|               15|            Full|\n|      E103| John|   Finance|  Alpha|        5|    NULL|    Delhi|Remote|   NULL|                5|         Partial|\n|      E104|Meena|        IT|  Gamma|        6|    NULL|Hyderabad|Onsite|   NULL|                6|         Partial|\n+----------+-----+----------+-------+---------+--------+---------+------+-------+-----------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# UDF & Conditional Logic\n",
    "# 13. Create a UDF to classify work hours:\n",
    "\n",
    "# def workload_tag(hours):\n",
    "# if hours >= 8: return \"Full\"\n",
    "# elif hours >= 4: return \"Partial\"\n",
    "# else: return \"Light\"\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def workload_tag(hours):\n",
    "    if hours >= 8:\n",
    "        return \"Full\"\n",
    "    elif hours >= 4:\n",
    "        return \"Partial\"\n",
    "    else:\n",
    "        return \"Light\"\n",
    "\n",
    "workload_udf = udf(workload_tag, StringType())\n",
    "\n",
    "# 14. Add a column WorkloadCategory using this UDF.\n",
    "df = df.withColumn(\"WorkloadCategory\", workload_udf(\"WorkHours\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65bfc6ce-deb1-4467-8078-4fc766be10ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+--------+---------+------+-------+-----------------+----------------+\n|EmployeeID| Name|Department|Project|WorkHours|WorkDate| Location|  Mode|Weekday|RunningTotalHours|WorkloadCategory|\n+----------+-----+----------+-------+---------+--------+---------+------+-------+-----------------+----------------+\n|      E101|Anita|        IT|  Alpha|        8|    NULL|Bangalore|Remote|   NULL|                8|            Full|\n|      E101|Anita|        IT|  Alpha|        9|    NULL|Bangalore|Remote|   NULL|               17|            Full|\n|      E102|  Raj|        HR|   Beta|        7|    NULL|   Mumbai|Onsite|   NULL|                7|         Partial|\n|      E102|  Raj|        HR|   Beta|        8|    NULL|   Mumbai|Remote|   NULL|               15|            Full|\n|      E103| John|   Finance|  Alpha|        5|    NULL|    Delhi|  NULL|   NULL|                5|         Partial|\n|      E104|Meena|        IT|  Gamma|        6|    NULL|Hyderabad|Onsite|   NULL|                6|         Partial|\n+----------+-----+----------+-------+---------+--------+---------+------+-------+-----------------+----------------+\n\n+----------+-----+----------+-------+---------+--------+---------+------------+-------+-----------------+----------------+\n|EmployeeID| Name|Department|Project|WorkHours|WorkDate| Location|        Mode|Weekday|RunningTotalHours|WorkloadCategory|\n+----------+-----+----------+-------+---------+--------+---------+------------+-------+-----------------+----------------+\n|      E101|Anita|        IT|  Alpha|        8|    NULL|Bangalore|      Remote|   NULL|                8|            Full|\n|      E101|Anita|        IT|  Alpha|        9|    NULL|Bangalore|      Remote|   NULL|               17|            Full|\n|      E102|  Raj|        HR|   Beta|        7|    NULL|   Mumbai|      Onsite|   NULL|                7|         Partial|\n|      E102|  Raj|        HR|   Beta|        8|    NULL|   Mumbai|      Remote|   NULL|               15|            Full|\n|      E103| John|   Finance|  Alpha|        5|    NULL|    Delhi|Not Provided|   NULL|                5|         Partial|\n|      E104|Meena|        IT|  Gamma|        6|    NULL|Hyderabad|      Onsite|   NULL|                6|         Partial|\n+----------+-----+----------+-------+---------+--------+---------+------------+-------+-----------------+----------------+\n\n+----------+-----+----------+-------+---------+--------+---------+------------+-------+-----------------+----------------+\n|EmployeeID| Name|Department|Project|WorkHours|WorkDate| Location|        Mode|Weekday|RunningTotalHours|WorkloadCategory|\n+----------+-----+----------+-------+---------+--------+---------+------------+-------+-----------------+----------------+\n|      E101|Anita|        IT|  Alpha|        8|    NULL|Bangalore|      Remote|   NULL|                8|            Full|\n|      E101|Anita|        IT|  Alpha|        9|    NULL|Bangalore|      Remote|   NULL|               17|            Full|\n|      E102|  Raj|        HR|   Beta|        7|    NULL|   Mumbai|      Onsite|   NULL|                7|         Partial|\n|      E102|  Raj|        HR|   Beta|        8|    NULL|   Mumbai|      Remote|   NULL|               15|            Full|\n|      E103| John|   Finance|  Alpha|        5|    NULL|    Delhi|Not Provided|   NULL|                5|         Partial|\n|      E104|Meena|        IT|  Gamma|        6|    NULL|Hyderabad|      Onsite|   NULL|                6|         Partial|\n+----------+-----+----------+-------+---------+--------+---------+------------+-------+-----------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Nulls and Cleanup\n",
    "# 15. Introduce some nulls in Mode column.\n",
    "from pyspark.sql.functions import when\n",
    "df = df.withColumn(\"Mode\", when(col(\"EmployeeID\") == \"E103\", None).otherwise(col(\"Mode\")))\n",
    "df.show()\n",
    "\n",
    "# 16. Fill nulls with \"Not Provided\".\n",
    "df = df.fillna({\"Mode\": \"Not Provided\"})\n",
    "df.show()\n",
    "\n",
    "# 17. Drop rows where WorkHours < 4.\n",
    "df = df.filter(col(\"WorkHours\") >= 4)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bba8afd-27b3-4d20-8264-ed25a9f2b896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+--------+---------+------------+-------+-----------------+----------------+-----------+-------------+\n|EmployeeID| Name|Department|Project|WorkHours|WorkDate| Location|        Mode|Weekday|RunningTotalHours|WorkloadCategory|RemoteRatio|   WorkerType|\n+----------+-----+----------+-------+---------+--------+---------+------------+-------+-----------------+----------------+-----------+-------------+\n|      E101|Anita|        IT|  Alpha|        8|    NULL|Bangalore|      Remote|   NULL|                8|            Full|        1.0|Remote Worker|\n|      E101|Anita|        IT|  Alpha|        9|    NULL|Bangalore|      Remote|   NULL|               17|            Full|        1.0|Remote Worker|\n|      E102|  Raj|        HR|   Beta|        7|    NULL|   Mumbai|      Onsite|   NULL|                7|         Partial|        0.5|Hybrid/Onsite|\n|      E102|  Raj|        HR|   Beta|        8|    NULL|   Mumbai|      Remote|   NULL|               15|            Full|        0.5|Hybrid/Onsite|\n|      E103| John|   Finance|  Alpha|        5|    NULL|    Delhi|Not Provided|   NULL|                5|         Partial|        0.0|Hybrid/Onsite|\n|      E104|Meena|        IT|  Gamma|        6|    NULL|Hyderabad|      Onsite|   NULL|                6|         Partial|        0.0|Hybrid/Onsite|\n+----------+-----+----------+-------+---------+--------+---------+------------+-------+-----------------+----------------+-----------+-------------+\n\n+----------+-----+----------+-------+---------+--------+---------+------------+-------+-----------------+----------------+-----------+-------------+----------+\n|EmployeeID| Name|Department|Project|WorkHours|WorkDate| Location|        Mode|Weekday|RunningTotalHours|WorkloadCategory|RemoteRatio|   WorkerType|ExtraHours|\n+----------+-----+----------+-------+---------+--------+---------+------------+-------+-----------------+----------------+-----------+-------------+----------+\n|      E101|Anita|        IT|  Alpha|        8|    NULL|Bangalore|      Remote|   NULL|                8|            Full|        1.0|Remote Worker|         0|\n|      E101|Anita|        IT|  Alpha|        9|    NULL|Bangalore|      Remote|   NULL|               17|            Full|        1.0|Remote Worker|         1|\n|      E102|  Raj|        HR|   Beta|        7|    NULL|   Mumbai|      Onsite|   NULL|                7|         Partial|        0.5|Hybrid/Onsite|         0|\n|      E102|  Raj|        HR|   Beta|        8|    NULL|   Mumbai|      Remote|   NULL|               15|            Full|        0.5|Hybrid/Onsite|         0|\n|      E103| John|   Finance|  Alpha|        5|    NULL|    Delhi|Not Provided|   NULL|                5|         Partial|        0.0|Hybrid/Onsite|         0|\n|      E104|Meena|        IT|  Gamma|        6|    NULL|Hyderabad|      Onsite|   NULL|                6|         Partial|        0.0|Hybrid/Onsite|         0|\n+----------+-----+----------+-------+---------+--------+---------+------------+-------+-----------------+----------------+-----------+-------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Advanced Conditions\n",
    "# 18. Use when-otherwise to mark employees as \"Remote Worker\" if >80% entries are\n",
    "# Remote.\n",
    "remote_counts = df.groupBy(\"EmployeeID\").agg(\n",
    "    (sum(when(col(\"Mode\") == \"Remote\", 1).otherwise(0)) / count(\"*\")).alias(\"RemoteRatio\")\n",
    ")\n",
    "df = df.join(remote_counts, on=\"EmployeeID\", how=\"left\")\n",
    "df = df.withColumn(\"WorkerType\", when(col(\"RemoteRatio\") > 0.8, \"Remote Worker\").otherwise(\"Hybrid/Onsite\"))\n",
    "df.show()\n",
    "\n",
    "# 19. Add a new column ExtraHours where hours > 8.\n",
    "df = df.withColumn(\"ExtraHours\", when(col(\"WorkHours\") > 8, col(\"WorkHours\") - 8).otherwise(0))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f289bc7-dd07-4a59-bcd5-a931d0e07cee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EmployeeID', 'Name', 'Department', 'Project', 'WorkHours', 'WorkDate', 'Location', 'Mode', 'Weekday', 'RunningTotalHours', 'WorkloadCategory', 'RemoteRatio', 'WorkerType', 'ExtraHours']\n14\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "print(len(df.columns))  # Should print 14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0afc9d8-dc04-4279-aac0-9f73c1418654",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+---------+----------+---------+------------+-------+-----------------+----------------+-----------+-------------+----------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|        Mode|Weekday|RunningTotalHours|WorkloadCategory|RemoteRatio|   WorkerType|ExtraHours|\n+----------+-----+----------+-------+---------+----------+---------+------------+-------+-----------------+----------------+-----------+-------------+----------+\n|      E101|Anita|        IT|  Alpha|        8|      NULL|Bangalore|      Remote|   NULL|                8|            Full|        1.0|Remote Worker|         0|\n|      E101|Anita|        IT|  Alpha|        9|      NULL|Bangalore|      Remote|   NULL|               17|            Full|        1.0|Remote Worker|         1|\n|      E102|  Raj|        HR|   Beta|        7|      NULL|   Mumbai|      Onsite|   NULL|                7|         Partial|        0.5|Hybrid/Onsite|         0|\n|      E102|  Raj|        HR|   Beta|        8|      NULL|   Mumbai|      Remote|   NULL|               15|            Full|        0.5|Hybrid/Onsite|         0|\n|      E103| John|   Finance|  Alpha|        5|      NULL|    Delhi|Not Provided|   NULL|                5|         Partial|        0.0|Hybrid/Onsite|         0|\n|      E104|Meena|        IT|  Gamma|        6|      NULL|Hyderabad|      Onsite|   NULL|                6|         Partial|        0.0|Hybrid/Onsite|         0|\n|      E200| Sara|        IT|  Gamma|        5|2024-05-05|  Chennai|      Remote| Sunday|                5|         Partial|        1.0|Remote Worker|         0|\n+----------+-----+----------+-------+---------+----------+---------+------------+-------+-----------------+----------------+-----------+-------------+----------+\n\n+----------+-----+----------+-------+---------+----------+---------+------------+-------+-----------------+----------------+-----------+-------------+----------+\n|EmployeeID| Name|Department|Project|WorkHours|  WorkDate| Location|        Mode|Weekday|RunningTotalHours|WorkloadCategory|RemoteRatio|   WorkerType|ExtraHours|\n+----------+-----+----------+-------+---------+----------+---------+------------+-------+-----------------+----------------+-----------+-------------+----------+\n|      E104|Meena|        IT|  Gamma|        6|      NULL|Hyderabad|      Onsite|   NULL|                6|         Partial|        0.0|Hybrid/Onsite|         0|\n|      E102|  Raj|        HR|   Beta|        7|      NULL|   Mumbai|      Onsite|   NULL|                7|         Partial|        0.5|Hybrid/Onsite|         0|\n|      E103| John|   Finance|  Alpha|        5|      NULL|    Delhi|Not Provided|   NULL|                5|         Partial|        0.0|Hybrid/Onsite|         0|\n|      E101|Anita|        IT|  Alpha|        9|      NULL|Bangalore|      Remote|   NULL|               17|            Full|        1.0|Remote Worker|         1|\n|      E102|  Raj|        HR|   Beta|        8|      NULL|   Mumbai|      Remote|   NULL|               15|            Full|        0.5|Hybrid/Onsite|         0|\n|      E101|Anita|        IT|  Alpha|        8|      NULL|Bangalore|      Remote|   NULL|                8|            Full|        1.0|Remote Worker|         0|\n|      E200| Sara|        IT|  Gamma|        5|2024-05-05|  Chennai|      Remote| Sunday|                5|         Partial|        1.0|Remote Worker|         0|\n+----------+-----+----------+-------+---------+----------+---------+------------+-------+-----------------+----------------+-----------+-------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Union + Duplicate Handling\n",
    "# 20. Append dummy intern record using unionByName()\n",
    "intern_data = [\n",
    "    (\"E200\", \"Sara\", \"IT\", \"Gamma\", 5, \"2024-05-05\", \"Chennai\", \"Remote\",\n",
    "     \"Sunday\", 5, \"Partial\", 1.0, \"Remote Worker\", 0)\n",
    "]\n",
    "\n",
    "columns = df.columns  # Ensure column count and order matches\n",
    "intern_df = spark.createDataFrame(intern_data, schema=columns)\n",
    "df = df.unionByName(intern_df)\n",
    "df.show()\n",
    "\n",
    "# 21. Remove duplicate rows based on all columns.\n",
    "df = df.dropDuplicates()\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Employee Timesheet System 2025-06-18 22:36:04",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}