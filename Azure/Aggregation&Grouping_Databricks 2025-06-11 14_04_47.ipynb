{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72dadc6d-002e-428e-a6df-ba5c35316754",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=4054280922637182#setting/sparkui/0611-041854-oedbfkos/driver-3616197065006495200\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*, 4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7a90e084dbd0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"PySpark Advanced Exercise\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8817b1cc-372e-46dc-bbb8-c366af4ba696",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+\n|  Name| Department|Salary|\n+------+-----------+------+\n|Ananya|         HR| 52000|\n| Rahul|Engineering| 75000|\n| Priya|  Marketing| 58000|\n|  Zoya|Engineering| 62000|\n| Karan|      Sales| 49000|\n|Naveen|Engineering| 68000|\n|Fatima|         HR| 54000|\n+------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "employee_data = [\n",
    "    (\"Ananya\", \"HR\", 52000),\n",
    "    (\"Rahul\", \"Engineering\", 75000),\n",
    "    (\"Priya\", \"Marketing\", 58000),\n",
    "    (\"Zoya\", \"Engineering\", 62000),\n",
    "    (\"Karan\", \"Sales\", 49000),\n",
    "    (\"Naveen\", \"Engineering\", 68000),\n",
    "    (\"Fatima\", \"HR\", 54000)\n",
    "]\n",
    "\n",
    "columns_employee = [\"Name\", \"Department\", \"Salary\"]\n",
    "df_employee = spark.createDataFrame(employee_data, columns_employee)\n",
    "df_employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fc42bab-d235-4008-af28-c858df0f746f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+\n|  Name|Year|Rating|\n+------+----+------+\n|Ananya|2023|   4.5|\n| Rahul|2023|   4.9|\n| Priya|2023|   4.3|\n|  Zoya|2023|   3.8|\n| Karan|2023|   4.1|\n|Naveen|2023|   4.7|\n|Fatima|2023|   3.9|\n+------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "performance_data = [\n",
    "    (\"Ananya\", 2023, 4.5),\n",
    "    (\"Rahul\", 2023, 4.9),\n",
    "    (\"Priya\", 2023, 4.3),\n",
    "    (\"Zoya\", 2023, 3.8),\n",
    "    (\"Karan\", 2023, 4.1),\n",
    "    (\"Naveen\", 2023, 4.7),\n",
    "    (\"Fatima\", 2023, 3.9)\n",
    "]\n",
    "\n",
    "columns_perf = [\"Name\", \"Year\", \"Rating\"]\n",
    "df_perf = spark.createDataFrame(performance_data, columns_perf)\n",
    "df_perf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21197e45-5197-4091-9dbd-8011fc6811e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n| Department|   Average_Salary|\n+-----------+-----------------+\n|         HR|          53000.0|\n|Engineering|68333.33333333333|\n|  Marketing|          58000.0|\n|      Sales|          49000.0|\n+-----------+-----------------+\n\n+-----------+--------------+\n| Department|Employee_Count|\n+-----------+--------------+\n|         HR|             2|\n|Engineering|             3|\n|  Marketing|             1|\n|      Sales|             1|\n+-----------+--------------+\n\n+----------+----------+\n|Max_Salary|Min_Salary|\n+----------+----------+\n|     75000|     62000|\n+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# GroupBy and Aggregations\n",
    "from pyspark.sql.functions import avg, count, max, min\n",
    "\n",
    "# 1. Average salary by department\n",
    "df_employee.groupBy(\"Department\").agg(avg(\"Salary\").alias(\"Average_Salary\")).show()\n",
    "\n",
    "# 2. Count of employees per department\n",
    "df_employee.groupBy(\"Department\").agg(count(\"*\").alias(\"Employee_Count\")).show()\n",
    "\n",
    "# 3. Max and min salary in Engineering\n",
    "df_employee.filter(df_employee.Department == \"Engineering\") \\\n",
    "    .agg(\n",
    "        max(\"Salary\").alias(\"Max_Salary\"),\n",
    "        min(\"Salary\").alias(\"Min_Salary\")\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eec2067-8185-44f5-9011-2af804179b8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n|  Name|Salary|Rating|\n+------+------+------+\n|Ananya| 52000|   4.5|\n|Fatima| 54000|   3.9|\n| Karan| 49000|   4.1|\n|Naveen| 68000|   4.7|\n| Priya| 58000|   4.3|\n| Rahul| 75000|   4.9|\n|  Zoya| 62000|   3.8|\n+------+------+------+\n\n+------+-----------+------+----+------+\n|  Name| Department|Salary|Year|Rating|\n+------+-----------+------+----+------+\n|Naveen|Engineering| 68000|2023|   4.7|\n| Rahul|Engineering| 75000|2023|   4.9|\n+------+-----------+------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "#Join and Combine Data\n",
    "# 4. Inner join on Name\n",
    "df_joined = df_employee.join(df_perf, on=\"Name\", how=\"inner\")\n",
    "\n",
    "# 5. Show each employee’s salary and performance rating\n",
    "df_joined.select(\"Name\", \"Salary\", \"Rating\").show()\n",
    "\n",
    "# 6. Filter: rating > 4.5 and salary > 60000\n",
    "df_joined.filter((df_joined.Rating > 4.5) & (df_joined.Salary > 60000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b7490cb-0916-4e2f-adfb-a2eaceb44515",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+-----------+\n|  Name| Department|Salary|Salary_Rank|\n+------+-----------+------+-----------+\n| Rahul|Engineering| 75000|          1|\n|Naveen|Engineering| 68000|          2|\n|  Zoya|Engineering| 62000|          3|\n|Fatima|         HR| 54000|          1|\n|Ananya|         HR| 52000|          2|\n| Priya|  Marketing| 58000|          1|\n| Karan|      Sales| 49000|          1|\n+------+-----------+------+-----------+\n\n+------+-----------+------+-----------------+\n|  Name| Department|Salary|Cumulative_Salary|\n+------+-----------+------+-----------------+\n| Rahul|Engineering| 75000|            75000|\n|Naveen|Engineering| 68000|           143000|\n|  Zoya|Engineering| 62000|           205000|\n|Fatima|         HR| 54000|            54000|\n|Ananya|         HR| 52000|           106000|\n| Priya|  Marketing| 58000|            58000|\n| Karan|      Sales| 49000|            49000|\n+------+-----------+------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Window & Rank (Bonus Challenge)\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, sum as _sum\n",
    "\n",
    "# 7. Rank employees by salary department-wise\n",
    "window_dept = Window.partitionBy(\"Department\").orderBy(df_employee[\"Salary\"].desc())\n",
    "\n",
    "df_employee.withColumn(\"Salary_Rank\", rank().over(window_dept)).show()\n",
    "\n",
    "# 8. Cumulative salary per department\n",
    "df_employee.withColumn(\"Cumulative_Salary\", _sum(\"Salary\").over(window_dept)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "124664ea-7de9-4432-a005-d4c4ade8dbe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----------+\n|Name  |Department |Salary|JoinDate  |\n+------+-----------+------+----------+\n|Ananya|HR         |52000 |2020-05-22|\n|Rahul |Engineering|75000 |2022-11-22|\n|Priya |Marketing  |58000 |2020-08-21|\n|Zoya  |Engineering|62000 |2020-02-22|\n|Karan |Sales      |49000 |2020-09-03|\n|Naveen|Engineering|68000 |2021-08-20|\n|Fatima|HR         |54000 |2020-05-10|\n+------+-----------+------+----------+\n\n+------+-----------+------+----------+----------------+\n|Name  |Department |Salary|JoinDate  |YearsWithCompany|\n+------+-----------+------+----------+----------------+\n|Ananya|HR         |52000 |2020-05-22|5               |\n|Rahul |Engineering|75000 |2022-11-22|2               |\n|Priya |Marketing  |58000 |2020-08-21|4               |\n|Zoya  |Engineering|62000 |2020-02-22|5               |\n|Karan |Sales      |49000 |2020-09-03|4               |\n|Naveen|Engineering|68000 |2021-08-20|3               |\n|Fatima|HR         |54000 |2020-05-10|5               |\n+------+-----------+------+----------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Date Operations\n",
    "from pyspark.sql.functions import current_date, datediff, col, floor\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# 9. Add JoinDate with random dates between 2020 and 2023\n",
    "def random_date():\n",
    "    start = datetime(2020, 1, 1)\n",
    "    end = datetime(2023, 12, 31)\n",
    "    return start + (end - start) * random.random()\n",
    "\n",
    "# Generate random dates for each employee\n",
    "random_dates = [random_date().strftime('%Y-%m-%d') for _ in range(df_employee.count())]\n",
    "df_random_dates = spark.createDataFrame([(d,) for d in random_dates], [\"JoinDate\"])\n",
    "\n",
    "# Add index to both DataFrames to join properly\n",
    "df_emp_indexed = df_employee.rdd.zipWithIndex().toDF([\"data\", \"index\"])\n",
    "df_dates_indexed = df_random_dates.rdd.zipWithIndex().toDF([\"date\", \"index\"])\n",
    "\n",
    "# Join on index and extract final employee DataFrame with JoinDate\n",
    "df_employee_with_date = df_emp_indexed.join(df_dates_indexed, \"index\") \\\n",
    "    .selectExpr(\"data.Name\", \"data.Department\", \"data.Salary\", \"date.JoinDate\") \\\n",
    "    .withColumn(\"JoinDate\", col(\"JoinDate\").cast(\"date\"))  # ✅ FIXED HERE\n",
    "\n",
    "df_employee_with_date.show(truncate=False)\n",
    "\n",
    "# Step 10: Calculate YearsWithCompany using current_date and datediff\n",
    "df_employee_final = df_employee_with_date.withColumn(\n",
    "    \"YearsWithCompany\",\n",
    "    floor(datediff(current_date(), col(\"JoinDate\")) / 365)\n",
    ")\n",
    "\n",
    "# Display final DataFrame\n",
    "df_employee_final.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba4b31eb-c7e8-4e03-862d-7986abba30fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+----------+----------------+\n|Name  |Department |Salary|JoinDate  |YearsWithCompany|\n+------+-----------+------+----------+----------------+\n|Ananya|HR         |52000 |2020-05-22|5               |\n|Rahul |Engineering|75000 |2022-11-22|2               |\n|Priya |Marketing  |58000 |2020-08-21|4               |\n|Zoya  |Engineering|62000 |2020-02-22|5               |\n|Karan |Sales      |49000 |2020-09-03|4               |\n|Naveen|Engineering|68000 |2021-08-20|3               |\n|Fatima|HR         |54000 |2020-05-10|5               |\n+------+-----------+------+----------+----------------+\n\n+------+-----------+------+----+------+\n|Name  |Department |Salary|Year|Rating|\n+------+-----------+------+----+------+\n|Ananya|HR         |52000 |2023|4.5   |\n|Fatima|HR         |54000 |2023|3.9   |\n|Karan |Sales      |49000 |2023|4.1   |\n|Naveen|Engineering|68000 |2023|4.7   |\n|Priya |Marketing  |58000 |2023|4.3   |\n|Rahul |Engineering|75000 |2023|4.9   |\n|Zoya  |Engineering|62000 |2023|3.8   |\n+------+-----------+------+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "#Writing to Files\n",
    "# 11. Write df_employee_final to CSV with headers\n",
    "df_csv_read = spark.read.option(\"header\", True).csv(\"dbfs:/tmp/employee_csv_output\")\n",
    "df_csv_read.show(truncate=False)\n",
    "\n",
    "# 12. Save df_joined (employee + performance) to Parquet\n",
    "df_parquet_read = spark.read.parquet(\"dbfs:/tmp/employee_perf_parquet\")\n",
    "df_parquet_read.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Aggregation&Grouping_Databricks 2025-06-11 14:04:47",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}