{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2111a2e7-f5fd-4642-933e-95221513efae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.session.SparkSession at 0xfffe20601dd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45db48ac-08e8-4886-a750-72a1ab2c0a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------+----------+----------+\n|order_id|supplier_id|delivery_date|delay_days|is_delayed|\n+--------+-----------+-------------+----------+----------+\n|       1|        101|   2025-05-01|        32|         1|\n|       2|        102|   2025-04-30|        33|         1|\n|       3|        103|   2025-05-20|        13|         1|\n+--------+-----------+-------------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#Load order data from CSV using PySpark\n",
    "df = spark.read.csv(\"/Volumes/workspace/supplychain/csv_data/cleaned_supply_chain_data.csv\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bafcec3-e56c-426c-b0b5-2a1a5dee8d0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------+----------+----------+\n|order_id|supplier_id|delivery_date|delay_days|is_delayed|\n+--------+-----------+-------------+----------+----------+\n|       1|        101|   2025-05-01|        32|         1|\n|       2|        102|   2025-04-30|        33|         1|\n|       3|        103|   2025-05-20|        13|         1|\n+--------+-----------+-------------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#Filter delayed shipments\n",
    "from pyspark.sql.functions import col\n",
    "delayed_df = df.filter(col(\"is_delayed\") == 1)\n",
    "delayed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ca8041d-fdde-4881-b34c-3420f187a922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n|supplier_id|delayed_orders|\n+-----------+--------------+\n|        102|             1|\n|        101|             1|\n|        103|             1|\n+-----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Group by supplier and count delayed orders\n",
    "grouped_df = delayed_df.groupBy(\"supplier_id\").count().withColumnRenamed(\"count\", \"delayed_orders\")\n",
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35085f1a-3069-4278-834e-bf78d7e6b02a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Save processed data to CSV or Parquet\n",
    "# Create volume using spark.sql if you can't use a SQL notebook\n",
    "spark.sql(\"\"\"\n",
    "    CREATE VOLUME IF NOT EXISTS workspace.supplychain.supply_output\n",
    "    COMMENT 'For storing processed supply chain data'\n",
    "\"\"\")\n",
    "\n",
    "# Save results to CSV \n",
    "grouped_df.write.mode(\"overwrite\").option(\"header\", \"true\") \\\n",
    "    .csv(\"/Volumes/workspace/supplychain/supply_output/week3_delayed_orders_csv\")\n",
    "\n",
    "# Save results to Parquet\n",
    "grouped_df.write.mode(\"overwrite\") \\\n",
    "    .parquet(\"/Volumes/workspace/supplychain/supply_output/week3_delayed_orders_parquet\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Supply Chain Monitoring2025-06-23 17:31:51",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}